<!doctype html>
<html
	lang="en" >
	<head>
		<title>
            Caio Raphael
		</title>
		<meta
			charset="utf-8" >
		<meta
			name="viewport" 
			content="width=device-width, initial-scale=1" >
		<meta
			name="description" 
			content="Senior Game Developer, Engine Developer, Low-Level Network, Low-Level Systems" >
		<meta
			name="author" 
			content="Caio Raphael" >
		<meta
			name="theme-color" 
			content="#ffffff" 
			media="(prefers-color-scheme: light)" >
		<meta
			name="theme-color" 
			content="#101010" 
			media="(prefers-color-scheme: dark)" >
		<link
			rel="icon" 
			href="/assets/favicon.ico" >
		<link
			rel="icon" 
			href="/assets/favicon-16x16.png" 
			sizes="16x16" 
			type="image/png" >
		<link
			rel="icon" 
			href="/assets/favicon-32x32.png" 
			sizes="32x32" 
			type="image/png" >
		<script
			src="/static/docs_load.js" >
		</script>
		<script>
window.MathJax = {
                tex: {
                    inlineMath: [['$', '$']],
                    displayMath: [['$$', '$$']]
                }
                };
		</script>
		<script
			src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" >
		</script>
		<script
			src="https://unpkg.com/@highlightjs/cdn-assets@11.11.1/highlight.min.js" >
		</script>
		<script
			src="https://unpkg.com/highlightjs-odinlang@1.4.0/dist/odin.min.js" >
		</script>
		<script
			type="module" >

                    import hljs from 'https://unpkg.com/@highlightjs/cdn-assets@11.11.1/es/highlight.min.js';
                    import hljs_odin from 'https://unpkg.com/highlightjs-odinlang@1.4.0/dist/odin.es.min.js';
                    hljs.registerLanguage('odin', hljs_odin);
                    hljs.highlightAll();
                
		</script>
		<link
			rel="stylesheet" 
			href="/static/docs.css" >
	</head>
	<body>
		<aside
			id="left-sidebar" >
			<a
				href="/" 
				class="site-logo" >
                Caio Raphael
			</a>
			<nav>
				<details
>
					<summary>
                        Graphics Programming
					</summary>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Graphics Programming/Vulkan/Vulkan.html" >
                                Vulkan
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Graphics Programming/Render Engineering/Render Engineering.html" >
                                Render Engineering
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Graphics Programming/Graphics and Shaders/Graphics and Shaders.html" >
                                Graphics and Shaders
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Graphics Programming/GLSL/GLSL.html" >
                                GLSL
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Graphics Programming/GPU/GPU.html" >
                                GPU
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Graphics Programming/OpenGL/OpenGL.html" >
                                OpenGL
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Graphics Programming/Slang.html" >
                                Slang
							</a>
						</li>
					</ul>
				</details>
				<details
>
					<summary>
                        Design
					</summary>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Design/Design - DOD e COP/Design - DOD e COP.html" >
                                Design - DOD e COP
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Design/Design - ECS/Design - ECS.html" >
                                Design - ECS
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Design/Design - Design Patterns/Design - Design Patterns.html" >
                                Design - Design Patterns
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Design/Design - Architecture Patterns.html" >
                                Design - Architecture Patterns
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Design/Design - Concepts and Terminology.html" >
                                Design - Concepts and Terminology
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Design/Design - Rules of thumb - Laws - Guidelines and Principles/Design - Rules of thumb - Laws - Guidelines and Principles.html" >
                                Design - Rules of thumb - Laws - Guidelines and Principles
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Design/Design - Paradigms.html" >
                                Design - Paradigms
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Design/Design - Production Methodologies/Design - Production Methodologies.html" >
                                Design - Production Methodologies
							</a>
						</li>
					</ul>
				</details>
				<details
>
					<summary>
                        Network
					</summary>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Network/Network - Backend/Network - Backend.html" >
                                Network - Backend
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Network/Network - Low Level e Etc/Network - Low Level e Etc.html" >
                                Network - Low Level e Etc
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Network/Network - Netcode/Network - Netcode.html" >
                                Network - Netcode
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Network/Network - HTTP/Network - HTTP.html" >
                                Network - HTTP
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Network/Encryption.html" >
                                Encryption
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Network/SSH.html" >
                                SSH
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Network/Serialization - Encoding/Serialization - Encoding.html" >
                                Serialization - Encoding
							</a>
						</li>
					</ul>
				</details>
				<details
					open="">
					<summary>
                        Things
					</summary>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Things/Build Systems - Compilation - Linking/Build Systems - Compilation - Linking.html" >
                                Build Systems - Compilation - Linking
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Things/CPU/CPU.html" >
                                CPU
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Things/Code Editors/NeoVim - Setup/NeoVim - Setup.html" >
                                NeoVim - Setup
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Things/Code Editors/NeoVim - Uso/NeoVim - Uso.html" >
                                NeoVim - Uso
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Things/Code Editors/VSCode - VSCodium.html" >
                                VSCode - VSCodium
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Things/Code Editors/Visual Studio/Visual Studio.html" >
                                Visual Studio
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Things/Debuggers.html" >
                                Debuggers
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Things/Dependencies.html" >
                                Dependencies
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Things/Git/Git.html" >
                                Git
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Things/Handmade Hero/Handmade Hero.html" >
                                Handmade Hero
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Things/Linux/Linux.html" >
                                Linux
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Things/Memory/Memory.html" >
                                Memory
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="active" 
								href="/docs/Things/Multithreading/Multithreading.html" >
                                Multithreading
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Things/RegEx.html" >
                                RegEx
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Things/Shells/Shells.html" >
                                Shells
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Things/Terminal/Terminal.html" >
                                Terminal
							</a>
						</li>
					</ul>
				</details>
				<details
>
					<summary>
                        Programming Languages
					</summary>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Programming Languages/Assembly - ASM.html" >
                                Assembly - ASM
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Programming Languages/C++/C++.html" >
                                C++
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Programming Languages/C.html" >
                                C
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Programming Languages/CSharp/CSharp.html" >
                                CSharp
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Programming Languages/Go.html" >
                                Go
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Programming Languages/Haxe.html" >
                                Haxe
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Programming Languages/JAI.html" >
                                JAI
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Programming Languages/Java.html" >
                                Java
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Programming Languages/Kotlin.html" >
                                Kotlin
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Programming Languages/Lua.html" >
                                Lua
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Programming Languages/Nim/Nim.html" >
                                Nim
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Programming Languages/Odin/Odin.html" >
                                Odin
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Programming Languages/Python.html" >
                                Python
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Programming Languages/Rust/Rust.html" >
                                Rust
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Programming Languages/Swift/Swift.html" >
                                Swift
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Programming Languages/Zig/Zig.html" >
                                Zig
							</a>
						</li>
					</ul>
				</details>
				<details
>
					<summary>
                        WebDev
					</summary>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/WebDev/WebDev/WebDev.html" >
                                WebDev
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/WebDev/HTML/HTML.html" >
                                HTML
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/WebDev/HTMX.html" >
                                HTMX
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/WebDev/WebAssembly - WASM/WebAssembly - WASM.html" >
                                WebAssembly - WASM
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/WebDev/CSS/CSS.html" >
                                CSS
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/WebDev/JavaScript/JavaScript.html" >
                                JavaScript
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/WebDev/TypeScript.html" >
                                TypeScript
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/WebDev/JavaScript - Frameworks and Libraries/JavaScript - Frameworks and Libraries.html" >
                                JavaScript - Frameworks and Libraries
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/WebDev/JavaScript - Runtime Environments.html" >
                                JavaScript - Runtime Environments
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/WebDev/Hugo/Hugo.html" >
                                Hugo
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/WebDev/Static Site Generators.html" >
                                Static Site Generators
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/WebDev/HTML - Tests/HTML - Tests.html" >
                                HTML - Tests
							</a>
						</li>
					</ul>
				</details>
				<details
>
					<summary>
                        Databases
					</summary>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Databases/Databases - MongoDB.html" >
                                Databases - MongoDB
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Databases/Databases - SQL - Relational/Databases - SQL - Relational.html" >
                                Databases - SQL - Relational
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Databases/Databases - Document Oriented.html" >
                                Databases - Document Oriented
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Databases/Databases - Object Oriented.html" >
                                Databases - Object Oriented
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Databases/Databases - ORMs.html" >
                                Databases - ORMs
							</a>
						</li>
					</ul>
				</details>
				<details
>
					<summary>
                        Electronics
					</summary>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Electronics/Electronics - Sources and Studies.html" >
                                Electronics - Sources and Studies
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Electronics/Electronics - Projects and Tutorials.html" >
                                Electronics - Projects and Tutorials
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/docs/Electronics/Arduino.html" >
                                Arduino
							</a>
						</li>
					</ul>
				</details>
			</nav>
		</aside>
		<div
			id="central-wrapper" >
			<header
				id="central-header" >
				<nav
					id="dropdown-menu" >
					<select
						onchange="if (this.value) window.location.href=this.value" >
						<option
							value="/" 
>
                            üè° Home
						</option>
						<option
							value="/docs/_index.html" 
							selected="">
                            üìñ Docs
						</option>
					</select>
				</nav>
				<button
					class="btn" 
					id="button-color-theme" >
					<i>
                        ‚òÄÔ∏è / üåë
					</i>
				</button>
			</header>
			<main>
				<article
					id="note-article" >
					<header>
						<h1>
                            Multithreading
						</h1>
						<p>
							<time
								datetime="2025-03-11" >
                                üïí Created: 2025-03-11
							</time>
							<time
								datetime="2025-10-29" >
                                | Updated: 2025-10-29
							</time>
						</p>
					</header>
					<div
						id="note-content" >
<h2
	id="core-concepts" >
    Core Concepts
</h2>
<ul>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=7ENFeb-J75k" 
				class="external-link" 
				target="_blank" >
                Cool Explanation of Process and Multithread
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    Uses a bit of machine code to demonstrate common multithreading issues.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=GNw3RXr-VJk" 
				class="external-link" 
				target="_blank" >
                Multithreading - pt1
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    He presents everything very well. Great talk.
				</p>
			</li>
			<li>
				<p>
                    Assumes you already know what mutex, deadlocks, etc., are.
				</p>
			</li>
			<li>
				<p>
                    It's an intermediate talk, not beginner.
				</p>
			</li>
			<li>
				<p>
                    Pt2 is very technical for C++, basically explaining a library with techniques.
				</p>
				<ul>
					<li>
						<p>
                            Maybe useful, but not relevant atm.
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
	<li>
		<p>
            <a href="Handmade Hero.html#Ep 122 - Introduction to Multithreading">
            Handmade Hero#Ep 122 - Introduction to Multithreading
            </a>
            .
		</p>
	</li>
</ul>
<h3
	id="process-vs-threads" >
    Process vs Threads
</h3>
<ul>
	<li>
		<p>
            <img src="assets/image_20250422073736.png" width="450" >
            .
		</p>
	</li>
	<li>
		<p>
            Threads are execution units within the same process.
		</p>
	</li>
	<li>
		<p>
            A process can contain multiple threads, all sharing the same process memory space.
		</p>
	</li>
	<li>
		<p>
            <img src="assets/screenshot_2025-04-22_071300.png" width="425" >
            .
		</p>
	</li>
	<li>
		<p>
			<strong>
                Memory
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    All threads (including main) share the same heap, code, and global data, but have independent stacks.
				</p>
			</li>
			<li>
				<p>
					<strong>
                        Memory shared among all threads
					</strong>
                    :
				</p>
				<ul>
					<li>
						<p>
                            All heap-allocated memory.
						</p>
					</li>
					<li>
						<p>
                            All memory of global/static variables by the main thread.
						</p>
					</li>
					<li>
						<p>
                            Main thread stack.
						</p>
						<ul>
							<li>
								<p>
                                    Each thread has its own stack. However, pointers to main thread stack variables can be passed to other threads, risking out-of-scope use.
								</p>
							</li>
							<li>
								<p>
                                    Passing a pointer to a local main thread variable to another thread is risky if main exits scope before the other thread finishes.
								</p>
							</li>
						</ul>
					</li>
				</ul>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="multithreading" >
    Multithreading
</h3>
<ul>
	<li>
		<p>
            Technique that allows concurrent execution of two or more threads within a single process.
		</p>
	</li>
	<li>
		<p>
            Threads can perform tasks independently while sharing process resources.
		</p>
	</li>
	<li>
		<p>
            Multithreading is both:
		</p>
		<ul>
			<li>
				<p>
                    A concept (the idea of using multiple threads).
				</p>
			</li>
			<li>
				<p>
                    A strategy if used directly (e.g., &quot;We use raw threads for parallelism&quot;).
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Models
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
					<strong>
                        User-Level Threads
					</strong>
                    :
				</p>
				<ul>
					<li>
						<p>
                            Managed by the application, no kernel involvement.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<strong>
                        Kernel-Level Threads
					</strong>
                    :
				</p>
				<ul>
					<li>
						<p>
                            Managed directly by the operating system.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<strong>
                        Hybrid Model
					</strong>
                    :
				</p>
				<ul>
					<li>
						<p>
                            Combines user and kernel threads.
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="main-thread" >
    Main Thread
</h3>
<ul>
	<li>
		<p>
            It's the first thread that starts executing when the process is launched.
		</p>
	</li>
	<li>
		<p>
            Automatically created by the OS when starting the process.
		</p>
	</li>
</ul>
<h3
	id="resource" >
    Resource
</h3>
<ul>
	<li>
		<p>
            Something you don't want 2 threads to access simultaneously.
		</p>
	</li>
	<li>
		<p>
            Ex:
		</p>
		<ul>
			<li>
				<p>
                    Memory location.
				</p>
			</li>
			<li>
				<p>
                    File handle.
				</p>
			</li>
			<li>
				<p>
                    Non-thread safe objects.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="starvation" >
    Starvation
</h3>
<ul>
	<li>
		<p>
            ~When a thread finishes work much faster than another and ends up doing nothing?
		</p>
	</li>
</ul>
<h3
	id="race-conditions" >
    Race conditions
</h3>
<ul>
	<li>
		<p>
            When a Resource is accessed by 2 or more threads simultaneously, and at least one of them is a 
			<em>
                write
			</em>
            .
		</p>
	</li>
	<li>
		<p>
            Causes 
			<strong>
                Undefined Behavior
			</strong>
            ; it's critical to avoid this.
		</p>
	</li>
</ul>
<h3
	id="deadlock" >
    Deadlock
</h3>
<ul>
	<li>
		<p>
            Occurs when two or more threads are blocked forever, waiting on each other‚Äôs resources.
		</p>
	</li>
</ul>
<h3
	id="context-switching" >
    Context Switching
</h3>
<ul>
	<li>
		<p>
            The CPU switches between threads to provide concurrency.
		</p>
	</li>
	<li>
		<p>
            Involves saving and loading thread states.
		</p>
	</li>
	<li>
		<p>
            Has performance cost due to saving/restoring CPU registers, etc.
		</p>
	</li>
</ul>
<h3
	id="parallelism" >
    Parallelism
</h3>
<ul>
	<li>
		<p>
            Executing multiple tasks simultaneously, typically on multiple cores or processors.
		</p>
	</li>
	<li>
		<p>
            Focuses on 
			<em>
                true simultaneous execution
			</em>
            .
		</p>
	</li>
	<li>
		<p>
            Requires multiple physical or logical processing units.
		</p>
	</li>
	<li>
		<p>
            Common in CPU-bound workloads (e.g., simulations, rendering).
		</p>
	</li>
	<li>
		<p>
			<strong>
                Ex
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Using 4 cores to calculate physics updates for 4 game entities at the same time.
				</p>
			</li>
			<li>
				<p>
                    &quot;Multiple workers doing different tasks 
					<em>
                        at the same time
					</em>
                    .&quot;
				</p>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="concurrency" >
    Concurrency
</h3>
<ul>
	<li>
		<p>
            Managing multiple tasks at the same time, but not necessarily executing them simultaneously.
		</p>
	</li>
	<li>
		<p>
            Focuses on interleaving execution of tasks.
		</p>
	</li>
	<li>
		<p>
			<em>
                Can occur on a single core
			</em>
            .
		</p>
	</li>
	<li>
		<p>
            Requires context switching.
		</p>
	</li>
	<li>
		<p>
            Involves task coordination, scheduling, and synchronization.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Exs
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    &quot;One worker juggling multiple tasks ‚Äî rapidly switching between them.&quot;
				</p>
			</li>
			<li>
				<p>
                    Switching between user input handling and background loading in a game loop.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                About Multiplexer
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    A multiplexer is a building block, not a full concurrency strategy.
				</p>
			</li>
			<li>
				<p>
                    Enables event-driven concurrency on a single thread, often forming the basis of asynchronous runtimes.
				</p>
			</li>
			<li>
				<p>
                    Common in network servers, GUIs, and I/O frameworks.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="sleeping" >
    Sleeping
</h3>
<ul>
	<li>
		<p>
            &quot;odd thing: while sleeping for 0.8 ms in my main loop I get 15% cpu usage, but if I sleep for 1ms then the cpu usage drops to 1%&quot;.
		</p>
		<ul>
			<li>
				<p>
                    This behavior is actually expected and reveals insights about how modern OSes handle thread scheduling and sleep durations:
				</p>
				<ul>
					<li>
						<p>
                            Windows (and most OSes) have a default scheduler tick interval (usually 15.6ms or 1ms depending on system)
						</p>
					</li>
					<li>
						<p>
                            Below ~1ms, sleeps are often implemented as busy-waits (explaining your 15% CPU at 0.8ms)
						</p>
					</li>
					<li>
						<p>
                            Above 1ms, the OS can properly suspend the thread (hence 1% at 1ms)
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    The 1% vs 15% difference represents your CPU entering deeper sleep states
				</p>
			</li>
			<li>
				<p>
                    Wow, crazy.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h2
	id="thread" >
    Thread
</h2>
<h5
	id="thread" >
    Thread
</h5>
<ul>
	<li>
		<p>
            A thread is the smallest unit of execution within a process.
		</p>
	</li>
	<li>
		<p>
            Multiple threads can exist within the same process and share the same memory space.
		</p>
	</li>
</ul>
<h5
	id="tid" >
    TID
</h5>
<ul>
	<li>
		<p>
            Thread ID.
		</p>
	</li>
</ul>
<h5
	id="pid" >
    PID
</h5>
<ul>
	<li>
		<p>
            Process ID.
		</p>
	</li>
</ul>
<h5
	id="how-many-threads-can-i-have-per-core" >
    How many threads can I have per core?
</h5>
<ul>
	<li>
		<p>
            A thread is a unit of execution, a core is a physical CPU processing unit.
		</p>
	</li>
	<li>
		<p>
            OS defines the relationship by scheduling threads on cores.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Creation Limit
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Number of threads you can spawn.
				</p>
			</li>
			<li>
				<p>
                    There is 
					<em>
                        no fixed limit
					</em>
                    &nbsp;on how many threads you can create per core; you can spawn hundreds or thousands.
				</p>
			</li>
			<li>
				<p>
                    This can be hundreds to millions, but most will be idle or waiting.
				</p>
			</li>
			<li>
				<p>
                    Often limited by OS, memory, and thread stack size.
				</p>
				<ul>
					<li>
						<p>
                            Linux:
						</p>
						<ul>
							<li>
								<p>
                                    Depends on 
                                    <code>ulimit -u</code>
                                    , memory, stack size (e.g., ~30K‚Äì100K).
								</p>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            Windows:
						</p>
						<ul>
							<li>
								<p>
                                    Limited by memory (e.g., ~2K‚Äì10K threads).
								</p>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            JVM:
						</p>
						<ul>
							<li>
								<p>
                                    Each Java thread uses ~1MB stack by default.
								</p>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            Go:
						</p>
						<ul>
							<li>
								<p>
                                    Goroutines scale to millions (they are not OS threads).
								</p>
							</li>
						</ul>
					</li>
				</ul>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Execution Limit
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Number of threads that can run at the same time on a core.
				</p>
			</li>
			<li>
				<p>
                    Only a limited number can be executed concurrently per core.
				</p>
			</li>
			<li>
				<p>
                    Can be 1 or 2 (with SMT/hyper-threading).
				</p>
			</li>
			<li>
				<p>
					<strong>
                        SMT (Simultaneous Multithreading)
					</strong>
                    :
				</p>
				<ul>
					<li>
						<p>
                            Is a CPU-level technique that allows a single physical core to execute multiple threads simultaneously by sharing its internal execution resources.
						</p>
					</li>
					<li>
						<p>
							<strong>
                                Hyper-threading
							</strong>
                            :
						</p>
						<ul>
							<li>
								<p>
                                    It's a implementation of SMT by Intel.
								</p>
							</li>
							<li>
								<p>
                                    A single core can run two hardware threads, allowing limited parallelism per core.
								</p>
							</li>
						</ul>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<strong>
                        Example
					</strong>
                    :
				</p>
				<ul>
					<li>
						<p>
                            1 thread, 1 core
						</p>
						<ul>
							<li>
								<p>
                                    Thread runs directly on the core.
								</p>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            N threads, 1 core
						</p>
						<ul>
							<li>
								<p>
                                    OS time-slices threads ‚Äî concurrent, not parallel.
								</p>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            N threads, M cores
						</p>
						<ul>
							<li>
								<p>
                                    Threads are distributed across cores and context-switched as needed.
								</p>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            N threads, M cores
						</p>
						<ul>
							<li>
								<p>
                                    Potential parallel execution, if the OS schedules them simultaneously.
								</p>
							</li>
						</ul>
					</li>
				</ul>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Affinity
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Threads can be pinned to specific cores to improve cache locality and reduce context switching.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Load balancing
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    OS may migrate threads between cores to balance CPU load.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="thread-safety" >
    Thread-Safety
</h5>
<ul>
	<li>
		<p>
            Code is thread-safe if it behaves correctly when accessed concurrently by multiple threads.
		</p>
		<ul>
			<li>
				<p>
                    No race conditions.
				</p>
			</li>
			<li>
				<p>
                    No data corruption.
				</p>
			</li>
			<li>
				<p>
                    No undefined behavior.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            Expected output is always produced, regardless of timing.
		</p>
	</li>
	<li>
		<p>
            Code is thread-safe if it functions correctly when executed concurrently by multiple threads.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Immutability
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Immutable objects cannot be changed, inherently thread-safe.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Thread-Local Storage
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Store separate variable copies for each thread.
				</p>
			</li>
			<li>
				<p>
					<em>
                        Example
					</em>
                    : 
                    <code>thread_local</code>
                    &nbsp;in C++, 
                    <code>ThreadLocal&lt;T&gt;</code>
                    &nbsp;in Java, 
                    <code>threading.local()</code>
                    &nbsp;in Python.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Lock-Free or Wait-Free Algorithms
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Data structures avoiding locks but preventing race conditions.
				</p>
			</li>
			<li>
				<p>
					<strong>
                        Advanced
					</strong>
                    : Requires atomic operations and memory ordering.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Avoid Shared State
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Design so threads do not share mutable data; use message-passing or immutable queues.
				</p>
			</li>
			<li>
				<p>
					<strong>
                        Example
					</strong>
                    : Actor model (Erlang, Akka in Scala).
				</p>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="fibers" >
    Fibers
</h3>
<ul>
	<li>
		<p>
            Fiber: 
			<em>
                lightweight thread
			</em>
            , manually scheduled, cooperatively multitasked.
		</p>
	</li>
	<li>
		<p>
            Not OS-managed; must explicitly yield control.
		</p>
		<ul>
			<li>
				<p>
                    Create, switch, resume fibers manually.
				</p>
			</li>
			<li>
				<p>
                    Hard to use correctly; OS doesn't schedule them.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            Optimize task switching within a single thread.
		</p>
	</li>
	<li>
		<p>
            Each fiber has its own stack.
		</p>
	</li>
	<li>
		<p>
            More like user-space threads, no preemption.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Use Case
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    User-level schedulers
				</p>
			</li>
			<li>
				<p>
                    High-performance I/O systems
				</p>
			</li>
			<li>
				<p>
                    Green thread implementations
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Exemples
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Found in lower-level systems like Windows fibers, libfiber, or languages like Ruby , C++ , and Rust .
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            [cppcon 2017] &quot;Not currently in C++. Available via Boost.Fiber&quot;
		</p>
	</li>
</ul>
<h3
	id="green-thread" >
    Green Thread
</h3>
<ul>
	<li>
		<p>
            A thread that is scheduled by a runtime library instead of natively by the underlying OS.
		</p>
	</li>
	<li>
		<p>
            Used to emulate multithreading without OS support.
		</p>
	</li>
	<li>
		<p>
            [cppcon 2017] &quot;Not currently in C++. Most OSs support native threads&quot;.
		</p>
	</li>
	<li>
		<p>
            Not widely used outside of:
		</p>
		<ul>
			<li>
				<p>
                    Go.
				</p>
			</li>
			<li>
				<p>
                    Erland.
				</p>
				<ul>
					<li>
						<p>
                            Sort of.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Java.
				</p>
				<ul>
					<li>
						<p>
                            In older versions.
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="goroutine" >
    Goroutine
</h3>
<ul>
	<li>
		<p>
            Lightweight 
			<em>
                user-space thread
			</em>
            , managed by Go runtime, not OS.
		</p>
	</li>
	<li>
		<p>
            Created with 
            <code>go</code>
            &nbsp;keyword: 
            <code>go myFunction</code>
		</p>
	</li>
	<li>
		<p>
            Multiplexed onto fewer OS threads by Go scheduler.
		</p>
	</li>
</ul>
<h5
	id="comparisons" >
    Comparisons
</h5>
<ul>
	<li>
		<p>
			<strong>
                OS Threads
			</strong>
		</p>
		<ul>
			<li>
				<p>
                    Goroutines are cheaper: Go can run thousands of goroutines on a few OS threads.
				</p>
			</li>
			<li>
				<p>
                    Less control: You don‚Äôt pin goroutines to cores or manage priorities.
				</p>
			</li>
			<li>
				<p>
                    Automatic scheduling: Go‚Äôs scheduler handles context switching without OS involvement.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Fibers/Coroutines
			</strong>
		</p>
		<ul>
			<li>
				<p>
                    Goroutines are preemptively scheduled , unlike pure coroutines&nbsp;&nbsp;which require manual yielding.
				</p>
			</li>
			<li>
				<p>
                    More natural for writing blocking-style code that looks synchronous.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Green Threads
			</strong>
		</p>
		<ul>
			<li>
				<p>
                    Goroutines are a modern green-thread model but include runtime preemption and parallelism support on multiple cores.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="advantages" >
    Advantages
</h5>
<ul>
	<li>
		<p>
            Simple syntax: 
            <code>go func()</code>
		</p>
	</li>
	<li>
		<p>
            Low memory and CPU overhead.
		</p>
	</li>
	<li>
		<p>
            Built-in channels provide CSP-style synchronization.
		</p>
	</li>
	<li>
		<p>
            Easier to write concurrent code with synchronous semantics.
		</p>
	</li>
</ul>
<h5
	id="disadvantages" >
    Disadvantages
</h5>
<ul>
	<li>
		<p>
            Less control than OS threads .
		</p>
	</li>
	<li>
		<p>
            Go runtime scheduling overhead may become a bottleneck in specific low-latency or real-time workloads.
		</p>
	</li>
	<li>
		<p>
            Blocking syscalls (C FFI) can still tie up OS threads unless handled specially.
		</p>
	</li>
</ul>
<h2
	id="strategies" >
    Strategies
</h2>
<ul>
	<li>
		<p>
            See 
            <a href="Physics Engines.html#Teoria">
            Physics Engines#Theory
            </a>
            &nbsp;for strategies.
		</p>
		<ul>
			<li>
				<p>
					<em>
                        Ex
					</em>
                    : Spin Lockless &gt; Lockless &gt; Sync Primitives.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="dedicated-threads-static-partitioning-domain-specific-threads" >
    Dedicated Threads / Static Partitioning / Domain-Specific Threads
</h3>
<ul>
	<li>
		<p>
            Assign specific long-running tasks to fixed threads.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Characteristics:
			</strong>
		</p>
		<ul>
			<li>
				<p>
                    Specialized threads (e.g., physics thread only does physics).
				</p>
			</li>
			<li>
				<p>
                    No dynamic task assignment.
				</p>
			</li>
			<li>
				<p>
                    Threads run continuously.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Used in:
			</strong>
		</p>
		<ul>
			<li>
				<p>
                    Older game engines, embedded systems, latency-critical apps.
				</p>
			</li>
			<li>
				<p>
                    Older Unreal Engine versions: separate threads for rendering, physics.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                When to Stick
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Game performance acceptable.
				</p>
			</li>
			<li>
				<p>
                    Stability/debug priority over throughput.
				</p>
			</li>
			<li>
				<p>
                    Workloads not easily parallelizable.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            Dedicated thread may call job system for subtasks.
		</p>
	</li>
</ul>
<h5
	id="pros" >
    Pros
</h5>
<ul>
	<li>
		<p>
            Simple to implement/debug.
		</p>
	</li>
	<li>
		<p>
            Low latency.
		</p>
	</li>
	<li>
		<p>
            Predictable (no unrelated contention).
		</p>
	</li>
</ul>
<h5
	id="cons" >
    Cons
</h5>
<ul>
	<li>
		<p>
            Poor CPU utilization (idle threads waste cores).
		</p>
	</li>
	<li>
		<p>
            Hard to scale.
		</p>
	</li>
</ul>
<h5
	id="implementation" >
    Implementation
</h5>
<ul>
	<li>
		<p>
            No Scheduler: thread manages own work manually.
		</p>
	</li>
	<li>
		<p>
            No task queue.
		</p>
	</li>
</ul>
<h5
	id="transition-to-job-system" >
    Transition to: Job System
</h5>
<ul>
	<li>
		<p>
			<strong>
                Technical Hurdles
			</strong>
		</p>
		<ul>
			<li>
				<p>
                    Task Granularity: Dedicated threads handle coarse work (e.g., &quot;whole physics step&quot;), while jobs need fine-grained tasks (e.g., &quot;collision check between A and B&quot;).
				</p>
			</li>
			<li>
				<p>
                    Synchronization: Dedicated threads often use locks; jobs rely on dependencies/atomics.
				</p>
			</li>
			<li>
				<p>
                    Latency Sensitivity: Jobs introduce scheduling overhead (bad for real-time threads like rendering).
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Architectural Shifts
			</strong>
		</p>
		<ul>
			<li>
				<p>
                    From &quot;Owned Data&quot; to &quot;Shared Data&quot;: Dedicated threads often own their data (e.g., physics thread owns physics state), while jobs assume data is transient/immutable.
				</p>
			</li>
			<li>
				<p>
                    Debugging Complexity: Race conditions in jobs are harder to trace than linear thread execution.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="job-system" >
    Job System
</h3>
<h5
	id="about" >
    About
</h5>
<ul>
	<li>
		<p>
			<em>
                Unit of work
			</em>
            : task/function executed asynchronously.
		</p>
	</li>
	<li>
		<p>
            Scheduled and managed by runtime/thread pool/scheduler.
		</p>
	</li>
	<li>
		<p>
            Jobs require execution context and synchronization.
		</p>
	</li>
	<li>
		<p>
            Like a thread pool but with:
		</p>
		<ul>
			<li>
				<p>
                    Fine-grained tasks, dependencies, work stealing.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="workflow" >
    Workflow
</h5>
<ul>
	<li>
		<p>
            A 
			<em>
                job
			</em>
            &nbsp;(e.g., compute something, handle request) is created.
		</p>
	</li>
	<li>
		<p>
            It's added to a 
			<em>
                job queue
			</em>
            .
		</p>
	</li>
	<li>
		<p>
            A 
			<em>
                scheduler
			</em>
            &nbsp;picks jobs from the queue.
		</p>
	</li>
	<li>
		<p>
            A 
			<em>
                worker
			</em>
            &nbsp;(thread/fiber/goroutine) executes the job.
		</p>
	</li>
	<li>
		<p>
			<em>
                Synchronization primitives
			</em>
            &nbsp;are used inside jobs if they access shared state.
		</p>
	</li>
</ul>
<h5
	id="job-or-task" >
    Job or Task?
</h5>
<ul>
	<li>
		<p>
            &quot;Job is a self contained task with parameters.&quot;
		</p>
	</li>
	<li>
		<p>
            They are closely related and often used interchangeably, but there are distinctions depending on context, level of abstraction, and language/runtime.
		</p>
	</li>
	<li>
		<p>
            A task can be considered a specialized kind of job.
		</p>
	</li>
	<li>
		<p>
            In many libraries and frameworks, the terms are synonymous in practice but may imply different underlying models (threaded vs. async).
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250419090857.png" width="325" >
            .
		</p>
	</li>
</ul>
<h5
	id="job-queue" >
    Job Queue
</h5>
<ul>
	<li>
		<p>
            Is a 
			<em>
                data structure
			</em>
            &nbsp;that holds pending jobs (tasks) to be executed by workers (threads, fibers, goroutines, etc.).
		</p>
	</li>
	<li>
		<p>
            It is typically implemented as a FIFO queue, but can vary depending on scheduling strategy.
		</p>
	</li>
	<li>
		<p>
            A 
			<em>
                buffer
			</em>
            &nbsp;that stores jobs waiting to be run.
		</p>
	</li>
	<li>
		<p>
            A job queue is often implemented using an array or linked list, with concurrency controls.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Tipos
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
					<strong>
                        FIFO Queue
					</strong>
				</p>
				<ul>
					<li>
						<p>
                            First In First Out.
						</p>
					</li>
					<li>
						<p>
                            Basic job queue, executed in submission order
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<strong>
                        Priority Queue
					</strong>
				</p>
				<ul>
					<li>
						<p>
                            Jobs with priority levels, higher priority first
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<strong>
                        Ring Buffer
					</strong>
				</p>
				<ul>
					<li>
						<p>
                            Fixed-size circular buffer, often for performance
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<strong>
                        Deque
					</strong>
				</p>
				<ul>
					<li>
						<p>
                            Double-ended queue
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="load-balancing-work-stealing" >
    Load-balancing / Work-stealing
</h5>
<ul>
	<li>
		<p>
			<a
				href="https://blog.molecular-matters.com/2012/04/05/building-a-load-balanced-task-scheduler-part-1-basics/" 
				class="external-link" 
				target="_blank" >
                Building a load-balanced task scheduler ‚Äì Part 1:¬†Basics
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://blog.molecular-matters.com/2012/04/12/building-a-load-balanced-task-scheduler-part-2-task-model-relationships/" 
				class="external-link" 
				target="_blank" >
                Part 2: Task model
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://blog.molecular-matters.com/2012/04/25/building-a-load-balanced-task-scheduler-part-3-parent-child-relationships/" 
				class="external-link" 
				target="_blank" >
                Part 3: Parent-child relationships
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.rismosch.com/article?id=building-a-job-system" 
				class="external-link" 
				target="_blank" >
                Building a Job System in Rust
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    <img src="assets/image_20250523085246.png" width="400" >
                    .
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250523090659.png" width="225" >
                    .
				</p>
				<ul>
					<li>
						<p>
                            Useful to understand its Ring Buffer.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250523091514.png" width="425" >
                    .
				</p>
				<ul>
					<li>
						<p>
                            &quot;Nonetheless, I really want to do what 
							<em>
                                Naughty Dog
							</em>
                            &nbsp;did with their engine: Jobify the entire thing. EVERYTHING will run on this job system, with a few significant exceptions: Startup, shutdown, logging and IO. Everything that runs on the job system will push local jobs.&quot;
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="advantages" >
    Advantages
</h5>
<ul>
	<li>
		<p>
			<strong>
                Fine-Grained Workload Distribution
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Jobs are small units of work, allowing better load balancing across threads.
				</p>
			</li>
			<li>
				<p>
                    Reduces idle time by keeping all threads busy with smaller tasks.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Scalability
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Jobs can be dynamically scheduled across available threads, scaling well with CPU core count.
				</p>
			</li>
			<li>
				<p>
                    Better suited for heterogeneous workloads than static thread partitioning.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Reduced Thread Overhead
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Avoids frequent thread creation/destruction by reusing a fixed pool of worker threads.
				</p>
			</li>
			<li>
				<p>
                    More efficient than per-task threading (e.g., spawning a thread for each small task).
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Dependency Management
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Jobs can express dependencies (e.g., &quot;Job B runs after Job A&quot;), enabling efficient task graphs.
				</p>
			</li>
			<li>
				<p>
                    Better than manual synchronization in raw thread-based approaches.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Cache-Friendly
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Jobs can be designed to process localized data, improving cache coherence compared to thread-per-task models.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Flexibility
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Supports both parallel and serial execution patterns.
				</p>
			</li>
			<li>
				<p>
                    Easier to integrate with other systems (e.g., async I/O, game engines, rendering pipelines).
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="disadvantages" >
    Disadvantages
</h5>
<ul>
	<li>
		<p>
			<strong>
                Scheduling Overhead
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Managing a job queue and dependencies introduces some runtime overhead.
				</p>
			</li>
			<li>
				<p>
                    May not be worth it for extremely small tasks (nanosecond-scale work).
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Complex Debugging
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Race conditions and deadlocks can be harder to debug than single-threaded or coarse-grained threading.
				</p>
			</li>
			<li>
				<p>
                    Job dependencies can create non-linear execution flows.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Potential for Starvation
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Poor job partitioning can lead to some threads being underutilized.
				</p>
			</li>
			<li>
				<p>
                    Long-running jobs may block others if not split properly.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Dependency Complexity
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    While dependencies are powerful, managing complex job graphs can become unwieldy.
				</p>
			</li>
			<li>
				<p>
                    Requires careful design to avoid bottlenecks.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Latency for Small Workloads
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    If job submission and scheduling latency is high, it may not be ideal for ultra-low-latency tasks.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="explanations" >
    Explanations
</h5>
<ul>
	<li>
		<p>
			<a
				href="https://www.gdcvault.com/play/1022186/Parallelizing-the-Naughty-Dog-Engine" 
				class="external-link" 
				target="_blank" >
                Multithreading with Fibers+Jobs in Naughty Dog engine
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    Fibers were highly praised, it was very appreciated.
				</p>
			</li>
			<li>
				<p>
					<em>
                        About libs
					</em>
                    :
				</p>
				<ul>
					<li>
						<p>
                            &quot;There are so few functions that I wouldn't bother with a library&quot;.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<em>
                        From what I understood of the strategy
					</em>
                    :
				</p>
				<ul>
					<li>
						<p>
                            Everything is transformed into a job and scheduled to run asynchronously across multiple threads.
						</p>
					</li>
					<li>
						<p>
                            This is handled automatically by the job system.
						</p>
					</li>
					<li>
						<p>
                            All new jobs added to the queue are executed completely spread out, out of order, and with some jobs in between.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<strong>
                        Setup
					</strong>
                    :
				</p>
				<ul>
					<li>
						<p>
                            6 worker threads, one for each CPU core.
						</p>
					</li>
					<li>
						<p>
                            160 fibers.
						</p>
					</li>
					<li>
						<p>
                            800-1000 jobs per frame in The Last of Us Remastered, at 60Hz.
						</p>
					</li>
					<li>
						<p>
                            3 global job queues.
						</p>
					</li>
					<li>
						<p>
                            <img src="assets/image_20250418185108.png" width="450" >
                            .
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<strong>
                        Jobs
					</strong>
                    :
				</p>
				<ul>
					<li>
						<p>
                            &quot;Jobfy the entire engine, everything needs to be a job&quot;.
						</p>
					</li>
					<li>
						<p>
                            EXCEPT I/O Threads:
						</p>
						<ul>
							<li>
								<p>
                                    sockets, file I/O, system calls, etc.
								</p>
							</li>
							<li>
								<p>
                                    &quot;There are system threads&quot;.
								</p>
							</li>
							<li>
								<p>
                                    &quot;Always waiting (sleeping) and never do expensive processing of the data&quot;.
								</p>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            <img src="assets/image_20250522164629.png" width="450" >
                            .
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<strong>
                        Fibers
					</strong>
                    :
				</p>
				<ul>
					<li>
						<p>
                            &quot;It's just a call stack&quot;.
						</p>
					</li>
					<li>
						<p>
							<em>
                                Pros
							</em>
                            :
						</p>
						<ul>
							<li>
								<p>
                                    Easy to implement in an existing system.
								</p>
							</li>
						</ul>
					</li>
					<li>
						<p>
							<em>
                                Cons
							</em>
                            :
						</p>
						<ul>
							<li>
								<p>
									<strong>
                                        &quot;System synchronization primitives can no longer be used&quot;
									</strong>
								</p>
								<ul>
									<li>
										<p>
                                            Mutex, semaphore, condition variables, etc.
										</p>
									</li>
									<li>
										<p>
                                            Locked to a particular thread. Fibers migrate between threads.
										</p>
									</li>
								</ul>
							</li>
							<li>
								<p>
                                    Synchronization has to be done at the hardware level.
								</p>
								<ul>
									<li>
										<p>
                                            Atomic spin locks are used almost everywhere.
										</p>
									</li>
									<li>
										<p>
                                            Special job mutex is used for locks held longer.
										</p>
										<ul>
											<li>
												<p>
                                                    Puts the current job to sleep if needed instead of spin lock.
												</p>
											</li>
										</ul>
									</li>
								</ul>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            Fibers and their call stacks are viewable in the debugger, just like threads.
						</p>
					</li>
					<li>
						<p>
                            Can be named/renamed.
						</p>
						<ul>
							<li>
								<p>
                                    Indicates the current job.
								</p>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            Crash handling
						</p>
						<ul>
							<li>
								<p>
                                    Fiber call stacks are saved in the core dumps just like threads.
								</p>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            &quot;Fiber-safe Thread Local Storage (TLS)&quot;.
						</p>
						<ul>
							<li>
								<p>
                                    Clang had issues with this in 2015.
								</p>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            Use adaptive mutexes in the job system.
						</p>
						<ul>
							<li>
								<p>
                                    Spin and attempt to grab lock before making a system call.
								</p>
							</li>
							<li>
								<p>
                                    Solves priority inversion deadlocks.
								</p>
							</li>
							<li>
								<p>
                                    Can avoid most system calls due to initial spin.
								</p>
							</li>
						</ul>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<strong>
                        FrameParams
					</strong>
                    :
				</p>
				<ul>
					<li>
						<p>
                            Data for each displayed frame.
						</p>
					</li>
					<li>
						<p>
                            Contains per-frame state:
						</p>
						<ul>
							<li>
								<p>
                                    Frame number.
								</p>
							</li>
							<li>
								<p>
                                    Delta time.
								</p>
							</li>
							<li>
								<p>
                                    Skinning matrices.
								</p>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            Entry point for each stage to access required data.
						</p>
					</li>
					<li>
						<p>
                            It's a non-disputed resource.
						</p>
					</li>
					<li>
						<p>
                            State variables are copied into this structure every frame:
						</p>
						<ul>
							<li>
								<p>
                                    Delta time.
								</p>
							</li>
							<li>
								<p>
                                    Camera position.
								</p>
							</li>
							<li>
								<p>
                                    Skinning matrices.
								</p>
							</li>
							<li>
								<p>
                                    List of meshes to render.
								</p>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            Stores start/end timestamps for each stage: game render, GPU, and flip.
						</p>
					</li>
					<li>
						<p>
                            <code>HasFrameCompleted(frameNumber)</code>
						</p>
					</li>
					<li>
						<p>
                            We have 16 FrameParams to check, but it's not 16 FrameParams worth of memory, as they are now freed.
						</p>
					</li>
					<li>
						<p>
                            &quot;The output of one frame is the input of the next&quot;.
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=v2Q_zHG3vqg" 
				class="external-link" 
				target="_blank" >
                Jobs in the Destiny engine
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    Written in C++.
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250418124555.png" width="325" >
                    .
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250418125339.png" width="325" >
                    .
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250418125515.png" width="325" >
                    .
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/screenshot_2025-05-22_132205.png" width="325" >
                    .
				</p>
			</li>
			<li>
				<p>
					<strong>
                        Jobs
					</strong>
                    :
				</p>
				<ul>
					<li>
						<p>
                            &quot;Jobs with ideal duration 500us - 2000us&quot;.
						</p>
					</li>
					<li>
						<p>
                            Priority-based FIFO
						</p>
						<ul>
							<li>
								<p>
                                    First In First Out.
								</p>
							</li>
						</ul>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    &quot;&quot;
					<strong>
                        FIBER
					</strong>
                    &quot;&quot;
				</p>
				<ul>
					<li>
						<p>
                            Set of jobs that always run serially and sequentially with an associated block of memory.
						</p>
					</li>
					<li>
						<p>
                            &quot;We have 3-4 fibers&quot;.
						</p>
					</li>
					<li>
						<p>
                            They are a convenience, as they are easier to understand than a mess of jobs.
						</p>
						<ul>
							<li>
								<p>
                                    &quot;Easier to describe&quot;.
								</p>
							</li>
						</ul>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<strong>
                        Resource
					</strong>
                    .
				</p>
				<ul>
					<li>
						<p>
                            Something that can be accessed.
						</p>
					</li>
					<li>
						<p>
                            Theoretical.
						</p>
					</li>
					<li>
						<p>
                            Havok World, player profile, AI data, etc.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<strong>
                        Policies
					</strong>
				</p>
				<ul>
					<li>
						<p>
							<strong>
                                &gt;&gt; Asserts &lt;&lt;
							</strong>
                            .
						</p>
						<ul>
							<li>
								<p>
                                    This was the biggest tip given.
								</p>
							</li>
							<li>
								<p>
                                    It's an insanely difficult job, and without asserts it would be impossible.
								</p>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            Describe what you can do with a resource.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Handles.
				</p>
				<ul>
					<li>
						<p>
                            [ ]
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<strong>
                        Resolving overlaps
					</strong>
                    :
				</p>
				<ul>
					<li>
						<p>
                            Add job dependencies.
						</p>
					</li>
					<li>
						<p>
                            Double buffer or cache data.
						</p>
					</li>
					<li>
						<p>
                            Queue messages to be acted upon later.
						</p>
					</li>
					<li>
						<p>
                            Restructure your algorithm.
						</p>
					</li>
					<li>
						<p>
                            {35:04 -&gt; 38:23}
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    &quot;I don't know if there were issues with the client-server architecture&quot;.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="job-system-in-unity" >
    Job System in Unity
</h5>
<ul>
	<li>
		<p>
			<a
				href="https://docs.unity3d.com/6000.1/Documentation/Manual/job-system-job-dependencies.html" 
				class="external-link" 
				target="_blank" >
                Explanation
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://docs.unity3d.com/6000.1/Documentation/ScriptReference/Unity.Jobs.IJob.html" 
				class="external-link" 
				target="_blank" >
                Interfaces
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250522112845.png" width="525" >
            .
		</p>
	</li>
</ul>
<h3
	id="thread-pools" >
    Thread Pools
</h3>
<h5
	id="tldr" >
    TLDR
</h5>
<ul>
	<li>
		<p>
            It's a 
			<strong>
                middle ground
			</strong>
            &nbsp;between Dedicated Threads and a Job System.
		</p>
	</li>
	<li>
		<p>
            I feel it's a &quot;soft&quot; version of a Job System.
		</p>
		<ul>
			<li>
				<p>
                    Job systems are not strictly better than a Thread Pool‚Äîthey‚Äôre more specialized.
				</p>
			</li>
			<li>
				<p>
                    Thread pools win for simplicity and low-latency tasks.
				</p>
			</li>
			<li>
				<p>
                    For most games, a hybrid approach is optimal.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="about" >
    About
</h5>
<ul>
	<li>
		<p>
            A collection of pre-instantiated reusable threads.
		</p>
	</li>
	<li>
		<p>
            A pool of generic threads that pull tasks from a shared queue.
		</p>
	</li>
	<li>
		<p>
            Reduces overhead from frequent thread creation/destruction.
		</p>
	</li>
	<li>
		<p>
            Thread pools are better than Dedicated Threads for:
		</p>
		<ul>
			<li>
				<p>
                    Bursty workloads (e.g., loading screens, asset decompression).
				</p>
			</li>
			<li>
				<p>
                    Scaling across CPU cores.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="how-it-works" >
    How It Works
</h5>
<ul>
	<li>
		<p>
            Tasks (arbitrary functions/lambdas) are pushed to a global queue.
		</p>
	</li>
	<li>
		<p>
            Idle threads pick up tasks first-come-first-served.
		</p>
	</li>
	<li>
		<p>
            No task dependencies (unlike job systems).
		</p>
	</li>
</ul>
<h5
	id="comparisons" >
    Comparisons
</h5>
<ul>
	<li>
		<p>
			<strong>
                Thread Pool
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    For simple fire-and-forget tasks (e.g., logging, file I/O).
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Job System
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    For complex, data-parallel workloads (e.g., physics, particle systems).
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Stick with Dedicated Threads
			</strong>
            &nbsp;if:
		</p>
		<ul>
			<li>
				<p>
                    Your game runs smoothly (no CPU bottlenecks).
				</p>
			</li>
			<li>
				<p>
                    You prioritize simplicity over peak performance.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Switch to Thread Pool
			</strong>
            &nbsp;if:
		</p>
		<ul>
			<li>
				<p>
                    You have ad-hoc parallel tasks (e.g., asset loading).
				</p>
			</li>
			<li>
				<p>
                    You want better CPU usage but 
					<em>
                        aren‚Äôt ready
					</em>
                    &nbsp;for jobs.
				</p>
			</li>
			<li>
				<p>
                    You need quick parallelism without refactoring.
				</p>
			</li>
			<li>
				<p>
                    Tasks are independent and coarse-grained.
				</p>
			</li>
			<li>
				<p>
                    You‚Äôre doing I/O or async operations.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Adopt a Job System
			</strong>
            &nbsp;if:
		</p>
		<ul>
			<li>
				<p>
                    Your game is CPU-bound and needs max core utilization (e.g., open-world, RTS).
				</p>
			</li>
			<li>
				<p>
                    You need fine-grained parallelism (e.g., 10,000 physics objects).
				</p>
			</li>
			<li>
				<p>
                    Work has complex dependencies.
				</p>
			</li>
			<li>
				<p>
                    You‚Äôre willing to restructure code.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="tbb-onetbb-threading-building-blocks" >
    TBB / oneTBB (Threading Building Blocks)
</h3>
<ul>
	<li>
		<p>
            It's a C++ Template Library.
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=2HCQWh75ooA" 
				class="external-link" 
				target="_blank" >
                Wiki
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://github.com/uxlfoundation/oneTBB" 
				class="external-link" 
				target="_blank" >
                oneTBB
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/onetbb.html" 
				class="external-link" 
				target="_blank" >
                About
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            Is a multithreading strategy (or more accurately, a parallel programming framework) rather than just an implementation detail of another strategy.
		</p>
	</li>
	<li>
		<p>
            Key Points About TBB:
		</p>
		<ol>
			<li>
				<p>
                    High-Level Parallelism Framework
				</p>
				<ul>
					<li>
						<p>
                            TBB provides a task-based parallel programming model, allowing developers to express parallelism without directly managing threads.
						</p>
					</li>
					<li>
						<p>
                            It abstracts low-level threading details (like thread creation and synchronization) and instead focuses on tasks scheduled across a thread pool.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Not Just an Implementation Detail
				</p>
				<ul>
					<li>
						<p>
                            While TBB does implement work-stealing schedulers and thread pools (which could be considered &quot;implementation details&quot;), it is primarily a strategy for parallelizing applications.
						</p>
					</li>
					<li>
						<p>
                            It competes with other parallelism approaches like OpenMP, raw 
                            <code>std::thread</code>
                            , or GPU-based parallelism (CUDA, SYCL).
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Key Features
				</p>
				<ul>
					<li>
						<p>
                            Task-based parallelism (instead of thread-based).
						</p>
					</li>
					<li>
						<p>
                            Work-stealing scheduler (for dynamic load balancing).
						</p>
					</li>
					<li>
						<p>
                            High-level parallel algorithms (
                            <code>parallelfor</code>
                            , 
                            <code>parallelreduce</code>
                            , 
                            <code>pipeline</code>
                            , etc.).
						</p>
					</li>
					<li>
						<p>
                            Concurrent containers (e.g., 
                            <code>concurrentqueue</code>
                            , 
                            <code>concurrenthashmap</code>
                            ).
						</p>
					</li>
					<li>
						<p>
                            Memory allocator optimizations 
                            <code>tbb::allocator</code>
                            &nbsp;for scalable memory allocation.
						</p>
					</li>
				</ul>
			</li>
		</ol>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=2HCQWh75ooA" 
				class="external-link" 
				target="_blank" >
                Video demo
			</a>
            .
		</p>
	</li>
</ul>
<h3
	id="data-parallelism" >
    Data Parallelism
</h3>
<ul>
	<li>
		<p>
            Same operation on different data chunks, e.g., OpenMP.
		</p>
	</li>
	<li>
		<p>
            Data Parallelism is a strategy, but SIMD (Single Instruction Multiple Data) is an implementation detail (a CPU feature enabling it).
		</p>
	</li>
</ul>
<h3
	id="actor-model" >
    Actor Model
</h3>
<ul>
	<li>
		<p>
            Independent actors messaging asynchronously.
		</p>
	</li>
</ul>
<h3
	id="pipeline-parallelism" >
    Pipeline Parallelism
</h3>
<ul>
	<li>
		<p>
            Split work into stages, like CPU instruction pipelines.
		</p>
	</li>
</ul>
<h2
	id="implementation-detail" >
    Implementation Detail
</h2>
<ul>
	<li>
		<p>
            Building blocks of strategies.
		</p>
	</li>
</ul>
<h3
	id="worker" >
    Worker
</h3>
<ul>
	<li>
		<p>
            &quot;A thread, fiber, or goroutine that executes jobs.&quot;
		</p>
	</li>
</ul>
<h3
	id="scheduler-and-task-queue" >
    Scheduler and Task Queue
</h3>
<ul>
	<li>
		<p>
            &quot;Decides which worker gets which task.&quot;
		</p>
	</li>
	<li>
		<p>
            Not needed for Dedicated Threads.
		</p>
	</li>
</ul>
<h5
	id="task-queue" >
    Task Queue
</h5>
<ul>
	<li>
		<p>
            &quot;Holds pending work (FIFO, priority-based, etc.).&quot;
		</p>
	</li>
</ul>
<h5
	id="work-stealing" >
    Work Stealing
</h5>
<ul>
	<li>
		<p>
            A scheduler optimization (workers steal tasks from others).
		</p>
	</li>
</ul>
<h3
	id="coroutine" >
    Coroutine
</h3>
<ul>
	<li>
		<p>
            A coroutine is a general programming construct that allows a function to suspend execution and resume later, preserving its state.
		</p>
	</li>
	<li>
		<p>
            Usually used in single-threaded environments.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Cooperative control
			</strong>
		</p>
		<ul>
			<li>
				<p>
                    A coroutine must explicitly yield control back to the caller or scheduler.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Resumable state
			</strong>
		</p>
		<ul>
			<li>
				<p>
                    Each coroutine has its own execution context and stack.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Use Case
			</strong>
		</p>
		<ul>
			<li>
				<p>
                    Asynchronous I/O
				</p>
			</li>
			<li>
				<p>
                    Event loops
				</p>
			</li>
			<li>
				<p>
                    Cooperative multitasking
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Examples
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Python (
                    <code>async/await</code>
                    , generators), Kotlin, Lua, C++, C#, JavaScript (
                    <code>async/await</code>
                    ), etc.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="types" >
    Types
</h5>
<ul>
	<li>
		<p>
			<strong>
                Stackless coroutines
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Use language support and compiler transformations (e.g., 
                    <code>async/await</code>
                    &nbsp;in JavaScript).
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Stackful coroutines
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Can suspend from deeper in the call stack (e.g., Lua, Boost.Coroutine in C++).
				</p>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="asynchronous-programming-async" >
    Asynchronous Programming (Async)
</h3>
<h5
	id="clarification" >
    Clarification
</h5>
<ul>
	<li>
		<p>
            Async Programming is a paradigm (like event-driven code), not strictly an &quot;implementation detail.&quot; It can be a strategy (e.g., Node.js‚Äôs event loop) or a tool (e.g., 
            <code>async/await</code>
            &nbsp;with threads). It's often implemented via callbacks/futures/coroutines.
		</p>
	</li>
	<li>
		<p>
            Async is a programming model that allows non-blocking execution of tasks using event loops and callbacks/futures.
		</p>
	</li>
	<li>
		<p>
			<em>
                Can be concurrent, but not inherently parallel
			</em>
            .
		</p>
	</li>
	<li>
		<p>
            Tasks can pause and resume without blocking threads.
		</p>
	</li>
	<li>
		<p>
            Optimized for I/O-bound workloads.
		</p>
	</li>
	<li>
		<p>
            Abstracts event-driven state machines.
		</p>
	</li>
</ul>
<h5
	id="examples" >
    Examples
</h5>
<ul>
	<li>
		<p>
            Sending an HTTP request without blocking the main thread while waiting for the response.
		</p>
	</li>
	<li>
		<p>
            One worker that 
			<em>
                starts a task
			</em>
            , and when waiting, 
			<em>
                does something else
			</em>
            &nbsp;instead of blocking.
		</p>
	</li>
</ul>
<h2
	id="system-synchronization-primitives" >
    System Synchronization Primitives
</h2>
<h5
	id="about" >
    About
</h5>
<ul>
	<li>
		<p>
            Synchronization primitives can come from multiple layers of abstraction‚Äîsome are provided directly by the operating system, while others are part of programming language libraries or runtime environments.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Operating System (OS) Level
			</strong>
		</p>
		<ul>
			<li>
				<p>
                    Most low-level synchronization primitives originate from the OS.
				</p>
			</li>
			<li>
				<p>
                    Provided through 
					<strong>
                        system calls
					</strong>
                    &nbsp;or 
					<strong>
                        kernel APIs
					</strong>
                    .
				</p>
			</li>
			<li>
				<p>
                    Examples:
				</p>
				<ul>
					<li>
						<p>
                            <code>pthread_mutex</code>
                            , 
                            <code>pthread_cond</code>
                            &nbsp;(POSIX Threads on Unix/Linux)
						</p>
					</li>
					<li>
						<p>
                            <code>WaitForSingleObject</code>
                            , 
                            <code>CreateSemaphore</code>
                            &nbsp;(Windows API)
						</p>
					</li>
					<li>
						<p>
                            <code>futex</code>
                            &nbsp;(Linux-specific syscall)
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Typically used in C/C++, or by language runtimes internally.
				</p>
			</li>
			<li>
				<p>
					<strong>
                        Responsibilities of the OS
					</strong>
                    :
				</p>
				<ul>
					<li>
						<p>
                            Manage thread scheduling.
						</p>
					</li>
					<li>
						<p>
                            Handle context switching and resource blocking.
						</p>
					</li>
					<li>
						<p>
                            Provide kernel support for locks, semaphores, etc.
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Programming Language Runtime
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Languages abstract OS primitives or implement their own strategies.
				</p>
			</li>
			<li>
				<p>
                    May use OS-level primitives under the hood or optimize further in user-space.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                User-Space Libraries
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Some libraries provide synchronization mechanisms independent of the OS.
				</p>
			</li>
			<li>
				<p>
                    Often optimized for performance (e.g., lock-free, wait-free algorithms).
				</p>
			</li>
			<li>
				<p>
                    Examples:
				</p>
				<ul>
					<li>
						<p>
                            Intel TBB (Threading Building Blocks)
						</p>
					</li>
					<li>
						<p>
                            Boost.Thread (C++)
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="double-buffering" >
    Double-Buffering
</h3>
<ul>
	<li>
		<p>
            It's a technique, not a synchronization primitive.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Use Case
			</strong>
            : Ideal when you have frequent reads and infrequent writes (e.g., rendering data, physics state).
		</p>
	</li>
	<li>
		<p>
			<strong>
                How It Works
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Maintain 
					<strong>
                        two buffers
					</strong>
                    : a 
					<strong>
                        front buffer
					</strong>
                    &nbsp;(for readers) and a 
					<strong>
                        back buffer
					</strong>
                    &nbsp;(for writers).
				</p>
			</li>
			<li>
				<p>
                    Readers always access the 
					<strong>
                        front buffer
					</strong>
                    &nbsp;(no locks needed).
				</p>
			</li>
			<li>
				<p>
                    Writers modify the 
					<strong>
                        back buffer
					</strong>
                    , then 
					<strong>
                        atomically swap
					</strong>
                    &nbsp;the buffers when done.
				</p>
			</li>
		</ul>
	</li>
</ul>
<pre><code class="language-cpp" data-lang="cpp">#include &lt;atomic&gt;
#include &lt;mutex&gt;

struct GameState {
&nbsp;&nbsp;&nbsp;&nbsp;int player_health;
&nbsp;&nbsp;&nbsp;&nbsp;float enemy_positions[1000];
&nbsp;&nbsp;&nbsp;&nbsp;// ... other game data
};

// Double-buffered data
std::atomic&lt;GameState*&gt; front_buffer;&nbsp;&nbsp;// Readers access this (lock-free)
GameState back_buffer;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // Writers modify this
std::mutex write_mutex;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // Protects back_buffer modifications

// Initialize
void Init() {
&nbsp;&nbsp;&nbsp;&nbsp;front_buffer.store(new GameState(), std::memory_order_relaxed);
}

// Read Job (Lock-Free)
void ReadJob() {
&nbsp;&nbsp;&nbsp;&nbsp;GameState* current = front_buffer.load(std::memory_order_acquire);
&nbsp;&nbsp;&nbsp;&nbsp;// Safely read from 'current' (no locks needed!)
&nbsp;&nbsp;&nbsp;&nbsp;int health = current-&gt;player_health;
&nbsp;&nbsp;&nbsp;&nbsp;// ...
}

// Write Job (Synchronized)
void WriteJob() {
&nbsp;&nbsp;&nbsp;&nbsp;std::lock_guard&lt;std::mutex&gt; lock(write_mutex);
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;// Modify the back buffer
&nbsp;&nbsp;&nbsp;&nbsp;back_buffer.player_health = 100;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;// Swap buffers (atomic, readers see the new state immediately)
&nbsp;&nbsp;&nbsp;&nbsp;GameState* old = front_buffer.exchange(&back_buffer, std::memory_order_release);
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;// Optional: Reuse 'old' for next write to avoid allocations
&nbsp;&nbsp;&nbsp;&nbsp;std::swap(back_buffer, *old);
}

// Cleanup
void Shutdown() {
&nbsp;&nbsp;&nbsp;&nbsp;delete front_buffer.load();
}
</code></pre>
<ul>
	<li>
		<p>
			<strong>
                Scenario: Writer Removes an Element in Double Buffering
			</strong>
		</p>
		<ol>
			<li>
				<p>
                    Initial State:
				</p>
				<ul>
					<li>
						<p>
                            <code>front_buffer</code>
                            &nbsp;‚Üí 
                            <code>[A, B, C, D]</code>
                            &nbsp;(readers see this)
						</p>
					</li>
					<li>
						<p>
                            <code>back_buffer</code>
                            &nbsp;‚Üí 
                            <code>[A, B, C, D]</code>
                            &nbsp;(writer modifies this)
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Writer Removes 
                    <code>C</code>
                    :
				</p>
				<ul>
					<li>
						<p>
                            Writer modifies 
                            <code>back_buffer</code>
                            &nbsp;‚Üí 
                            <code>[A, B, D]</code>
						</p>
					</li>
					<li>
						<p>
                            Writer swaps 
                            <code>frontbuffer</code>
                            &nbsp;and 
                            <code>backbuffer</code>
                            &nbsp;(now 
                            <code>frontbuffer</code>
                            &nbsp;points to 
                            <code>A, B, D</code>
                            ).
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Reader Access:
				</p>
				<ul>
					<li>
						<p>
                            Before Swap: A reader sees 
                            <code>A, B, C, D</code>
                            &nbsp;(old 
                            <code>frontbuffer</code>
                            ).
						</p>
					</li>
					<li>
						<p>
                            After Swap: New readers see 
                            <code>A, B, D</code>
                            &nbsp;(updated 
                            <code>frontbuffer</code>
                            ).
						</p>
					</li>
				</ul>
			</li>
		</ol>
		<ul>
			<li>
				<p>
					<strong>
                        What If a Reader Was Accessing 
                        <code>C</code>
                        &nbsp;When It Was Removed?
					</strong>
				</p>
				<ul>
					<li>
						<p>
                            If a reader was iterating over 
                            <code>frontbuffer</code>
                            &nbsp;(old buffer: 
                            <code>A, B, C, D</code>
                            ) while the swap happened, it still sees 
                            <code>C</code>
                            &nbsp;because:
						</p>
						<ul>
							<li>
								<p>
                                    The reader holds a pointer/reference to the old 
                                    <code>frontbuffer</code>
                                    &nbsp;(not affected by the swap).
								</p>
							</li>
							<li>
								<p>
                                    The writer‚Äôs changes only affect new readers (those accessing 
                                    <code>frontbuffer</code>
                                    &nbsp;after the swap).
								</p>
							</li>
						</ul>
					</li>
				</ul>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Is This Safe?
			</strong>
		</p>
		<ul>
			<li>
				<p>
                    Yes:
				</p>
				<ul>
					<li>
						<p>
                            Readers see a consistent snapshot (old buffer remains valid until they finish).
						</p>
					</li>
					<li>
						<p>
                            No data races (the swap is atomic, and old buffers aren‚Äôt modified).
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<strong>
                        BUT there are Memory Leak Risks
					</strong>
                    :
				</p>
				<ul>
					<li>
						<p>
                            If the old 
                            <code>front_buffer</code>
                            &nbsp;is discarded, readers holding references to it may access freed memory.
						</p>
					</li>
					<li>
						<p>
							<strong>
                                Strategy
							</strong>
                            :
						</p>
						<ul>
							<li>
								<p>
                                    Manage lifetimes collectively.
								</p>
								<ul>
									<li>
										<p>
                                            Only &quot;mark something as dead&quot; without freeing it.
										</p>
									</li>
									<li>
										<p>
                                            The free is done when no one is trying to read.
										</p>
									</li>
								</ul>
							</li>
						</ul>
					</li>
				</ul>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="mutex-mutual-exclusion" >
    Mutex (Mutual Exclusion)
</h3>
<ul>
	<li>
		<p>
            Ensures only one thread accesses a critical section at a time.
		</p>
	</li>
	<li>
		<p>
            If another thread tries to lock an already locked mutex, it blocks until the mutex is released.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Use case
			</strong>
            : Protect shared variables from concurrent modification.
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://yashdhadve.hashnode.dev/multi-threading-in-odin-lang#heading-handling-deadlock-using-mutex" 
				class="external-link" 
				target="_blank" >
                Preventing deadlock with mutex
			</a>
            .
		</p>
	</li>
</ul>
<h5
	id="states" >
    States
</h5>
<ul>
	<li>
		<p>
			<strong>
                unlocked
			</strong>
		</p>
		<ul>
			<li>
				<p>
                    Any thread can acquire it.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                locked
			</strong>
		</p>
		<ul>
			<li>
				<p>
                    Only the thread that acquired it can release it.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="problems-prevented-by-mutex" >
    Problems Prevented by Mutex
</h5>
<ul>
	<li>
		<p>
			<strong>
                Race conditions:
			</strong>
		</p>
		<ul>
			<li>
				<p>
                    When multiple threads access and modify a resource simultaneously in an unpredictable way.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Data inconsistency:
			</strong>
		</p>
		<ul>
			<li>
				<p>
                    Ensures that operations in a critical section are atomic.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="challenges-of-using-mutex" >
    Challenges of Using Mutex
</h5>
<ul>
	<li>
		<p>
			<strong>
                Deadlock:
			</strong>
		</p>
		<ul>
			<li>
				<p>
                    Occurs when two or more threads are stuck waiting for each other to release mutexes.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Starvation:
			</strong>
		</p>
		<ul>
			<li>
				<p>
                    A thread may wait indefinitely if mutexes are constantly acquired by others.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="what-a-mutex-actually-blocks" >
    What a Mutex Actually Blocks
</h5>
<ul>
	<li>
		<p>
            It blocks other threads from locking the same mutex.
		</p>
		<ul>
			<li>
				<p>
                    If Thread A locks 
                    <code>mutexx</code>
                    , Thread B will wait if it tries to lock 
                    <code>mutexx</code>
                    &nbsp;until Thread A unlocks it.
				</p>
			</li>
			<li>
				<p>
                    It does not block threads that don‚Äôt try to lock 
                    <code>mutexx</code>
                    .
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            It does not block the entire function or thread.
		</p>
		<ul>
			<li>
				<p>
                    The rest of your code (outside the mutex) runs normally.
				</p>
			</li>
			<li>
				<p>
                    Only the critical section (between 
                    <code>lock</code>
                    /
                    <code>unlock</code>
                    ) is protected.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            Mutexes Protect Data, Not Functions or Threads.
		</p>
<pre><code class="language-odin" data-lang="odin"> textures: map[u32]Texture2D
 textures_mutex: sync.Mutex
 
 // Thread A (locks mutex, modifies textures)
 sync.mutex_lock(&textures_mutex)&nbsp;&nbsp;// üîí
 textures[123] = load_texture("test.png")&nbsp;&nbsp;// Protected
 sync.mutex_unlock(&textures_mutex)&nbsp;&nbsp;// üîì
 
 // Thread B (also needs the mutex to touch textures)
 sync.mutex_lock(&textures_mutex)&nbsp;&nbsp;// ‚è≥ Waits if Thread A holds the lock
 unload_texture(textures[123])
 sync.mutex_unlock(&textures_mutex)
</code></pre>
		<ul>
			<li>
				<p>
                    The mutex only blocks Thread B if it tries to lock 
                    <code>textures_mutex</code>
                    &nbsp;while Thread A holds it.
				</p>
			</li>
			<li>
				<p>
                    If Thread B runs code that doesn‚Äôt lock 
                    <code>textures_mutex</code>
                    , it runs freely.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            Mutexes Are Not &quot;Global Barriers&quot;. If two threads use different mutexes, they don‚Äôt block each other:
		</p>
<pre><code class="language-odin" data-lang="odin"> // Thread A (uses mutex_x)
 sync.mutex_lock(&mutex_x)&nbsp;&nbsp;// üîí
 // ... do work ...
 sync.mutex_unlock(&mutex_x)&nbsp;&nbsp;// üîì
 
 // Thread B (uses mutex_y)
 sync.mutex_lock(&mutex_y)&nbsp;&nbsp;// ‚úÖ Runs in parallel (no conflict)
 // ... do work ...
 sync.mutex_unlock(&mutex_y)
</code></pre>
	</li>
</ul>
<h5
	id="example-task-queue" >
    Example: Task Queue
</h5>
<pre><code class="language-odin" data-lang="odin">// --- Shared Task System (Thread-safe) ---
Task_Type :: enum {
&nbsp;&nbsp;&nbsp;&nbsp;UNLOAD_TEXTURE,
&nbsp;&nbsp;&nbsp;&nbsp;LOAD_TEXTURE,
&nbsp;&nbsp;&nbsp;&nbsp;SWAP_SCENE_TEXTURES,
&nbsp;&nbsp;&nbsp;&nbsp;// Add more as needed...
}

Task :: struct {
&nbsp;&nbsp;&nbsp;&nbsp;type: Task_Type,
&nbsp;&nbsp;&nbsp;&nbsp;// Add payload if needed (e.g., texture IDs)
}

task_queue: [dynamic]Task
task_mutex: sync.Mutex

// Thread A (Game Thread): Processes tasks
process_tasks :: proc() {
&nbsp;&nbsp;&nbsp;&nbsp;// üîí LOCK the mutex to safely read the queue
&nbsp;&nbsp;&nbsp;&nbsp;sync.mutex_lock(&task_mutex)
&nbsp;&nbsp;&nbsp;&nbsp;defer sync.mutex_unlock(&task_mutex)&nbsp;&nbsp;// üîì Unlock when done

&nbsp;&nbsp;&nbsp;&nbsp;// Make a local copy of tasks (optional but cleaner)
&nbsp;&nbsp;&nbsp;&nbsp;tasks_to_process := task_queue[:]
&nbsp;&nbsp;&nbsp;&nbsp;clear(&task_queue)&nbsp;&nbsp;// Empty the shared queue
&nbsp;&nbsp;&nbsp;&nbsp;// üîì Mutex unlocked here (defer runs)

&nbsp;&nbsp;&nbsp;&nbsp;// Process tasks WITHOUT holding the mutex
&nbsp;&nbsp;&nbsp;&nbsp;for task in tasks_to_process {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;handle_task(task)&nbsp;&nbsp;// e.g., UnloadTexture(...)
&nbsp;&nbsp;&nbsp;&nbsp;}
}

// Thread B (Network Thread): Adds a task
add_task :: proc(new_task: Task) {
&nbsp;&nbsp;&nbsp;&nbsp;// üîí LOCK the mutex to safely append
&nbsp;&nbsp;&nbsp;&nbsp;sync.mutex_lock(&task_mutex)
&nbsp;&nbsp;&nbsp;&nbsp;defer sync.mutex_unlock(&task_mutex)&nbsp;&nbsp;// üîì Unlock when done

&nbsp;&nbsp;&nbsp;&nbsp;append(&task_queue, new_task)
}
</code></pre>
<h5
	id="example-read-and-write" >
    Example: Read and Write
</h5>
<ul>
	<li>
		<p>
            READ: MAKES A COPY (wrapped in a mutex), WRITE: MUTEX GUARD
		</p>
		<ul>
			<li>
				<p>
                    For read operations, make a copy of the data to read; for write operations, use a mutex.
				</p>
			</li>
		</ul>
	</li>
</ul>
<pre><code class="language-cpp" data-lang="cpp">// Shared data with mutex and atomic pointer for efficient reads
std::mutex data_mutex;
Data* atomic_data_ptr; // Atomic pointer for thread-safe access

// Read Job
void ReadJob() {
&nbsp;&nbsp;Data local_copy;
&nbsp;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;std::lock_guard&lt;std::mutex&gt; lock(data_mutex); // Optional: Only if data isn't atomically swappable
&nbsp;&nbsp;&nbsp;&nbsp;local_copy = *atomic_data_ptr; // Copy under mutex (or use atomic load)
&nbsp;&nbsp;}
&nbsp;&nbsp;// Use local_copy safely...
}

// Write Job
void WriteJob() {
&nbsp;&nbsp;std::lock_guard&lt;std::mutex&gt; lock(data_mutex);
&nbsp;&nbsp;// Modify data...
}
</code></pre>
<h5
	id="futex-fast-userspace-mutex-linux-specific" >
    Futex (Fast Userspace Mutex) (Linux-specific)
</h5>
<ul>
	<li>
		<p>
            Allows most lock/unlock operations to be done in userspace.
		</p>
	</li>
	<li>
		<p>
            System call is only used for contention resolution.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Use case
			</strong>
            : Efficient locking on Linux systems.
		</p>
	</li>
</ul>
<h3
	id="spinlock" >
    Spinlock
</h3>
<ul>
	<li>
		<p>
            Like a mutex, but instead of blocking, it repeatedly checks (spins) until the lock becomes available.
		</p>
	</li>
	<li>
		<p>
            Low overhead but inefficient if held for long.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Use case
			</strong>
            : Short critical sections on multiprocessor systems.
		</p>
	</li>
</ul>
<h3
	id="barrier" >
    Barrier
</h3>
<ul>
	<li>
		<p>
            Blocks a group of threads until all have reached the same point of execution.
		</p>
	</li>
	<li>
		<p>
            Once all threads arrive, they are released simultaneously.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Use case
			</strong>
            : Synchronize phases in parallel algorithms.
		</p>
	</li>
</ul>
<h3
	id="read-write-mutex-lock-rwlock-rw_mutex" >
    Read-Write Mutex/Lock (RWLock) (RW_Mutex)
</h3>
<ul>
	<li>
		<p>
            Allows 
			<strong>
                multiple readers
			</strong>
            &nbsp;or 
			<strong>
                one writer
			</strong>
            , but not both.
		</p>
	</li>
	<li>
		<p>
            Optimizes performance for read-heavy workloads.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Use case
			</strong>
            : Shared resource with frequent read, infrequent write.
		</p>
	</li>
</ul>
<h5
	id="read-write-mutex" >
    Read-Write Mutex
</h5>
<ul>
	<li>
		<p>
			<a
				href="https://pkg.odin-lang.org/core/sync/#RW_Mutex" 
				class="external-link" 
				target="_blank" >
                RW_Mutex
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            Multiple threads can read simultaneously (shared access).
		</p>
	</li>
	<li>
		<p>
            Only one thread can write at a time (exclusive access).
		</p>
	</li>
	<li>
		<p>
            Blocks new readers while a writer is waiting (to prevent starvation).
		</p>
	</li>
</ul>
<pre><code class="language-cpp" data-lang="cpp">#include &lt;shared_mutex&gt;

struct GameWorld {
&nbsp;&nbsp;&nbsp;&nbsp;std::unordered_map&lt;int, GameObject&gt; entities;
&nbsp;&nbsp;&nbsp;&nbsp;// ... other game data
};

GameWorld game_world;
std::shared_mutex rw_mutex;&nbsp;&nbsp;// Allows multiple readers or one writer

// Read Job (Shared Lock)
void ReadJob() {
&nbsp;&nbsp;&nbsp;&nbsp;std::shared_lock&lt;std::shared_mutex&gt; lock(rw_mutex); // Multiple threads can enter
&nbsp;&nbsp;&nbsp;&nbsp;// Safe to read 'game_world' here
&nbsp;&nbsp;&nbsp;&nbsp;auto it = game_world.entities.find(10);
&nbsp;&nbsp;&nbsp;&nbsp;if (it != game_world.entities.end()) {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;// Process entity...
&nbsp;&nbsp;&nbsp;&nbsp;}
}

// Write Job (Exclusive Lock)
void WriteJob() {
&nbsp;&nbsp;&nbsp;&nbsp;std::unique_lock&lt;std::shared_mutex&gt; lock(rw_mutex); // Only one writer allowed
&nbsp;&nbsp;&nbsp;&nbsp;// Modify 'game_world' safely
&nbsp;&nbsp;&nbsp;&nbsp;game_world.entities[20] = GameObject{...};
}
</code></pre>
<ul>
	<li>
		<p>
            RW_Mutex vs Double-Buffering
		</p>
		<ul>
			<li>
				<p>
                    <img src="assets/image_20250522145205.png" width="425" >
                    .
				</p>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="semaphore" >
    Semaphore
</h3>
<ul>
	<li>
		<p>
            A counter-based synchronization primitive.
		</p>
	</li>
	<li>
		<p>
            Can allow multiple threads to access a resource (unlike a mutex, which is binary).
		</p>
	</li>
	<li>
		<p>
            Types:
		</p>
		<ul>
			<li>
				<p>
					<strong>
                        Counting Semaphore
					</strong>
                    :
				</p>
				<ul>
					<li>
						<p>
                            Allows 
                            <code>n</code>
                            &nbsp;concurrent accesses.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<strong>
                        Binary Semaphore
					</strong>
                    :
				</p>
				<ul>
					<li>
						<p>
                            Functions like a mutex.
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Use case
			</strong>
            : Limit access to a pool of resources (e.g., connection pool).
		</p>
	</li>
</ul>
<h3
	id="condition-variable" >
    Condition Variable
</h3>
<ul>
	<li>
		<p>
            Allows threads to sleep and be awakened when a specific condition is true.
		</p>
	</li>
	<li>
		<p>
            Used with a mutex to wait for or signal a condition.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Use case
			</strong>
            : Producer-consumer problem.
		</p>
	</li>
</ul>
<h3
	id="atomic-operations" >
    Atomic Operations
</h3>
<ul>
	<li>
		<p>
            Low-level CPU-supported operations (e.g., 
            <code>atomic_add</code>
            , 
            <code>compare_and_swap</code>
            ) that complete indivisibly.
		</p>
	</li>
	<li>
		<p>
            No locking required.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Use case
			</strong>
            : Lightweight synchronization for counters, flags, etc.
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://blog.molecular-matters.com/2012/03/05/volatile-thread-synchronization/" 
				class="external-link" 
				target="_blank" >
                Avoid using Volatile in C++
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    I think this exemplifies a bit the necessity of using atomic operations.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h2
	id="networking" >
    Networking
</h2>
<h3
	id="strategies" >
    Strategies
</h3>
<ul>
	<li>
		<p>
            &quot;Is it better to create a thread for every new connection to the server, considering the connection will block thread execution? Or should I unblock the connection and let every pooling occur within the same thread? How often are both strategies used?&quot;
		</p>
	</li>
	<li>
		<p>
            Both strategies are common and have trade-offs. The best choice depends on scale, latency requirements, OS constraints, and language/runtime capabilities.
		</p>
	</li>
</ul>
<h5
	id="thread-per-connection" >
    Thread per Connection
</h5>
<ul>
	<li>
		<p>
            Each incoming client connection is handled by a dedicated OS thread.
		</p>
	</li>
	<li>
		<p>
            Blocking I/O operations are allowed (e.g., 
            <code>recv()</code>
            , 
            <code>send()</code>
            ).
		</p>
		<ul>
			<li>
				<p>
                    Thread sleeps when waiting for I/O and resumes when data is available.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Advantages
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Straightforward to implement and debug.
				</p>
			</li>
			<li>
				<p>
                    Each connection is isolated.
				</p>
			</li>
			<li>
				<p>
                    Uses OS APIs, so no need for complex event loops.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Disadvantages
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Scalability
				</p>
				<ul>
					<li>
						<p>
                            High memory usage
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Performance
				</p>
				<ul>
					<li>
						<p>
                            High context-switch overhead
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    OS limits
				</p>
				<ul>
					<li>
						<p>
                            OS-imposed thread limits
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Use cases
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Small-scale servers
				</p>
			</li>
			<li>
				<p>
                    Systems with &lt;1000 concurrent connections
				</p>
			</li>
			<li>
				<p>
                    Languages like Java or C++ where thread costs are acceptable
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="single-thread-non-blocking-i-o-with-multiplexing" >
    Single Thread + Non-Blocking I/O (with Multiplexing)
</h5>
<ul>
	<li>
		<p>
            All connections are handled on one or few threads.
		</p>
	</li>
	<li>
		<p>
            Use non-blocking sockets and a multiplexer (
            <code>epoll</code>
            , 
            <code>kqueue</code>
            , 
            <code>select</code>
            ).
		</p>
	</li>
	<li>
		<p>
            Manual polling or event-driven callback system processes I/O.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Advantages
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Scalability
				</p>
				<ul>
					<li>
						<p>
                            Supports tens of thousands of connections
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Efficiency
				</p>
				<ul>
					<li>
						<p>
                            Minimal context-switching
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Memory
				</p>
				<ul>
					<li>
						<p>
                            Lower per-connection memory overhead
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Disadvantages
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Complexity
				</p>
				<ul>
					<li>
						<p>
                            Manual state machines or async programming
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Error-prone
				</p>
				<ul>
					<li>
						<p>
                            Harder to debug, manage timeouts, etc.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Latency
				</p>
				<ul>
					<li>
						<p>
                            Single-threaded bottlenecks under high load
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Use cases
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    High-concurrency servers (e.g., web servers, proxies)
				</p>
			</li>
			<li>
				<p>
                    Event-driven platforms (e.g., Node.js, Nginx)
				</p>
			</li>
			<li>
				<p>
                    Languages with async runtimes (Rust/Tokio, Python/asyncio, Java NIO, etc.)
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="hybrid-models-thread-pools-non-blocking-i-o" >
    Hybrid Models (Thread Pools + Non-blocking I/O)
</h5>
<ul>
	<li>
		<p>
            Use a small thread pool (e.g., per-core or per-socket).
		</p>
	</li>
	<li>
		<p>
            Each thread runs an event loop handling multiple connections.
		</p>
	</li>
	<li>
		<p>
            Scales well and reduces the risk of single-thread saturation.
		</p>
	</li>
	<li>
		<p>
            Common in Go, Netty (Java), libuv (Node.js).
		</p>
	</li>
</ul>

					</div>
					<footer
						id="previous-next" >
						<a
							href="/docs/Things/Memory/Memory.html" >
                            &nbsp;&lsaquo; Previous
						</a>
						<a
							href="/docs/Things/RegEx.html" >
                            Next &rsaquo; 
						</a>
					</footer>
				</article>
			</main>
			<footer
				id="central-footer" >
                üßë‚Äçüíª built by and copyright
				<a
					href="https://github.com/caioraphael1" 
					target="_blank" >
                    Caio Raphael
				</a>
                üìÖ 2025-10-21 .&nbsp;&nbsp;2025-10-30 üöÄ
			</footer>
		</div>
		<aside
			id="right-sidebar" >
			<nav
				id="table-of-contents" >
				<strong>
                    On this page
				</strong>
				<ul>
					<li>
						<a
							href="#core-concepts" >
                            Core Concepts
						</a>
						<ul>
							<li>
								<a
									href="#process-vs-threads" >
                                    Process vs Threads
								</a>
							</li>
							<li>
								<a
									href="#multithreading" >
                                    Multithreading
								</a>
							</li>
							<li>
								<a
									href="#main-thread" >
                                    Main Thread
								</a>
							</li>
							<li>
								<a
									href="#resource" >
                                    Resource
								</a>
							</li>
							<li>
								<a
									href="#starvation" >
                                    Starvation
								</a>
							</li>
							<li>
								<a
									href="#race-conditions" >
                                    Race conditions
								</a>
							</li>
							<li>
								<a
									href="#deadlock" >
                                    Deadlock
								</a>
							</li>
							<li>
								<a
									href="#context-switching" >
                                    Context Switching
								</a>
							</li>
							<li>
								<a
									href="#parallelism" >
                                    Parallelism
								</a>
							</li>
							<li>
								<a
									href="#concurrency" >
                                    Concurrency
								</a>
							</li>
							<li>
								<a
									href="#sleeping" >
                                    Sleeping
								</a>
							</li>
						</ul>
					</li>
					<li>
						<a
							href="#thread" >
                            Thread
						</a>
						<ul>
							<li>
								<a
									href="#fibers" >
                                    Fibers
								</a>
							</li>
							<li>
								<a
									href="#green-thread" >
                                    Green Thread
								</a>
							</li>
							<li>
								<a
									href="#goroutine" >
                                    Goroutine
								</a>
							</li>
						</ul>
					</li>
					<li>
						<a
							href="#strategies" >
                            Strategies
						</a>
						<ul>
							<li>
								<a
									href="#dedicated-threads-static-partitioning-domain-specific-threads" >
                                    Dedicated Threads / Static Partitioning / Domain-Specific Threads
								</a>
							</li>
							<li>
								<a
									href="#job-system" >
                                    Job System
								</a>
							</li>
							<li>
								<a
									href="#thread-pools" >
                                    Thread Pools
								</a>
							</li>
							<li>
								<a
									href="#tbb-onetbb-threading-building-blocks" >
                                    TBB / oneTBB (Threading Building Blocks)
								</a>
							</li>
							<li>
								<a
									href="#data-parallelism" >
                                    Data Parallelism
								</a>
							</li>
							<li>
								<a
									href="#actor-model" >
                                    Actor Model
								</a>
							</li>
							<li>
								<a
									href="#pipeline-parallelism" >
                                    Pipeline Parallelism
								</a>
							</li>
						</ul>
					</li>
					<li>
						<a
							href="#implementation-detail" >
                            Implementation Detail
						</a>
						<ul>
							<li>
								<a
									href="#worker" >
                                    Worker
								</a>
							</li>
							<li>
								<a
									href="#scheduler-and-task-queue" >
                                    Scheduler and Task Queue
								</a>
							</li>
							<li>
								<a
									href="#coroutine" >
                                    Coroutine
								</a>
							</li>
							<li>
								<a
									href="#asynchronous-programming-async" >
                                    Asynchronous Programming (Async)
								</a>
							</li>
						</ul>
					</li>
					<li>
						<a
							href="#system-synchronization-primitives" >
                            System Synchronization Primitives
						</a>
						<ul>
							<li>
								<a
									href="#double-buffering" >
                                    Double-Buffering
								</a>
							</li>
							<li>
								<a
									href="#mutex-mutual-exclusion" >
                                    Mutex (Mutual Exclusion)
								</a>
							</li>
							<li>
								<a
									href="#spinlock" >
                                    Spinlock
								</a>
							</li>
							<li>
								<a
									href="#barrier" >
                                    Barrier
								</a>
							</li>
							<li>
								<a
									href="#read-write-mutex-lock-rwlock-rw_mutex" >
                                    Read-Write Mutex/Lock (RWLock) (RW_Mutex)
								</a>
							</li>
							<li>
								<a
									href="#semaphore" >
                                    Semaphore
								</a>
							</li>
							<li>
								<a
									href="#condition-variable" >
                                    Condition Variable
								</a>
							</li>
							<li>
								<a
									href="#atomic-operations" >
                                    Atomic Operations
								</a>
							</li>
						</ul>
					</li>
					<li>
						<a
							href="#networking" >
                            Networking
						</a>
						<ul>
							<li>
								<a
									href="#strategies" >
                                    Strategies
								</a>
							</li>
						</ul>
					</li>
				</ul>
			</nav>
		</aside>
		<script
			src="/static/docs.js" >
		</script>
	</body>
</html>
