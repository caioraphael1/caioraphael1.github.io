<!doctype html>
<html
	lang="en" >
	<head>
		<title>
            Caio Raphael
		</title>
		<meta
			charset="utf-8" >
		<meta
			name="viewport" 
			content="width=device-width, initial-scale=1" >
		<meta
			name="description" 
			content="Senior Game Developer, Engine Developer, Low-Level Network, Low-Level Systems" >
		<meta
			name="author" 
			content="Caio Raphael" >
		<meta
			name="theme-color" 
			content="#ffffff" 
			media="(prefers-color-scheme: light)" >
		<meta
			name="theme-color" 
			content="#101010" 
			media="(prefers-color-scheme: dark)" >
		<link
			rel="icon" 
			href="/assets/icon.ico" >
		<link
			rel="icon" 
			href="/assets/icon-16x16.png" 
			sizes="16x16" 
			type="image/png" >
		<link
			rel="icon" 
			href="/assets/icon-32x32.png" 
			sizes="32x32" 
			type="image/png" >
		<script>
window.MathJax = {
                tex: {
                    inlineMath: [['$', '$']],
                    displayMath: [['$$', '$$']]
                }
                };
		</script>
		<script
			src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" >
		</script>
		<script
			type="module" >

                    import hljs from 'https://unpkg.com/@highlightjs/cdn-assets@11.11.1/es/highlight.min.js';
                    import hljs_odin from 'https://unpkg.com/highlightjs-odinlang@1.4.0/dist/odin.es.min.js';
                    import hljs_glsl from 'https://unpkg.com/@highlightjs/cdn-assets@11.11.1/es/languages/glsl.min.js';
                    import hljs_swift  from 'https://unpkg.com/@highlightjs/cdn-assets@11.11.1/es/languages/swift.min.js';
                    hljs.registerLanguage('odin', hljs_odin);
                    hljs.registerLanguage('glsl', hljs_glsl);
                    hljs.registerLanguage('gdscript', hljs_swift);
                    hljs.highlightAll();
                
		</script>
		<link
			rel="stylesheet" 
			href="/static/studies.36380.css" >
	</head>
	<body>
		<aside
			id="left-sidebar" >
			<a
				href="/" 
				class="site-logo" >
                Caio Raphael
			</a>
			<nav>
				<details
>
					<summary>
                        Vulkan
					</summary>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Vulkan/starting.html" >
                                Starting
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Vulkan/tutorials.html" >
                                Tutorials
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Vulkan/samples.html" >
                                Samples
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Vulkan/core.html" >
                                Core
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Vulkan/render-loop.html" >
                                Render Loop
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Vulkan/synchronization-and-cache-control.html" >
                                Synchronization and Cache Control
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Vulkan/command-buffers.html" >
                                Command Buffers
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Vulkan/pipelines.html" >
                                Pipelines
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Vulkan/graphics-pipeline.html" >
                                Graphics Pipeline
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Vulkan/compute-pipeline.html" >
                                Compute Pipeline
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Vulkan/resources.html" >
                                Resources
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Vulkan/depth.html" >
                                Depth
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Vulkan/mapping-data-to-shaders.html" >
                                Mapping Data to Shaders
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Vulkan/memory-allocation.html" >
                                Memory Allocation
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Vulkan/hdr-support.html" >
                                HDR Support
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Vulkan/profiling.html" >
                                Profiling
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Vulkan/mobile.html" >
                                Mobile
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Vulkan/vr.html" >
                                VR
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Vulkan/video-decoding.html" >
                                Video Decoding
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Vulkan/spir-v.html" >
                                SPIR-V
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Vulkan/web.html" >
                                Web
							</a>
						</li>
					</ul>
				</details>
				<details
					open="">
					<summary>
                        Render Engineering
					</summary>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Render Engineering/apis.html" >
                                APIs
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Render Engineering/tools.html" >
                                Tools
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Render Engineering/choosing-the-space-to-compute-lighting.html" >
                                Choosing the Space to compute Lighting
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Render Engineering/bsdf-bidirectional-scattering-distribution-function.html" >
                                BSDF (Bidirectional Scattering Distribution Function)
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Render Engineering/material.html" >
                                Material
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Render Engineering/direct-lighting.html" >
                                Direct Lighting
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Render Engineering/shadows.html" >
                                Shadows
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Render Engineering/skybox-skydome.html" >
                                Skybox / Skydome
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Render Engineering/transparency.html" >
                                Transparency
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="active" 
								href="/studies/Graphics Programming/Render Engineering/global-illumination-indirect-lighting.html" >
                                Global Illumination / Indirect Lighting
							</a>
							<ul>
								<li>
									<a
										href="#terms" >
                                        Terms
									</a>
									<ul>
									</ul>
								</li>
								<li>
									<a
										href="#techniques" >
                                        Techniques
									</a>
									<ul>
										<li>
											<a
												href="#parameterizations-sampling-layouts-for-the-sphere" >
                                                Parameterizations / sampling layouts for the sphere
											</a>
										</li>
										<li>
											<a
												href="#directional-data-represent-or-approximate-functions-on-the-sphere-bases-or-parametric-lobes" >
                                                Directional Data (Represent or approximate functions on the sphere (bases or parametric lobes))
											</a>
										</li>
									</ul>
								</li>
								<li>
									<a
										href="#solutions" >
                                        Solutions
									</a>
									<ul>
									</ul>
								</li>
								<li>
									<a
										href="#discarded-solutions" >
                                        Discarded Solutions
									</a>
									<ul>
									</ul>
								</li>
								<li>
									<a
										href="#lightmaps" >
                                        Lightmaps
									</a>
									<ul>
									</ul>
								</li>
							</ul>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Render Engineering/post-processing.html" >
                                Post-Processing
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Render Engineering/anti-aliasing.html" >
                                Anti-Aliasing
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Render Engineering/light-path-rendering-method.html" >
                                Light Path / Rendering Method
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Render Engineering/optimization-techniques.html" >
                                Optimization Techniques
							</a>
						</li>
					</ul>
				</details>
				<details
>
					<summary>
                        Graphics and Shaders
					</summary>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Graphics and Shaders/sources.html" >
                                Sources
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Graphics and Shaders/math-linear-algebra.html" >
                                Math, Linear Algebra
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Graphics and Shaders/spaces-transformations-and-graphics-pipeline.html" >
                                Spaces, Transformations and Graphics Pipeline
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Graphics and Shaders/common-techniques.html" >
                                Common Techniques
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/Graphics and Shaders/shaders.html" >
                                Shaders
							</a>
						</li>
					</ul>
				</details>
				<details
>
					<summary>
                        GLSL
					</summary>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/GLSL/about.html" >
                                About
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/GLSL/basic.html" >
                                Basic
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/GLSL/storage-qualifiers.html" >
                                Storage Qualifiers
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/GLSL/layout-qualifiers.html" >
                                Layout Qualifiers
							</a>
						</li>
					</ul>
				</details>
				<details
>
					<summary>
                        GPU
					</summary>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/GPU/execution-building-blocks.html" >
                                Execution Building Blocks
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/GPU/specialized-units-and-instructions.html" >
                                Specialized units &amp; instructions
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/GPU/memory.html" >
                                Memory
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/GPU/cache.html" >
                                Cache
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/GPU/gpu-va-virtual-address.html" >
                                GPU VA (Virtual Address)
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/GPU/tiled-gpus.html" >
                                Tiled-GPUs
							</a>
						</li>
					</ul>
				</details>
				<details
>
					<summary>
                        Slang
					</summary>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/about.html" >
                                About
							</a>
						</li>
					</ul>
				</details>
				<details
>
					<summary>
                        Font Rendering
					</summary>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/techniques.html" >
                                Techniques
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/text-processing-pipeline.html" >
                                Text Processing Pipeline
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/concepts.html" >
                                Concepts
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/formats.html" >
                                Formats
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/libs.html" >
                                Libs
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/tools.html" >
                                Tools
							</a>
						</li>
					</ul>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/fonts.html" >
                                Fonts
							</a>
						</li>
					</ul>
				</details>
				<details
>
					<summary>
                        OpenGL
					</summary>
					<ul>
						<li>
							<a
								class="" 
								href="/studies/Graphics Programming/OpenGL/about.html" >
                                About
							</a>
						</li>
					</ul>
				</details>
			</nav>
		</aside>
		<div
			id="central-wrapper" >
			<a
				href="/" 
				class="icon-home" >

                <svg version="1.1" id="Capa_1" fill="currentColor" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 299.021 299.021" xml:space="preserve" style="color: whitesmoke;">
                    <g>
                        <g>
                            <path d="M292.866,254.432c-2.288,0-4.443-1.285-5.5-3.399c-0.354-0.684-28.541-52.949-146.169-54.727v51.977
                                c0,2.342-1.333,4.48-3.432,5.513c-2.096,1.033-4.594,0.793-6.461-0.63L2.417,154.392C0.898,153.227,0,151.425,0,149.516
                                c0-1.919,0.898-3.72,2.417-4.888l128.893-98.77c1.87-1.426,4.365-1.667,6.461-0.639c2.099,1.026,3.432,3.173,3.432,5.509v54.776
                                c3.111-0.198,7.164-0.37,11.947-0.37c43.861,0,145.871,13.952,145.871,143.136c0,2.858-1.964,5.344-4.75,5.993
                                C293.802,254.384,293.34,254.432,292.866,254.432z"></path>
                        </g>
                    </g>
                </svg>
                    
			</a>
			<main>
				<article
					id="note-article" >
					<header>
						<h1>
                            Global Illumination / Indirect Lighting
						</h1>
						<p>
							<time
								datetime="2025-07-03" >
                                üïí Created: 2025-07-03
							</time>
							<time
								datetime="2025-12-14" >
                                | Updated: 2025-12-14
							</time>
						</p>
					</header>
					<div
						id="note-content" >
<ul>
	<li>
		<p>
            A broad term that refers to any algorithm or technique that simulates how light bounces around a scene, not just directly from light sources but also after interactions with surfaces.
		</p>
	</li>
	<li>
		<p>
            It encompasses both 
			<strong>
                direct lighting
			</strong>
            &nbsp;(light coming straight from a source) and 
			<strong>
                indirect lighting
			</strong>
            &nbsp;(light that has bounced one or more times).
		</p>
	</li>
</ul>
<h3
	id="terms" >
    Terms
</h3>
<h5
	id="irradiance" >
    Irradiance
</h5>
<ul>
	<li>
		<p>
			<strong>
                Irradiance (scalar)
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    At a surface point, the integral of incoming radiance over the hemisphere (units: W/m¬≤).
				</p>
			</li>
			<li>
				<p>
                    Often the quantity of interest for diffuse shading.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Irradiance field
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    An Irradiance Field is the continuous phenomenon.
				</p>
			</li>
			<li>
				<p>
                    A function that maps spatial position (and sometimes orientation) to irradiance.
				</p>
			</li>
			<li>
				<p>
                    In papers this term can mean the true continuous field, or a specific continuous representation (e.g., a voxel grid, analytic basis, or neural field).
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Irradiance probes
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    An Irradiance Probe is a discrete measurement of that phenomenon.
				</p>
			</li>
			<li>
				<p>
                    A set of sampled measurements placed at discrete spatial locations.
				</p>
			</li>
			<li>
				<p>
                    Each probe stores an irradiance representation (examples: spherical-harmonic coefficients, a small cubemap, or directional coefficients).
				</p>
			</li>
			<li>
				<p>
                    During rendering the scene samples/interpolates between probes to approximate the irradiance at arbitrary points.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="techniques" >
    Techniques
</h3>
<ul>
	<li>
		<p>
			<input
				type="checkbox" 
				disabled=""
>
            
			<a
				href="https://holger.dammertz.org/stuff/notes_HammersleyOnHemisphere.html" 
				class="external-link" 
				target="_blank" >
                Hammersley Points on the Hemisphere
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<input
				type="checkbox" 
				disabled=""
>
            
            <img src="assets/image_2025-09-19_20-52-54.png" width="400" >
		</p>
		<ul>
			<li>
				<p>
                    Appeared in Surfels.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h4
	id="parameterizations-sampling-layouts-for-the-sphere" >
    Parameterizations / sampling layouts for the sphere
</h4>
<h5
	id="octahedral-maps" >
    Octahedral Maps
</h5>
<ul>
	<li>
		<p>
            A bijective mapping (with a fold) that encodes a unit 3D direction 
            <code>n = (x,y,z)</code>
            &nbsp;into 2D texture coordinates 
            <code>u,v ‚àà [0,1]</code>
            .
		</p>
	</li>
	<li>
		<p>
            Designed to pack the sphere into a single square texture with less angular distortion than latitude‚Äìlongitude and usually fewer wasted texels than cube maps.
		</p>
	</li>
	<li>
		<p>
            Commonly used to store normals, unit vectors (reflection vectors, tangent-space directions), or per-direction scalar/vector fields (e.g., an environment map sampled per texel).
		</p>
	</li>
</ul>
<h5
	id="cube-maps" >
    Cube Maps
</h5>
<ul>
	<li>
		<p>
			<a
				href="https://google.github.io/filament/Filament.html.html#annex/importancesamplingfortheibl" 
				class="internal-link" 
				target="_self" >
                Filament Importance Sampling for the IBL
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<em>
                Precomputing $L_{DFG}$
			</em>
            :
		</p>
		<ul>
			<li>
				<p>
                    The term $L_{DFG}$ is only dependent on $n.v$. Below, the normal is arbitrarily set to $n = [0, 0, 1]$ and $v$ is chosen to satisfy $n.v$. The vector $h_i$ is the $D_{GGX}(\alpha)$ important direction sample $i$.
				</p>
			</li>
		</ul>
<pre><code class="language-glsl" data-lang="glsl">float GDFG(float NoV, float NoL, float a) {
&nbsp;&nbsp;&nbsp;&nbsp;float a2 = a * a;
&nbsp;&nbsp;&nbsp;&nbsp;float GGXL = NoV * sqrt((-NoL * a2 + NoL) * NoL + a2);
&nbsp;&nbsp;&nbsp;&nbsp;float GGXV = NoL * sqrt((-NoV * a2 + NoV) * NoV + a2);
&nbsp;&nbsp;&nbsp;&nbsp;return (2 * NoL) / (GGXV + GGXL);
}

float2 DFG(float NoV, float a) {
&nbsp;&nbsp;&nbsp;&nbsp;float3 V;
&nbsp;&nbsp;&nbsp;&nbsp;V.x = sqrt(1.0f - NoV*NoV);
&nbsp;&nbsp;&nbsp;&nbsp;V.y = 0.0f;
&nbsp;&nbsp;&nbsp;&nbsp;V.z = NoV;

&nbsp;&nbsp;&nbsp;&nbsp;float2 r = 0.0f;
&nbsp;&nbsp;&nbsp;&nbsp;for (uint i = 0; i &lt; sampleCount; i++) {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float2 Xi = hammersley(i, sampleCount);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float3 H = importanceSampleGGX(Xi, a, N);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float3 L = 2.0f * dot(V, H) * H - V;

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float VoH = saturate(dot(V, H));
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float NoL = saturate(L.z);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float NoH = saturate(H.z);

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if (NoL &gt; 0.0f) {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float G = GDFG(NoV, NoL, a);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float Gv = G * VoH / NoH;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float Fc = pow(1 - VoH, 5.0f);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;r.x += Gv * (1 - Fc);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;r.y += Gv * Fc;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;return r * (1.0f / sampleCount);
}
</code></pre>
	</li>
	<li>
		<p>
			<strong>
                Filament Engine coordinates system
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    <img src="assets/image_20250922082120.png" width="400" >
                    .
				</p>
			</li>
			<li>
				<p>
                    To simplify the rendering of reflections, IBL cubemaps are stored mirrored on the X axis. This is the default behaviour of the 
                    <code>cmgen</code>
                    &nbsp;tool. This means that an IBL cubemap used as environment background needs to be mirrored again at runtime. An easy way to achieve this for skyboxes is to use textured back faces. Filament does this by default.
				</p>
			</li>
			<li>
				<p>
                    To convert equirectangular environment maps to horizontal/vertical cross cubemaps we position the +Z face in the center of the source rectilinear environment map.
				</p>
			</li>
			<li>
				<p>
                    When specifying a skybox or an IBL in Filament, the specified cubemap is oriented such that its -Z face points towards the +Z axis of the world (this is because filament assumes mirrored cubemaps). However, because environments and skyboxes are expected to be pre-mirrored, their -Z (back) face points towards the world's -Z axis as expected (and the camera looks toward that direction by default).
				</p>
			</li>
		</ul>
	</li>
</ul>
<h4
	id="directional-data-represent-or-approximate-functions-on-the-sphere-bases-or-parametric-lobes" >
    Directional Data (Represent or approximate functions on the sphere (bases or parametric lobes))
</h4>
<ul>
	<li>
		<p>
			<input
				type="checkbox" 
				disabled=""
>
            
			<a
				href="https://mynameismjp.wordpress.com/2016/10/09/sg-series-part-1-a-brief-and-incomplete-history-of-baked-lighting-representations/" 
				class="external-link" 
				target="_blank" >
                SG Series Part 1: A Brief (and Incomplete) History of Baked Lighting Representations
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    Sounds very interesting and relevant.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h6
	id="when-an-sh-sg-wavelet-can-replace-a-cubemap-oct-map-and-when-it-cannot" >
    When an SH/SG/wavelet can ‚Äúreplace‚Äù a cubemap/oct map ‚Äî and when it cannot
</h6>
<ul>
	<li>
		<p>
			<strong>
                Replace (acceptable / typical):
			</strong>
		</p>
		<ul>
			<li>
				<p>
                    Low-frequency lighting (diffuse irradiance): SH (low-order) is commonly used instead of sampling a full cubemap at runtime because SH compactly encodes low-frequency content and allows analytic convolution with Lambertian cosine. That avoids per-pixel cubemap lookups for diffuse IBL.
				</p>
			</li>
			<li>
				<p>
                    Compact specular approximation: Representing an environment by a small number of SG lobes can replace a cubemap for fast approximate glossy shading or importance sampling when the environment is lobe-like.
				</p>
			</li>
			<li>
				<p>
                    Compression / streaming: Wavelets (or hierarchical transforms) can replace a naive cubemap storage by providing a compressed, multiresolution representation that is progressively refinable.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Cannot fully replace (or will be lossy / expensive to use):
			</strong>
		</p>
		<ul>
			<li>
				<p>
                    Detailed, high-frequency reflections (sharp mirror-like): Full-resolution cubemap/oct textures or prefiltered mipmap chains (PMREM) are the usual approach. SH needs very high band count (many coefficients) to represent sharp features, and SG requires many lobes to approximate complex high-frequency structure ‚Äî both become expensive or inaccurate.
				</p>
			</li>
			<li>
				<p>
                    Arbitrary sampling and filtering: A texture parameterization (cubemap/oct) is a direct sampling representation and is straightforward to sample, filter (with caveats) and prefilter with hardware. Basis expansions require projection and reconstruction steps before sampling in the standard pipeline.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="haar-wavelets" >
    Haar Wavelets
</h5>
<ul>
	<li>
		<p>
            A family of localized, multiresolution basis functions (Haar is the simplest wavelet) that represent signals with coarse-to-fine detail. Haar wavelets are piecewise-constant, have compact support, and provide spatial locality and hierarchical decomposition.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Typical form (1D/2D idea):
			</strong>
            &nbsp;scaling functions and wavelet functions that split signal into averages + differences at successive scales. On the sphere one can build analogous spherical wavelet bases (e.g., via hierarchical partitioning of the sphere).
		</p>
	</li>
	<li>
		<p>
			<strong>
                Properties:
			</strong>
            &nbsp;spatially localized, multi-resolution (supports progressive refinement), good at representing localized/high-frequency features and discontinuities, many coefficients are zero or small for sparse signals, not globally smooth (Haar is discontinuous). Wavelet transforms can be fast (O(n)). Rotation is awkward for bases tied to a fixed partitioning.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Common graphics uses:
			</strong>
            &nbsp;compression and multi-resolution representation of environment maps or textures, adaptive shading, fast hierarchical importance sampling / level-of-detail, GPU-friendly encodings, and sparse approximations of signals with local sharp features.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Trade-offs:
			</strong>
            &nbsp;better for localized/high-frequency structure and compression; Haar specifically is low-order (blocky) unless higher-order wavelets are used; rotation and analytic convolution are not generally simple in the wavelet domain without additional structure.
		</p>
	</li>
</ul>
<h5
	id="spherical-gaussians-sg" >
    Spherical Gaussians (SG)
</h5>
<ul>
	<li>
		<p>
			<strong>
                What it is (short):
			</strong>
            &nbsp;parametric, localized ‚Äúlobe‚Äù functions on the sphere that approximate a single-peaked angular distribution (an axis-aligned Gaussian-like lobe).
		</p>
	</li>
	<li>
		<p>
			<strong>
                Typical form (common form):
			</strong>
            &nbsp;$G(\omega;\mu,\kappa)=\exp\big(\kappa(\mu\cdot\omega-1)\big)$, where $\mu$ is the lobe axis and $\kappa$ (or similar) controls concentration (larger $\kappa$ = narrower lobe). (Different papers use slightly different normalization/scales.)
		</p>
	</li>
	<li>
		<p>
			<strong>
                Properties:
			</strong>
            &nbsp;strongly localized, easily rotated by changing $\mu$, analytic approximations exist for products and convolutions with some BRDF lobes (useful approximations), compact parametric representation (axis + sharpness), cheap evaluation of a single lobe. Not an orthogonal basis. Multiple SGs can be summed to approximate complex lobes.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Common graphics uses:
			</strong>
            &nbsp;representing specular lobes and glossy reflections, analytic or semi-analytic shading and convolution approximations, importance sampling, fitting environment lighting with a sum of lobes for real-time shading. SGs are also used in prefiltered environment maps where single-lobe behavior is important.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Trade-offs:
			</strong>
            &nbsp;simple and efficient for localized lobes; approximating arbitrary functions requires many SGs; not linear-orthonormal (so projection/coefficients don‚Äôt have the same algebraic niceties as SH).
		</p>
	</li>
</ul>
<h5
	id="spherical-harmonics-sh" >
    Spherical Harmonics (SH)
</h5>
<ul>
	<li>
		<p>
			<a
				href="https://cseweb.ucsd.edu/~ravir/papers/envmap/envmap.pdf" 
				class="external-link" 
				target="_blank" >
                An Efficient Representation for Irradiance Environment Maps - Ramamoorthi &amp; Hanrahan - Siggraph 2001
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://cseweb.ucsd.edu/~ravir/papers/freqenv/freqenv.pdf" 
				class="external-link" 
				target="_blank" >
                Frequency Space Environment Map Rendering - Ramamoorthi &amp; Hanrahan- Siggraph 2002
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    &quot;Spherical harmonic reflection map (SHRM)&quot;.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://pdfs.semanticscholar.org/566f/c019459435ac3ad25d9a4941d5da02d7ba59.pdf" 
				class="external-link" 
				target="_blank" >
                Lighting and Material of Halo 3 - Siggraph 2008
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://media.gdcvault.com/gdc08/slides/S6220i1.pdf" 
				class="external-link" 
				target="_blank" >
                Lighting and Material of Halo 3 - Siggraph 2008
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    Bungie pioneered baked SH lighting in a shipped game with Halo 3 (Xbox 360, 2007). Halo 3‚Äôs engine precomputed ‚Äúlight probe‚Äù textures: each texel stored multiple SH coefficients (the slides cite 9‚Äì16 floats per texel) representing the incoming diffuse light at that point. These SH lightmaps were then sampled in shaders, so that static geometry received baked global illumination, and dynamic models could be lit by dot-producting their SH transfer vectors against the light-probe SH. (Siggraph 2008 ‚ÄúLighting and Materials of Halo 3‚Äù notes SH lightmaps as a natural extension of lightmaps.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://google.github.io/filament/Filament.html.html#annex/sphericalharmonics" 
				class="internal-link" 
				target="_self" >
                Filament - Spherical Harmonics
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    C++ implementation to compute a non-normalized SH basis:
				</p>
			</li>
		</ul>
<pre><code class="language-glsl" data-lang="glsl">static inline size_t SHindex(ssize_t m, size_t l) {
&nbsp;&nbsp;&nbsp;&nbsp;return l * (l + 1) + m;
}

void computeShBasis(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;double* const SHb,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;size_t numBands,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;const vec3& s)
{
&nbsp;&nbsp;&nbsp;&nbsp;// handle m=0 separately, since it produces only one coefficient
&nbsp;&nbsp;&nbsp;&nbsp;double Pml_2 = 0;
&nbsp;&nbsp;&nbsp;&nbsp;double Pml_1 = 1;
&nbsp;&nbsp;&nbsp;&nbsp;SHb[0] =&nbsp;&nbsp;Pml_1;
&nbsp;&nbsp;&nbsp;&nbsp;for (ssize_t l = 1; l &lt; numBands; l++) {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;double Pml = ((2 * l - 1) * Pml_1 * s.z - (l - 1) * Pml_2) / l;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Pml_2 = Pml_1;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Pml_1 = Pml;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SHb[SHindex(0, l)] = Pml;
&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;double Pmm = 1;
&nbsp;&nbsp;&nbsp;&nbsp;for (ssize_t m = 1; m &lt; numBands ; m++) {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Pmm = (1 - 2 * m) * Pmm;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;double Pml_2 = Pmm;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;double Pml_1 = (2 * m + 1)*Pmm*s.z;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;// l == m
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SHb[SHindex(-m, m)] = Pml_2;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SHb[SHindex( m, m)] = Pml_2;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if (m + 1 &lt; numBands) {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;// l == m+1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SHb[SHindex(-m, m + 1)] = Pml_1;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SHb[SHindex( m, m + 1)] = Pml_1;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for (ssize_t l = m + 2; l &lt; numBands; l++) {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;double Pml = ((2 * l - 1) * Pml_1 * s.z - (l + m - 1) * Pml_2)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/ (l - m);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Pml_2 = Pml_1;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Pml_1 = Pml;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SHb[SHindex(-m, l)] = Pml;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SHb[SHindex( m, l)] = Pml;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;double Cm = s.x;
&nbsp;&nbsp;&nbsp;&nbsp;double Sm = s.y;
&nbsp;&nbsp;&nbsp;&nbsp;for (ssize_t m = 1; m &lt;= numBands ; m++) {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for (ssize_t l = m; l &lt; numBands; l++) {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SHb[SHindex(-m, l)] *= Sm;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SHb[SHindex( m, l)] *= Cm;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;double Cm1 = Cm * s.x - Sm * s.y;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;double Sm1 = Sm * s.x + Cm * s.y;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Cm = Cm1;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sm = Sm1;
&nbsp;&nbsp;&nbsp;&nbsp;}
}
</code></pre>
		<ul>
			<li>
				<p>
                    C++ code to compute $\hat{C}_l$:
				</p>
			</li>
		</ul>
<pre><code class="language-glsl" data-lang="glsl">static double factorial(size_t n, size_t d = 1);

// &lt; cos(theta) &gt; SH coefficients pre-multiplied by 1 / K(0,l)
double computeTruncatedCosSh(size_t l) {
&nbsp;&nbsp;&nbsp;&nbsp;if (l == 0) {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return M_PI;
&nbsp;&nbsp;&nbsp;&nbsp;} else if (l == 1) {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return 2 * M_PI / 3;
&nbsp;&nbsp;&nbsp;&nbsp;} else if (l & 1) {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return 0;
&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;const size_t l_2 = l / 2;
&nbsp;&nbsp;&nbsp;&nbsp;double A0 = ((l_2 & 1) ? 1.0 : -1.0) / ((l + 2) * (l - 1));
&nbsp;&nbsp;&nbsp;&nbsp;double A1 = factorial(l, l_2) / (factorial(l_2) * (1 &lt;&lt; l));
&nbsp;&nbsp;&nbsp;&nbsp;return 2 * M_PI * A0 * A1;
}

// returns n! / d!
double factorial(size_t n, size_t d ) {
&nbsp;&nbsp; d = std::max(size_t(1), d);
&nbsp;&nbsp; n = std::max(size_t(1), n);
&nbsp;&nbsp; double r = 1.0;
&nbsp;&nbsp; if (n == d) {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // intentionally left blank
&nbsp;&nbsp; } else if (n &gt; d) {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for ( ; n&gt;d ; n--) {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; r *= n;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }
&nbsp;&nbsp; } else {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for ( ; d&gt;n ; d--) {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; r *= d;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; r = 1.0 / r;
&nbsp;&nbsp; }
&nbsp;&nbsp; return r;
}
</code></pre>
	</li>
	<li>
		<p>
            Basis functions $Y_{\ell}^m(\theta,\phi)$ indexed by degree $\ell\ge 0$ and order $-\ell\le m\le\ell$. A function $f(\omega)$ on the sphere is expanded as $f(\omega)=\sum_{\ell=0}^{L}\sum_{m=-\ell}^{\ell} c_{\ell m} Y_{\ell}^m(\omega)$.
		</p>
	</li>
	<li>
		<p>
            Are an orthonormal basis for functions on the sphere. In real-time lighting they are used to compactly represent low-frequency angular functions (environment illumination, visibility, convolution kernels).
		</p>
	</li>
	<li>
		<p>
            Irradiance probes commonly store SH coefficients so the renderer can approximate diffuse lighting quickly.
		</p>
	</li>
	<li>
		<p>
			<em>
                Why is it useful for Irradiance / Probes
			</em>
            :
		</p>
		<ul>
			<li>
				<p>
					<strong>
                        Compression
					</strong>
                    : low-frequency lighting (diffuse environment lighting, soft shadows) is well-approximated by a small number of SH bands (e.g. 9 coefficients).
				</p>
			</li>
			<li>
				<p>
					<strong>
                        Linear projection
					</strong>
                    : you can project an environment (cubemap, sampling) onto SH. Once you have certain coefficients‚Äã, you store those in a probe instead of storing a full cubemap.
				</p>
			</li>
			<li>
				<p>
					<strong>
                        Fast evaluation
					</strong>
                    : to evaluate the approximated radiance or irradiance at some direction $\omega$, evaluate the SH basis at $\omega$ and form the dot product with coefficients: $f(\omega)=\sum a_{l}^{m}Y_{l}^{m}(\omega)$. That dot product is inexpensive for small L.
				</p>
			</li>
			<li>
				<p>
					<strong>
                        Convolution property (why diffuse works well)
					</strong>
                    : for Lambertian reflection you need the cosine-weighted integral of incoming radiance. The cosine kernel is low-frequency and its SH representation has non-zero weight only on low bands. Convolving $L_i$‚Äã with the cosine kernel reduces to scaling SH bands by precomputed factors. Practically, this means you can compute irradiance from the projected SH coefficients cheaply without re-sampling the entire environment at render time.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=dDQTHFeJf5M" 
				class="external-link" 
				target="_blank" >
                Spherical Harmonics Demo
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    A demonstration of spherical harmonics used to construct the surface of the earth at increasing angular resolution.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/shorts/t06UTZAxCgw" 
				class="external-link" 
				target="_blank" >
                Spherical Harmonics Demo - Earth Magnetic Field
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=Ziz7t1HHwBw" 
				class="external-link" 
				target="_blank" >
                Spherical Harmonics Visualization
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    We describe the possible fundamental vibrations on a sphere in three dimensions by counting, mirroring and rotating nodal lines.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/shorts/IlvTHPClwHI" 
				class="external-link" 
				target="_blank" >
                Spherical Harmonics Demo
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    Idk, wtf.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=M--6_0F62pQ" 
				class="external-link" 
				target="_blank" >
                Spherical Harmonics in Quantum Mechanics
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    The video is cool.
				</p>
			</li>
			<li>
				<p>
                    The idea is to take this 1D visualization for the energy states of the electron:
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250919134538.png" width="350" >
                    &nbsp;
                    <img src="assets/image_20250919134703.png" width="200" >
				</p>
			</li>
			<li>
				<p>
                    And arrive at this 3D visualization:
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250919134409.png" width="400" >
                    .
				</p>
			</li>
			<li>
				<p>
                    All the visual characteristics of the visualization come from the ways in which the nodes can be represented.
				</p>
			</li>
			<li>
				<p>
                    Apparently nodes can be radial or angular. The angular representations appear when considering that the node can be a plane.
				</p>
			</li>
			<li>
				<p>
                    It is these nodes represented as a plane that make the shapes more interesting.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            The &quot;overlap&quot; I mentioned is not about spatial overlap. It's about mathematical independence.
		</p>
	</li>
	<li>
		<p>
            Spherical harmonics are precisely the spherical analogue of the Fourier transform.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Fourier Series (for a function on a circle):
			</strong>
            &nbsp;Any function on a 1D circle (like a sound wave over time) can be broken down into a sum of simple, orthogonal basis functions: sines and cosines of different frequencies (
            <code>sin(nŒ∏)</code>
            , 
            <code>cos(nŒ∏)</code>
            ).
		</p>
		<ul>
			<li>
				<p>
                    The low-frequency sines/cosines capture the broad, smooth shape.
				</p>
			</li>
			<li>
				<p>
                    The high-frequency ones capture the sharp details and edges.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Spherical Harmonics (for a function on a sphere):
			</strong>
            &nbsp;Any function on a 2D sphere (like an environment map) can be broken down into a sum of simple, orthogonal basis functions: the spherical harmonics (
            <code>Y_lm(Œ∏, œÜ)</code>
            ).
		</p>
		<ul>
			<li>
				<p>
                    The low-
                    <code>l</code>
                    &nbsp;(low-frequency) SHs capture the broad, smooth lighting (the average color, the dominant light direction).
				</p>
			</li>
			<li>
				<p>
                    The high-
                    <code>l</code>
                    &nbsp;(high-frequency) SHs capture the sharp details and edges (sharp reflections, tiny light sources).
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            We are not projecting a 3D scene onto a sphere.
		</p>
		<ul>
			<li>
				<p>
                    This is called environment map capture (e.g., taking a 360¬∞ photo). That gives us a function 
                    <code>f(Œ∏, œÜ)</code>
                    &nbsp;that is defined on the surface of a sphere.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            The Spherical Harmonic transform is the next step. We are taking that already-spherical function 
            <code>f(Œ∏, œÜ)</code>
            &nbsp;and projecting it onto the spherical harmonic basis functions.
		</p>
	</li>
	<li>
		<p>
            The word &quot;projection&quot; here is used in the linear algebra sense, just like projecting a vector onto a set of basis axes.
		</p>
	</li>
	<li>
		<p>
            Imagine your environment map 
            <code>f(œâ)</code>
            &nbsp;is a vector in a giant, infinite-dimensional space.
		</p>
	</li>
	<li>
		<p>
            The spherical harmonics 
            <code>Ylm(œâ)</code>
            &nbsp;form a complete set of orthonormal basis vectors for that space.
		</p>
	</li>
	<li>
		<p>
            Projecting 
            <code>f</code>
            &nbsp;onto a specific basis vector 
            <code>Ylm</code>
            &nbsp;is how we find the coefficient 
            <code>clm</code>
            &nbsp;for that function. The formula for this projection is the inner product:&nbsp;&nbsp;
            <code>c_lm = ‚à´ f(œâ) * Y_lm(œâ) dœâ</code>
            .
		</p>
	</li>
	<li>
		<p>
            This is identical to how in 3D space, you find the 
            <code>x</code>
            -component of a vector 
            <code>V</code>
            &nbsp;by projecting it onto the unitX basis vector: 
            <code>V_x = V ‚Ä¢ unitX</code>
            .
		</p>
	</li>
	<li>
		<p>
			<strong>
                &quot;Orthonormalization&quot;
			</strong>
            &nbsp;means we have ensured two things for our basis functions 
            <code>Y_lm</code>
            :
		</p>
		<ul>
			<li>
				<p>
					<strong>
                        Orthogonal:
					</strong>
                    &nbsp;
                    <code>‚à´ Y_lm(œâ) * Y_l'm'(œâ) dœâ = 0</code>
                    &nbsp;if 
                    <code>(l, m) ‚â† (l', m')</code>
                    &nbsp;(they are independent).
				</p>
			</li>
			<li>
				<p>
					<strong>
                        Normalized:
					</strong>
                    &nbsp;
                    <code>‚à´ Y_lm(œâ) * Y_lm(œâ) dœâ = 1</code>
                    &nbsp;(each one has a &quot;unit length&quot;).
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            This orthonormality is what makes the math so clean and the coefficients independent.
		</p>
	</li>
	<li>
		<p>
            In Quantum Mechanics, the spherical harmonics 
            <code>Y_lm</code>
            &nbsp;are famous because they are the 
			<strong>
                angular solutions to the Laplace equation in spherical coordinates.
			</strong>
		</p>
	</li>
	<li>
		<p>
            This describes the probability distribution of an electron around a hydrogen atom (its orbital). The shapes of the s, p, d, f orbitals are visualizations of the 
            <code>Y_lm</code>
            &nbsp;functions!
		</p>
		<ul>
			<li>
				<p>
                    <code>Y00</code>
                    &nbsp;is the s-orbital (spherical).
				</p>
			</li>
			<li>
				<p>
                    <code>Y1m</code>
                    &nbsp;are the three p-orbitals (dumbbell shaped along x, y, z).
				</p>
			</li>
			<li>
				<p>
                    <code>Y2m</code>
                    &nbsp;are the five d-orbitals (cloverleaf shapes).
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                In Computer Graphics:
			</strong>
            &nbsp;We are using the exact same mathematical functions to describe the distribution of light energy around a point. The &quot;orbital&quot; is now the &quot;irradiance environment.&quot;
		</p>
	</li>
	<li>
		<p>
            The key difference is one of interpretation:
		</p>
	</li>
	<li>
		<p>
            In Quantum Mechanics, you're solving for a wavefunction.
		</p>
	</li>
	<li>
		<p>
            In Computer Graphics, you're using the SH basis to compactly represent a function (light) defined on a sphere.
		</p>
	</li>
	<li>
		<p>
			<strong>
                The process for computer graphics
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    For a render engineer, the process is this:
				</p>
			</li>
		</ul>
		<ol>
			<li>
				<p>
					<strong>
                        Capture/Define:
					</strong>
                    &nbsp;Start with an environment map 
                    <code>f(œâ)</code>
                    . This is your function on the sphere.
				</p>
			</li>
			<li>
				<p>
					<strong>
                        Project (Precompute/Integrate):
					</strong>
                    &nbsp;For each SH basis function 
                    <code>Y_lm</code>
                    &nbsp;you care about (e.g., the first 9), compute the coefficient 
                    <code>c_lm</code>
                    &nbsp;by integrating the product 
                    <code>f(œâ) * Y_lm(œâ)</code>
                    &nbsp;over the entire sphere. This is the &quot;transformation&quot; into the SH frequency space.&nbsp;&nbsp;
                    <code>c_lm = ‚à´ f(œâ) * Y_lm(œâ) dœâ</code>
				</p>
			</li>
			<li>
				<p>
					<strong>
                        Reconstruct (Runtime):
					</strong>
                    &nbsp;To get the approximate value of the environment at any direction 
                    <code>œâ</code>
                    , you evaluate the sum:&nbsp;&nbsp;
                    <code>f(œâ) ‚âà Œ£ c_lm * Y_lm(œâ)</code>
                    &nbsp;for 
                    <code>l=0...N-1</code>
                    .
				</p>
			</li>
		</ol>
	</li>
	<li>
		<p>
            The reason this is a &quot;convolution&quot; is that the rendering equation often includes a cosine term (
            <code>n ‚Ä¢ œâ</code>
            ). The magic is that projecting this cosine lobe into SH space results in analytic attenuation factors (
            <code>A_l</code>
            ) that you can just multiply by your 
            <code>c_lm</code>
            &nbsp;coefficients 
			<em>
                before
			</em>
            &nbsp;the reconstruction step. This turns a complex integral into a simple dot product of two SH vectors (lighting coefficients and attenuated coefficients).
		</p>
	</li>
	<li>
		<p>
            So, you were right on all counts. It is a Fourier-style transform. It is a projection onto an orthonormal basis. And it uses the same elegant math as quantum physics to solve a seemingly unrelated problem in computer graphics with breathtaking efficiency.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Application
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
					<a
						href="https://www.youtube.com/watch?v=jN7FX5COASM" 
						class="external-link" 
						target="_blank" >
                        Spherical Harmonics Exponentials for Efficient Glossy Reflections
					</a>
                    .
				</p>
				<ul>
					<li>
						<p>
                            I watched a bit of the video, very technical and specific, about an optimization for computing glossiness.
						</p>
					</li>
					<li>
						<p>
                            <img src="assets/image_20250920105624.png" width="500" >
                            .
						</p>
						<ul>
							<li>
								<p>
                                    Ringing happens when you try using high order spherical harmonics.
								</p>
							</li>
						</ul>
					</li>
				</ul>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="solutions" >
    Solutions
</h3>
<ul>
	<li>
		<p>
			<input
				type="checkbox" 
				disabled=""
>
            
			<a
				href="https://www.youtube.com/watch?v=Qz0KTGYJtUk" 
				class="external-link" 
				target="_blank" >
                Coding Adventure: Ray Tracing
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<input
				type="checkbox" 
				disabled=""
>
            
			<a
				href="https://www.youtube.com/watch?v=C1H4zIiCOaI" 
				class="external-link" 
				target="_blank" >
                Coding Adventure: More Ray Tracing!
			</a>
            .
		</p>
	</li>
</ul>
<h5
	id="restir-gi" >
    ReSTIR GI
</h5>
<ul>
	<li>
		<p>
            ReSTIR GI is a spatio-temporal resampling algorithm for path/path-sample reuse (improving sampling of indirect lighting), i.e., a sampling/resampling approach for path/path-trace-based GI rather than a probe or voxel store.
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://research.nvidia.com/publication/2021-06_restir-gi-path-resampling-real-time-path-tracing" 
				class="external-link" 
				target="_blank" >
                ReSTIR GI
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=_1g-XhlI_5A" 
				class="external-link" 
				target="_blank" >
                ReSTIR GI Demo - Kajiya
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://github.com/EmbarkStudios/kajiya" 
				class="external-link" 
				target="_blank" >
                Kajiya
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://github.com/EmbarkStudios/kajiya/blob/main/docs/gi-overview.html" 
				class="internal-link" 
				target="_self" >
                ReSTIR GI - How it was implemented on Kajiya
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://cs.dartmouth.edu/~wjarosz/publications/bitterli20spatiotemporal.html" 
				class="external-link" 
				target="_blank" >
                Siggraph 2020 - Spatiotemporal reservoir resampling for real-time ray tracing with dynamic direct lighting
			</a>
            .
		</p>
	</li>
</ul>
<h5
	id="lumen" >
    Lumen
</h5>
<ul>
	<li>
		<p>
			<a
				href="https://dev.epicgames.com/documentation/en-us/unreal-engine/lumen-global-illumination-and-reflections-in-unreal-engine" 
				class="external-link" 
				target="_blank" >
                Lumen - Unreal Engine 5
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    We settled on Mesh Signed Distance Fields for our Software Ray Tracing geometry representation. These give reliable occlusion, all areas have coverage, and we still get fast software ray tracing through sphere tracing, which skips through empty space. The intersection with the distance field surface only gives us the hit position and normal, we can‚Äôt find the material attributes or the lighting.
				</p>
			</li>
			<li>
				<p>
                    We tried runtime voxelization and voxel cone tracing, but merging geometry properties into a volume causes lots of leaking, especially in the lower mip maps.
				</p>
			</li>
			<li>
				<p>
                    We also tried voxel bit bricks, where we stored 1 bit per voxel to mark whether it contains geometry or not. Simple ray marching of bit bricks was surprisingly slow and after adding a proximity map for acceleration, we just decided to drop voxels and arrived at a Global Distance Field
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=2GYXuM10riw" 
				class="external-link" 
				target="_blank" >
                Radiance Caching for Real-time Global Illumination - 2021
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    I watched the first 10 minutes of the video.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            Keywords:
		</p>
		<ul>
			<li>
				<p>
                    Downsample incoming radiance.
				</p>
			</li>
		</ul>
	</li>
	<li>
        <img src="assets/image_20250919103905.png" width="350" >

	</li>
	<li>
		<p>
            Probes:
		</p>
		<ul>
			<li>
				<p>
                    Octahedral atlas with border.
				</p>
			</li>
			<li>
				<p>
                    8x8 per probe, resulting in 64 traces per probe.
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250919104113.png" width="450" >
                    .
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250919104206.png" width="450" >
                    .
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250919104235.png" width="450" >
                    .
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="screen-space-radiance-caching" >
    Screen Space Radiance Caching
</h5>
<ul>
	<li>
		<ol>
			<li>
			</li>
		</ol>
	</li>
	<li>
		<p>
            UE5 - Instead of tracing for every single pixel on screen, we bundle up our rays and we trace from a much smaller set of pixels.
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=oJDonbFZbHk" 
				class="external-link" 
				target="_blank" >
                Two-Level Radiance Caching - AMD - GDC 2023
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    Instead tradeoff between pathtracing and probes.
				</p>
			</li>
			<li>
				<p>
                    Less noise with less samples, sounds like a good idea.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<input
				type="checkbox" 
				disabled=""
>
            
			<a
				href="https://www.youtube.com/watch?v=57F1ezwH7Mk" 
				class="external-link" 
				target="_blank" >
                Enshourded - 2024
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    SDF Rays.
				</p>
			</li>
			<li>
				<p>
                    Spatial Cascaded Cache.
				</p>
			</li>
			<li>
				<p>
                    Froxel Volumes.
				</p>
			</li>
			<li>
				<p>
                    Etc.
				</p>
			</li>
			<li>
				<p>
                    Intense.
				</p>
			</li>
			<li>
				<p>
                    I don't know if it is Forward+ or Deferred Rendering...
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="radiance-cascades-rc" >
    Radiance Cascades (RC)
</h5>
<ul>
	<li>
		<p>
			<input
				type="checkbox" 
				disabled=""
>
            
			<a
				href="https://www.youtube.com/@Alexander_Sannikov/videos" 
				class="external-link" 
				target="_blank" >
                Author's channel
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<input
				type="checkbox" 
				disabled=""
>
            
			<a
				href="https://drive.google.com/file/d/1L6v1_7HY2X-LV3Ofb6oyTIxgEaP4LOI6/view" 
				class="external-link" 
				target="_blank" >
                Radiance Cascades: A Novel Approach to Calculating Global Illumination - August 2023
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=TrHHTQqmAaM" 
				class="external-link" 
				target="_blank" >
                Radiance Cascades - ExileCon 2023
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://youtu.be/TrHHTQqmAaM?feature=shared&t=2246" 
				class="external-link" 
				target="_blank" >
                Implementation of Radiance Cascades in World Space, purely
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=3so7xdZHKxw" 
				class="external-link" 
				target="_blank" >
                Exploring Radiance Cascades; soft-shadows, etc
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=6O9-BUDk_-c" 
				class="external-link" 
				target="_blank" >
                2D Global Illumination with Radiance Cascades
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://x.com/mxacop/status/1822851233708732579" 
				class="external-link" 
				target="_blank" >
                3D Demo
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://github.com/Raikiri/LegitEngine" 
				class="external-link" 
				target="_blank" >
                Legit Engine
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    A rendergraph-based graphical framework for Vulkan, in C++.
				</p>
			</li>
			<li>
				<p>
                    The author of Radiance Cascades contributed to this engine.
				</p>
			</li>
			<li>
				<p>
                    Radiance Cascades is not implemented in this engine.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            They require a lot of compute and memory in 3D space.
		</p>
	</li>
	<li>
		<p>
            For 2D and screen-space 3D (like in Path of Exile 2) the amount of data you store is small enough that RC is an efficient solution, but it doesn't scale well for large 3D worlds.
		</p>
	</li>
	<li>
		<p>
            Path of Exile 2.
		</p>
	</li>
	<li>
		<p>
            Godot Discussion:
		</p>
		<ul>
			<li>
				<p>
                    Juan (against):
				</p>
				<ul>
					<li>
						<p>
                            Despite the hype, radiance cascades actually aren't all that practical, or at least aren't any more practical than competing techniques, in world space mode.
						</p>
					</li>
					<li>
						<p>
                            Radiance cascades shine in screen space mode (which is why they work excellently for Path of Exile), so they're less an alternative to Lumen/SDFGI and more an alternative to SSGI.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    fakhraldin (in favor):
				</p>
				<ul>
					<li>
						<p>
                            Suggested Radiance Cascades.
						</p>
					</li>
					<li>
						<p>
                            This GI solution is not restricted to 2D. Why should a probe based GI be restricted to only 2D in the first place? This doesn't make any sense for every graphics developer who got experience with probe based GI techniques, strange.
						</p>
					</li>
					<li>
						<p>
                            Radiance Cascades are not restricted to screen space but can be expanded to polygonal hardware ray tracing as well. Alexander clearly confirms this 
							<a
								href="https://youtu.be/bYTw3ISxgUw?feature=shared&t=74" 
								class="external-link" 
								target="_blank" >
                                here
							</a>
                            . And again it is strange to claim the opposite to every experienced graphics developer. In fact there are several released triple A titles, which use probe based GI with polygonal hardware ray tracing instead of screen space.
						</p>
					</li>
					<li>
						<p>
                            Screen Space is still being used even by triple A studios. Even Software Lumen itself does partially use SSGI for details of close objects among other techniques. Remember, Lumen is composed of several GI techniques. Million dollar game productions do rely on Screen Space. You don't come around using screen space for high frequency details in cross-gen graphics. Signed distance fields and voxel solutions are missing high frequency details.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Other people against:
				</p>
				<ul>
					<li>
						<p>
                            We both very explicitly say that yes, radiance cascades can be used in world space. Their limitations just don't make them a better or more practical alternative to SDFGI/HDDAGI. We both very explicitly say that radiance cascades could be a great alternative to SSGI.
						</p>
					</li>
					<li>
						<p>
                            I would like to see someone create a 3D radiance cascades solution (open-world would be even better). I'm a bit skeptical about its performance since, even in screen space, it's not that impressive compared to other screen space solutions, though it does look better.
						</p>
					</li>
					<li>
						<p>
                            Juan stated in his thread, open-world is actually radiance cascade's greatest weakness as a world space effect.
						</p>
					</li>
					<li>
						<p>
                            This is actually what makes radiance cascades (as a screen space effect) really good and why it's used in production in Path of Exile: the cost is the same as other SSGI approaches, but the quality is much higher.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Neither SDFGI nor HDDAGI are proven as practical solutions. SDFGI is being replaced by HDDAGI due to its inefficacy, and HDDAGI isn't released yet. Neither have effective parity with production-proven GI techniques and are ultimately experimental.
				</p>
			</li>
			<li>
				<p>
                    Take Avatar Frontiers of Pandora for example. Their technique is currently seen as the most sophisticated dynamic GI solution for cross-gen and next-gen realtime video games.
				</p>
			</li>
			<li>
				<p>
                    But it may surprise many people that their solution actually shares a similar ground concept of &quot;radiance cascades&quot;. They also use a probe based system. Instead of using radiance cascades, they combine different techniques like world space, screen space for high frequency details and ray tracing per hardware RT or compute shaders RT as a fallback. Similar to radiance cascades they implement additional different layers to achieve a wider spectrum of GI. They capture the world's details by &quot;different grades of detection&quot; so to speak.
				</p>
			</li>
			<li>
				<p>
                    The solution with &quot;radiance cascades&quot; is way less complicated, more performant and more scalable for hardware. Just like with SDFGI and HDDAGI we already use a probe grid. &quot;Radiance Cascades&quot; is just adding hierarchical probe grids with different resolutions to the existing one. This step increases detail capture and quality tremendously at cheap costs even with ray tracing.
				</p>
			</li>
			<li>
				<p>
                    We don't even need to make additional probe grids mandatory. It could be optional in the editor and even be offered as an in-game option. The more grid levels you can add, the more quality you can achieve according to your liking and machine. It is highly flexible.
				</p>
			</li>
			<li>
				<p>
                    From a technical standpoint i really don't see insurmountable objections against this solution, as it doesn't even interfere dramatically with the existing one. Rather it can serve as an additional, supportive and optional layer to the basic probe grid. If you don't want to apply it for world space, than don't do it. There are many another ways.
				</p>
			</li>
			<li>
				<p>
                    &quot;Radiance Cascades&quot; can be combined with world space and hardware rt or compute shaders rt to achieve similar results to ubisoft's GI solution, if not even better. Many features in godot turned out to be short-lived obsolete code. But i really don't see &quot;Radiance Cascades&quot; as such. It rather could serve as a basis for further development and options, which build upon it.
				</p>
			</li>
			<li>
				<p>
                    Our resources are limited and it would truly be a missed opportunity not to take advantage of this low-hanging fruit from which a great tree could grow.
				</p>
			</li>
			<li>
				<p>
                    World space radiance cascades has occlusion challenges (sound familiar?) and memory consumption challenges - just like pretty much all real time GI solutions.
				</p>
			</li>
			<li>
				<p>
                    However due to the specific nature of these challenges for radiance cascades, any practical implementation of world space radiance cascades will be limited to 3, at most 4, cascades.
				</p>
			</li>
			<li>
				<p>
                    What if you need GI for an open world map, for example? You might be able to make it work beyond that range, but most potential approaches are challenging to implement, and will likely be expensive to run.
				</p>
			</li>
			<li>
				<p>
                    For example, one potential approach is a cascade of radiance cascades. Literally running the entire thing multiple times at multiple sizes, and interpolating between them.
				</p>
			</li>
			<li>
				<p>
                    So, you might say: &quot;well maybe it can just be a higher quality short range alternative to SDFGI/HDDAGI?&quot; You may have some misconceptions about the level of quality of world space radiance cascades, compared to its much more impressive screen space counterpart.
				</p>
			</li>
			<li>
				<p>
                    Watch 
					<a
						href="https://www.youtube.com/watch?v=5Ua-h1pg6yM" 
						class="external-link" 
						target="_blank" >
                        this video that Sannikov himself posted
					</a>
                    . Does the blockiness and crawliness of the lighting look familiar to you? Does the volume representation look familiar to you? The quality tradeoffs of world space radiance cascades are very similar to the quality tradeoffs of SDFGI/HDDAGI. By replacing SDFGI/HDDAGI with world space radiance cascades, you are literally swapping it out with a technique that has practically the same quality, but much more limited range. You're gaining nothing, and losing something.
				</p>
			</li>
			<li>
				<p>
                    For screen space radiance cascades however, that's a different story. Pretty much everyone agrees it's incredible - probably the best screen space GI the industry has to offer right now, both in terms of performance and quality.
				</p>
			</li>
			<li>
				<p>
                    Author of Radiance Cascades, commenting on the HDDAGI PR:
				</p>
				<ul>
					<li>
						<p>
                            First, I never pitched 3d RC as the ultimate GI solution. I never even pitched it as a good GI solution. I wouldn't even call it practically viable by my standards, to be honest. I'm just saying that it's a direct improvement (in pretty much all parameters) over anything that has a regular grid (or nested grids) of radiance probes. DDGI for example.
						</p>
					</li>
					<li>
						<p>
                            Second, the screenspace version of RC is only limited to on-screen occluders and light sources if screenspace raymarching is used. However, screenspace cascades can store worldspace radiance intervals (including offscreen geometry) if you have a way of casting worldspace rays, using either a BVH, an SDM or a voxel raymarcher of some sort. The main limitation of this approach is that it only allows storing radiance on the surface of the depth buffer and you can't use it for example for volumetric lighting.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Juan apparently thought of an approach that's even better than HDDAGI, and will eventually work on that instead of continuing HDDAGI.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="dynamic-diffuse-global-illumination-ddgi-ray-traced-irradiance-fields" >
    Dynamic Diffuse Global Illumination (DDGI) / Ray-Traced Irradiance Fields
</h5>
<ul>
	<li>
		<p>
			<a
				href="https://www.gdcvault.com/play/1026182/" 
				class="external-link" 
				target="_blank" >
                &quot;Dynamic Diffuse Global Illumination with Ray-Traced Irradiance Fields&quot; - Majercik 2019
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
					<a
						href="https://www.youtube.com/watch?v=KufJBCTdn_o" 
						class="external-link" 
						target="_blank" >
                        Dynamic Diffuse Global Illumination with Ray-Traced Irradiance Fields - Majercik 2019
					</a>
                    .
				</p>
			</li>
			<li>
				<p>
                    Much of the talk is about:
				</p>
				<ul>
					<li>
						<p>
                            Fixing very common light leaks in techniques involving light probes.
						</p>
						<ul>
							<li>
								<p>
                                    <img src="assets/image_20250919095820.png" width="350" >
                                    .
								</p>
							</li>
							<li>
								<p>
                                    <img src="assets/image_20250919095845.png" width="350" >
                                    .
								</p>
							</li>
							<li>
								<p>
                                    <img src="assets/image_20250919095750.png" width="350" >
                                    .
								</p>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            Better automatic placing of Light Probes.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    &quot;Up to 256 rays per probe per frame, scaled down in some cases&quot;.
				</p>
			</li>
			<li>
				<p>
					<em>
                        Limitations
					</em>
                    :
				</p>
				<ul>
					<li>
						<p>
                            Don't sample on the surface, as it's really unstable.
						</p>
						<ul>
							<li>
								<p>
                                    1mm up and you are ok, 1mm down and you are inside the surface, where everything is black.
								</p>
							</li>
							<li>
								<p>
                                    Use a bias to improve this.
								</p>
							</li>
						</ul>
					</li>
				</ul>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjSgIuTlZ_vAhXxKX0KHeo-BNEQFjAEegQICRAD&url=http%3A%2F%2Fjcgt.org%2Fpublished%2F0008%2F02%2F01%2Fpaper-lowres.pdf&usg=AOvVaw2vJzwNmZPF7b5pjAf4Vtv8" 
				class="external-link" 
				target="_blank" >
                &quot;Dynamic Diffuse Global Illumination with Ray-Traced Irradiance Fields&quot; - Majercik 2019
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://jcgt.org/published/0010/02/01/paper-lowres.pdf" 
				class="external-link" 
				target="_blank" >
                &quot;Scaling Probe-Based Real-Time Dynamic Global Illumination for Production&quot; - Majercik 2021
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://cescg.org/wp-content/uploads/2022/04/Rohacek-Improving-Probes-in-Dynamic-Diffuse-Global-Illumination.pdf" 
				class="external-link" 
				target="_blank" >
                &quot;Improving Probes in Dynamic Diffuse Global Illumination&quot; - Rohacek 2022
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            DDGI isn't an alternative to RTXGI, but a way to implement it using light probes.
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=L1vhle74AEU" 
				class="external-link" 
				target="_blank" >
                Voxel engine with DDGI
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<strong>
                Probes
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Uses probe volumes (a set of points in space storing irradiance + visibility).
				</p>
			</li>
			<li>
				<p>
                    A probe typically holds an estimate of 
					<em>
                        irradiance
					</em>
                    &nbsp;or sometimes 
					<em>
                        radiance distribution
					</em>
                    &nbsp;directly (often SH coefficients).
				</p>
				<ul>
					<li>
						<p>
                            Comparing to PRT, the DDGI Probes represent the current lighting, not a precomputed transfer.
						</p>
					</li>
					<li>
						<p>
                            PRT probe = precomputed transfer.
						</p>
						<ul>
							<li>
								<p>
                                    ‚ÄúThis is how light at this spot responds to any lighting, given the static scene geometry.‚Äù
								</p>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            DDGI probe = runtime-sampled irradiance.
						</p>
						<ul>
							<li>
								<p>
                                    ‚ÄúThis is the actual indirect lighting at this spot right now, given whatever is in the scene.‚Äù
								</p>
							</li>
						</ul>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Probes store the incoming light in every single direction for rays passing through their center.
				</p>
			</li>
			<li>
				<p>
                    It may also store depth/visibility information (to reduce light leakage).
				</p>
			</li>
			<li>
				<p>
                    Each probe gathers lighting from the environment using ray tracing (usually hardware-accelerated).
				</p>
			</li>
			<li>
				<p>
                    Lighting between probes is interpolated at runtime.
				</p>
			</li>
			<li>
				<p>
                    When shading a point on a surface we sample the probes that are nearby, and blend for smooth lighting.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            Requires a set of probes, each storing low-frequency irradiance (spherical harmonics, octahedral maps, or similar).
		</p>
	</li>
	<li>
		<p>
            Probes can be updated progressively with ray tracing, amortizing cost across frames.
		</p>
	</li>
	<li>
		<p>
            Usually only practical for low-frequency GI (no fine detail).
		</p>
	</li>
	<li>
		<p>
            Bias samples for probes that just had changes in irradiance, like importance sampling, could speed convergence after edits.
		</p>
	</li>
</ul>
<h5
	id="screen-space-indirect-lighting-with-visibility-bitmask-vbao-ssilvb" >
    Screen Space Indirect Lighting with Visibility Bitmask (VBAO) (SSILVB)
</h5>
<ul>
	<li>
		<p>
            The technique is essentially an extension to ground-truth ambient occlusion (GTAO).
		</p>
	</li>
	<li>
		<p>
            Huge performance gain from GTAO.
		</p>
	</li>
	<li>
		<p>
            Quality visuals of near raytracing ambient occlusion.
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://arxiv.org/pdf/2301.11376" 
				class="external-link" 
				target="_blank" >
                Screen Space Indirect Lighting with Visibility Bitmask - 2023
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<input
				type="checkbox" 
				disabled=""
>
            
			<a
				href="https://cybereality.com/screen-space-indirect-lighting-with-visibility-bitmask-improvement-to-gtao-ssao-real-time-ambient-occlusion-algorithm-glsl-shader-implementation/" 
				class="external-link" 
				target="_blank" >
                SSAO vs GTAO vs SSILVB and implementation of SSILVB
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    I didn't read it properly.
				</p>
			</li>
			<li>
				<p>
                    The implementation is adapted from &quot;Screen Space Indirect Lighting with Visibility Bitmask&quot; by Olivier Therrien https://cdrinmatane.github.io/posts/cgspotlight-slides/.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=_RhsLkQRoi4" 
				class="external-link" 
				target="_blank" >
                SSILVB - Demo
			</a>
            .
		</p>
	</li>
</ul>
<h5
	id="horizon-based-indirect-lighting" >
    Horizon-Based Indirect Lighting
</h5>
<ul>
	<li>
		<p>
			<a
				href="https://github.com/Patapom/GodComplex/blob/master/Tests/TestHBIL/2018%20Mayaux%20-%20Horizon-Based%20Indirect%20Lighting%20(HBIL).pdf" 
				class="external-link" 
				target="_blank" >
                Horizon-Based Indirect Lighting - Benoit Patapom Mayoux 2018
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://github.com/Patapom/GodComplex/tree/master/Tests/TestHBIL" 
				class="external-link" 
				target="_blank" >
                HBIL Demo by Patapom (the author)
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            &quot;The ideal companion for your far-field indirect lighting solution&quot;.
		</p>
	</li>
	<li>
		<p>
            Treat the depth buffer as sort of a height field and march across that in slices.
		</p>
	</li>
	<li>
		<p>
            A horizon tells if new samples are hidden behind old samples.
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250918121447.png" width="300" >
            &nbsp;
            <img src="assets/image_20250918121534.png" width="300" >
            .
		</p>
	</li>
</ul>
<h5
	id="multi-scale-ambient-occlusion-msao" >
    Multi-Scale Ambient Occlusion (MSAO)
</h5>
<ul>
	<li>
		<p>
			<a
				href="https://www.comp.nus.edu.sg/~lowkl/publications/mssao_visual_computer_2012.pdf" 
				class="external-link" 
				target="_blank" >
                Efficient screen-space approach to high-quality multiscale ambient occlusion - 2012
			</a>
		</p>
	</li>
	<li>
		<p>
            &quot;I know HBAO+ is released but this method is far cheaper as you can see on that table in the end of the paper (They tested on GTX 460M! and it is 23ms average). I believe this method will give us far superior visuals than just SSAO.&quot;
		</p>
	</li>
	<li>
		<p>
			<strong>
                Wicked Engine 2024
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    This was my favourite SSAO so far because it handles large areas and small detail alike without any noise or temporal issues. It works by computing the AO in a deinterleaved version of the depth buffer that is contained in a Texture2DArray. It computes the AO in multiple resolutions, then upsamples and combines all of them into a final texture with bilateral blurring.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="screen-space-reflection-ssr" >
    Screen Space Reflection (SSR)
</h5>
<ul>
	<li>
		<p>
			<a
				href="https://josselinsomervilleroberts.github.io/papers/Report_INF584.pdf" 
				class="external-link" 
				target="_blank" >
                Screen Space Reflections - 
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            Used to capture reflections based on the rendered scene (using the previous frame for instance) by ray-marching in the depth buffer.
		</p>
	</li>
	<li>
		<p>
            SSR gives great results but can be very 
			<em>
                expensive
			</em>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://docs.godotengine.org/en/4.4/tutorials/3d/environment_and_post_processing.html#screen-space-reflections-ssr" 
				class="external-link" 
				target="_blank" >
                SSR - Godot
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=AHrG-rur3nQ" 
				class="external-link" 
				target="_blank" >
                SSR - Wicked Engine 2017 Demo
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    I implemented screen space reflections around two years ago but never showed it off so here you go.
				</p>
			</li>
			<li>
				<p>
                    This is the simplest technique that I know of. It is using binary search when raymarching in view space.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="distant-environment-probes-cube-map-ibl" >
    Distant Environment Probes / Cube Map IBL
</h5>
<ul>
	<li>
		<p>
			<a
				href="https://google.github.io/filament/Filament.html.html#lighting/imagebasedlights" 
				class="internal-link" 
				target="_self" >
                IBL with Cube Maps - Filament
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=yEkryaaAsBU" 
				class="external-link" 
				target="_blank" >
                Lightmaps, Ambient Color, IBL with CubeMaps, Probe-based Lighting
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            Used to capture lighting information at ‚Äúinfinity‚Äù, where parallax can be ignored. Distant probes typically contain the sky, distant landscape features or buildings, etc.
		</p>
	</li>
	<li>
		<p>
            The light is assumed to come from infinitely far away (which means every point on the object's surface uses the same environment map).
		</p>
	</li>
	<li>
		<p>
            They are either captured by the engine or acquired from a camera as high dynamic range images (HDRI).
		</p>
	</li>
	<li>
		<p>
			<strong>
                Irradiance
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    The whole environment contributes light to a given point on the object's surface.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Radiance
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    The resulting light bouncing off of the object.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            Incident lighting must be applied consistently to the diffuse and specular parts of the BRDF.
		</p>
	</li>
	<li>
		<p>
            Typically, the environment image is acquired offline in the real world, or generated by the engine either offline or at run time; either way, local or distant probes are used.
		</p>
	</li>
	<li>
		<p>
            Obviously the environment image must be acquired somehow and as we'll see below it needs to be pre-processed before it can be used for lighting.
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250911114146.png" width="400" >
            .
		</p>
	</li>
	<li>
		<p>
			<strong>
                Limitations
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Implementing a fully dynamic day/night cycle requires for instance to recompute the distant light probes dynamically.
				</p>
			</li>
			<li>
				<p>
                    As probes only capture basic color information and direction, very shiny surfaces are not very doable.
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250918205514.png" width="450" >
                    .
				</p>
			</li>
			<li>
				<p>
                    Far Cry 3 used probes. In that game you wouldn't find any shiny metallic object, as the probes cannot represent that type of lighting very easily.
				</p>
			</li>
			<li>
				<p>
                    IBL Cube Maps, on the other hand, could represent metals much better.
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250918205618.png" width="450" >
                    .
				</p>
			</li>
			<li>
				<p>
                    Images, in particular cubemaps, are a great way to encode such an ‚Äúenvironment light‚Äù.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Processing Light Probes
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    We saw previously that the radiance of an IBL is computed by integrating over the surface's hemisphere.
				</p>
			</li>
			<li>
				<p>
                    Since this would obviously be too expensive to do in real-time, we must first pre-process our light probes to convert them into a format better suited for real-time interactions.
				</p>
			</li>
			<li>
				<p>
                    The sections below will discuss the techniques used to accelerate the evaluation of light probes:
				</p>
				<ul>
					<li>
						<p>
							<strong>
                                Specular reflectance
							</strong>
                            :
						</p>
						<ul>
							<li>
								<p>
                                    pre-filtered importance sampling and split-sum approximation
								</p>
							</li>
						</ul>
					</li>
					<li>
						<p>
							<strong>
                                Diffuse reflectance
							</strong>
                            :
						</p>
						<ul>
							<li>
								<p>
                                    irradiance map and spherical harmonics
								</p>
							</li>
						</ul>
					</li>
				</ul>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Implementation (Filament)
			</strong>
            :
		</p>
<pre><code class="language-glsl" data-lang="glsl">vec3 irradianceSH(vec3 n) {
&nbsp;&nbsp;&nbsp;&nbsp;// uniform vec3 sphericalHarmonics[9]
&nbsp;&nbsp;&nbsp;&nbsp;// We can use only the first 2 bands for better performance
&nbsp;&nbsp;&nbsp;&nbsp;return
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sphericalHarmonics[0]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+ sphericalHarmonics[1] * (n.y)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+ sphericalHarmonics[2] * (n.z)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+ sphericalHarmonics[3] * (n.x)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+ sphericalHarmonics[4] * (n.y * n.x)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+ sphericalHarmonics[5] * (n.y * n.z)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+ sphericalHarmonics[6] * (3.0 * n.z * n.z - 1.0)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+ sphericalHarmonics[7] * (n.z * n.x)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+ sphericalHarmonics[8] * (n.x * n.x - n.y * n.y);
}

// NOTE: this is the DFG LUT implementation of the function above
vec2 prefilteredDFG_LUT(float coord, float NoV) {
&nbsp;&nbsp;&nbsp;&nbsp;// coord = sqrt(roughness), which is the mapping used by the
&nbsp;&nbsp;&nbsp;&nbsp;// IBL prefiltering code when computing the mipmaps
&nbsp;&nbsp;&nbsp;&nbsp;return textureLod(dfgLut, vec2(NoV, coord), 0.0).rg;
}

vec3 evaluateSpecularIBL(vec3 r, float perceptualRoughness) {
&nbsp;&nbsp;&nbsp;&nbsp;// This assumes a 256x256 cubemap, with 9 mip levels
&nbsp;&nbsp;&nbsp;&nbsp;float lod = 8.0 * perceptualRoughness;
&nbsp;&nbsp;&nbsp;&nbsp;// decodeEnvironmentMap() either decodes RGBM or is a no-op if the
&nbsp;&nbsp;&nbsp;&nbsp;// cubemap is stored in a float texture
&nbsp;&nbsp;&nbsp;&nbsp;return decodeEnvironmentMap(textureCubeLodEXT(environmentMap, r, lod));
}

vec3 evaluateIBL(vec3 n, vec3 v, vec3 diffuseColor, vec3 f0, vec3 f90, float perceptualRoughness) {
&nbsp;&nbsp;&nbsp;&nbsp;float NoV = max(dot(n, v), 0.0);
&nbsp;&nbsp;&nbsp;&nbsp;vec3 r = reflect(-v, n);

&nbsp;&nbsp;&nbsp;&nbsp;// Specular indirect
&nbsp;&nbsp;&nbsp;&nbsp;vec3 indirectSpecular = evaluateSpecularIBL(r, perceptualRoughness);
&nbsp;&nbsp;&nbsp;&nbsp;vec2 env = prefilteredDFG_LUT(perceptualRoughness, NoV);
&nbsp;&nbsp;&nbsp;&nbsp;vec3 specularColor = f0 * env.x + f90 * env.y;

&nbsp;&nbsp;&nbsp;&nbsp;// Diffuse indirect
&nbsp;&nbsp;&nbsp;&nbsp;// We multiply by the Lambertian BRDF to compute radiance from irradiance
&nbsp;&nbsp;&nbsp;&nbsp;// With the Disney BRDF we would have to remove the Fresnel term that
&nbsp;&nbsp;&nbsp;&nbsp;// depends on NoL (it would be rolled into the SH). The Lambertian BRDF
&nbsp;&nbsp;&nbsp;&nbsp;// can be baked directly in the SH to save a multiplication here
&nbsp;&nbsp;&nbsp;&nbsp;vec3 indirectDiffuse = max(irradianceSH(n), 0.0) * Fd_Lambert();


&nbsp;&nbsp;&nbsp;&nbsp;// Indirect contribution
&nbsp;&nbsp;&nbsp;&nbsp;return diffuseColor * indirectDiffuse + indirectSpecular * specularColor;
}
</code></pre>
	</li>
	<li>
		<p>
			<strong>
                Implementation (Vulkan-glTF-PBR)
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    The snippets below come from 
                    <code>material_pbr.frag</code>
				</p>
			</li>
		</ul>
<pre><code class="language-glsl" data-lang="glsl">// Calculation of the lighting contribution from an optional Image Based Light source.
// Precomputed Environment Maps are required uniform inputs and are computed as outlined in [1].
// See our README.md on Environment Maps [3] for additional discussion.
vec3 getIBLContribution(PBRInfo pbrInputs, vec3 n, vec3 reflection)
{
&nbsp;&nbsp;&nbsp;&nbsp;float lod = (pbrInputs.perceptualRoughness * uboParams.prefilteredCubeMipLevels);
&nbsp;&nbsp;&nbsp;&nbsp;// retrieve a scale and bias to F0. See [1], Figure 3
&nbsp;&nbsp;&nbsp;&nbsp;vec3 brdf = (texture(samplerBRDFLUT, vec2(pbrInputs.NdotV, 1.0 - pbrInputs.perceptualRoughness))).rgb;
&nbsp;&nbsp;&nbsp;&nbsp;vec3 diffuseLight = SRGBtoLINEAR(tonemap(texture(samplerIrradiance, n))).rgb;
&nbsp;&nbsp;&nbsp;&nbsp;vec3 specularLight = SRGBtoLINEAR(tonemap(textureLod(prefilteredMap, reflection, lod))).rgb;
&nbsp;&nbsp;&nbsp;&nbsp;vec3 diffuse = diffuseLight * pbrInputs.diffuseColor;
&nbsp;&nbsp;&nbsp;&nbsp;vec3 specular = specularLight * (pbrInputs.specularColor * brdf.x + brdf.y);
&nbsp;&nbsp;&nbsp;&nbsp;// For presentation, this allows us to disable IBL terms
&nbsp;&nbsp;&nbsp;&nbsp;// For presentation, this allows us to disable IBL terms
&nbsp;&nbsp;&nbsp;&nbsp;diffuse *= uboParams.scaleIBLAmbient;
&nbsp;&nbsp;&nbsp;&nbsp;specular *= uboParams.scaleIBLAmbient;
&nbsp;&nbsp;&nbsp;&nbsp;return diffuse + specular;
}
</code></pre>
		<ul>
			<li>
				<p>
                    Code where the IBL is calculated and used.
				</p>
			</li>
		</ul>
<pre><code class="language-glsl" data-lang="glsl">// Bla bla bla, basic lighting.
vec3 F = specularReflection(pbrInputs);
float G = geometricOcclusion(pbrInputs);
float D = microfacetDistribution(pbrInputs);
const vec3 u_LightColor = vec3(1.0);
vec3 diffuseContrib = (1.0 - F) * diffuse(pbrInputs);
vec3 specContrib = F * G * D / (4.0 * NdotL * NdotV);
vec3 color = NdotL * u_LightColor * (diffuseContrib + specContrib);

// Calculate lighting contribution from image based lighting source (IBL)
color += getIBLContribution(pbrInputs, n, reflection);
const float u_OcclusionStrength = 1.0f;

// -&gt; Ambient Occlusion
// Apply optional PBR terms for additional (optional) shading
if (material.occlusionTextureSet &gt; -1) {
&nbsp;&nbsp;&nbsp;&nbsp;float ao = texture(aoMap, (material.occlusionTextureSet == 0 ? inUV0 : inUV1)).r;
&nbsp;&nbsp;&nbsp;&nbsp;color = mix(color, color * ao, u_OcclusionStrength);
}

// Emissive
vec3 emissive = material.emissiveFactor.rgb * material.emissiveStrength;
if (material.emissiveTextureSet &gt; -1) {
&nbsp;&nbsp;&nbsp;&nbsp;emissive *= SRGBtoLINEAR(texture(emissiveMap, material.emissiveTextureSet == 0 ? inUV0 : inUV1)).rgb;
};
color += emissive;
outColor = vec4(color, baseColor.a);
</code></pre>
	</li>
</ul>
<h5
	id="reflection-probes-cube-map-reflections" >
    Reflection Probes / Cube Map Reflections
</h5>
<ul>
	<li>
		<p>
            Store environment 
			<em>
                radiance
			</em>
            &nbsp;(not just 
			<em>
                irradiance
			</em>
            ).
		</p>
	</li>
	<li>
		<p>
            Stores a cube map texture of the surroundings, representing light incoming from all directions.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Usage
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Used for specular reflections (environment mapping).
				</p>
			</li>
			<li>
				<p>
                    Mipmapped cube maps can be used with roughness filtering (for PBR specular).
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Good fits
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Games with a lot of metals, as that's where cube maps &quot;shine&quot; (literally).
				</p>
			</li>
			<li>
				<p>
                    Racing games are a good fit.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Limitations
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    They are static; if the scene changes, the cubemap needs to be regenerated, and that's expensive.
				</p>
			</li>
			<li>
				<p>
                    Doesn't handle self-reflections.
				</p>
				<ul>
					<li>
						<p>
                            <img src="assets/image_20250918204816.png" width="300" >
                            .
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://docs.godotengine.org/en/4.4/tutorials/3d/global_illumination/reflection_probes.html#doc-reflection-probes" 
				class="external-link" 
				target="_blank" >
                Reflection Probes - Godot
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    Reflection probes are used as a source of reflected and ambient light for objects inside their area of influence.
				</p>
			</li>
			<li>
				<p>
                    They can be used to provide more accurate reflections than VoxelGI and SDFGI while being fairly cheap on system resources.
				</p>
			</li>
			<li>
				<p>
                    Since reflection probes can also store ambient light, they can be used as a low-end alternative to VoxelGI and SDFGI when baked lightmaps aren't viable (e.g. in procedurally generated levels).
				</p>
			</li>
			<li>
				<p>
                    Good reflections, but poor indirect lighting.
				</p>
			</li>
			<li>
				<p>
                    Indirect lighting can be disabled, set to a constant color spread throughout the probe, or automatically read from the probe's environment (and applied as a cubemap). This essentially acts as local ambient lighting. Reflections and indirect lighting are blended with other nearby probes.
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250918153234.png" width="350" >
                    &nbsp;
                    <img src="assets/image_20250918153250.png" width="350" >
                    .
				</p>
			</li>
			<li>
				<p>
                    It interacts with LightmapGI:
				</p>
				<ul>
					<li>
						<p>
                            <img src="assets/image_20250918153218.png" width="350" >
                            .
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Reflection probes can also be used at the same time as SSR to provide reflections for off-screen objects.
				</p>
				<ul>
					<li>
						<p>
                            Godot will blend together the SSRs and reflections from reflection probes.
						</p>
					</li>
					<li>
						<p>
                            This way you can get the best of both worlds: high-quality reflections for general room structure (that remain present when off-screen), while also having real-time reflections for small details.
						</p>
					</li>
					<li>
						<p>
                            <img src="assets/image_20250918153004.png" width="350" >
                            .
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    To get reasonably accurate reflections, you should generally have one ReflectionProbe node per room (sometimes more for large rooms).
				</p>
			</li>
			<li>
				<p>
                    The extents don't have to be square, and you can even rotate the ReflectionProbe node to fit rooms that aren't aligned with the X/Z grid.
				</p>
				<ul>
					<li>
						<p>
                            Use this to your advantage to better cover rooms without having to place too many ReflectionProbe nodes.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<a
						href="https://docs.godotengine.org/en/4.4/tutorials/3d/global_illumination/reflection_probes.html#reflectionprobe-properties" 
						class="external-link" 
						target="_blank" >
                        Properties
					</a>
                    .
				</p>
			</li>
			<li>
				<p>
					<em>
                        Blending
					</em>
                    :
				</p>
				<ul>
					<li>
						<p>
                            To make transitions between reflection sources smoother, Godot supports automatic probe blending:
						</p>
						<ul>
							<li>
								<p>
                                    Up to 4 ReflectionProbes can be blended together at a given location. A ReflectionProbe will also fade out smoothly back to environment lighting when it isn't touching any other ReflectionProbe node.
								</p>
							</li>
							<li>
								<p>
                                    SDFGI and VoxelGI will blend in smoothly with ReflectionProbes if used. This allows placing ReflectionProbes strategically to get more accurate (or fully real-time) reflections where needed, while still having rough reflections available in the VoxelGI or SDFGI's area of influence.
								</p>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            To make several ReflectionProbes blend with each other, you need to have part of each ReflectionProbe overlap each other's area. The extents should only overlap as little as possible with other reflection probes to improve rendering performance (typically a few units in 3D space).
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<em>
                        Performance
					</em>
                    :
				</p>
				<ul>
					<li>
						<p>
                            ReflectionProbes with their update mode set to 
							<em>
                                Always
							</em>
                            &nbsp;are much more expensive than probes with their update mode set to 
							<em>
                                Once
							</em>
                            &nbsp;(the default). Suited for integrated graphics when using the Once update mode.
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
	<li>
		<p>
            Rotate the camera 6 times to render the 6 faces of the cube.
		</p>
	</li>
	<li>
		<p>
            You only render the plane, not the Tea Pot.
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250918101524.png" width="250" >
            &nbsp;
            <img src="assets/image_20250918101708.png" width="300" >
            .
		</p>
	</li>
	<li>
		<p>
            Instead of using the environment map loaded, you use the cubemap generated. So the reflection ON the Tea Pot only comes from this Cube Map.
		</p>
	</li>
	<li>
		<p>
            You should also use the Cube Map on the mirror image of the Tea Pot (made when using Planar Reflections).
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250918102206.png" width="350" >
            .
		</p>
		<ul>
			<li>
				<p>
                    The mirror image is clearer now.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            <img src="assets/image_20250918095737.png" width="400" >
            .
		</p>
	</li>
</ul>
<h5
	id="planar-reflections-flat-mirror" >
    Planar Reflections / Flat Mirror
</h5>
<ul>
	<li>
		<p>
            Used to capture reflections by rendering the scene mirrored by a plane. This technique works only for flat surfaces such as building floors, roads and water.
		</p>
	</li>
	<li>
		<p>
            Instead of reflecting the entire world, we reflect the camera; it's the same thing.
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250918101612.png" width="300" >
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://wickedengine.net/2024/12/wicked-engines-graphics-in-2024/" 
				class="external-link" 
				target="_blank" >
                Wicked Engine 2024
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    I‚Äôm disappointed whenever I see a modern game not supporting proper mirror reflections or if it has screen space reflection (SSR) on flat water. That‚Äôs why in Wicked Engine I‚Äôd like to show that planar reflection is still relevant and it should be one of the first choices of a game when a reflection needs to be rendered. The planar reflection is the perfect solution for a mirror because that‚Äôs what it was made for, and it‚Äôs good enough to use even on a large water surface with waves, like an ocean or a lake. Even though the waves are not totally accurate to be represented, it‚Äôs still a lot better than noisy SSR that cuts of abruptly.
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250919090214.png" width="450" >
                    .
				</p>
			</li>
			<li>
				<p>
                    In Wicked Engine, the planar reflections are using a second full depth prepass + color pass with all the forward rendering pipeline capabilities. Although most of the secondary effects are turned off for them, simply by not running those passes for planar reflections. Also they don‚Äôt generate visibility buffer, only depth buffer in the prepass. Planar reflections rendering is also scheduled in the frame asynchronously to the main camera‚Äôs compute effects, so there is also room to utilize the modern graphics API to render them. Compared to the main camera, planar reflections are rendered in quarter of the main camera resolution in both axes so they become less dependent on the pixel shader performance, but more geometry heavy, which helps a bit with async compute passes at the same time. To combat the low resolution look, I choose to render them at 4x MSAA right now for some additional anti-aliasing. Quarter resolution means that the resolution is 1/16 compared to the main camera, and adding 4x MSAA on top doesn‚Äôt bring back the full detail, but I found it quite nice, for now, although it can be tweaked easily if needed.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="discarded-solutions" >
    Discarded Solutions
</h3>
<h5
	id="voxel-cone-tracing-gi-vct-voxel-gi-sparse-voxel-octree-gi-svogi-voxel-traced-global-illumination-vtgi" >
    Voxel Cone Tracing GI (VCT) / Voxel GI / Sparse Voxel Octree GI (SVOGI) / Voxel Traced Global Illumination (VTGI)
</h5>
<ul>
	<li>
		<p>
			<input
				type="checkbox" 
				disabled=""
>
            
			<a
				href="https://research.nvidia.com/sites/default/files/pubs/2011-09_Interactive-Indirect-Illumination/GIVoxels-pg2011-authors.pdf" 
				class="external-link" 
				target="_blank" >
                Interactive Indirect Illumination Using Voxel Cone Tracing - 2011
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<input
				type="checkbox" 
				disabled=""
>
            
			<a
				href="https://onlinelibrary.wiley.com/doi/10.1111/cgf.15262" 
				class="external-link" 
				target="_blank" >
                Dynamic Voxel Based Global Illumination
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            Instead of using rays, we use voxel cones.
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250918110456.png" width="450" >
            &nbsp;
            <img src="assets/image_20250918110515.png" width="450" >
            .
		</p>
		<ul>
			<li>
				<p>
                    This is a high resolution of voxelization.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            Just like mipmaps, we generate lower and lower resolutions of this.
		</p>
	</li>
	<li>
		<p>
            And cones are used, where each cone accounts for a range of directions, instead of a single direction.
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250918110613.png" width="400" >
            .
		</p>
	</li>
	<li>
		<p>
            The cone tracing technique works best with a regular voxel grid because we perform ray-marching against the data like with screen space reflections for example.
		</p>
	</li>
	<li>
		<p>
            A regular voxel grid consumes more memory, but it is faster to create (voxelize), and more cache efficient to traverse (ray-march).
		</p>
	</li>
	<li>
		<p>
            The nice thing about this technique is that we can retrieve all sorts of effects. We have ‚Äúfree‚Äù ambient occlusion by default when doing this cone tracing, light bouncing, but we can retrieve reflections, refractions and shadows as well from this voxel structure with additional ray march steps. We can have a configurable amount of light bounces. Cone tracing code can be shared between the bouncing and querying shader and different types of rays as well. The entire thing remains fully on the GPU, the CPU is only responsible for command buffer generation.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Limitations / Drawbacks
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    The main drawback being the limited resolution.
				</p>
			</li>
			<li>
				<p>
                    Voxel GI gives plausible multi-bounce GI but costs in memory, bandwidth and costly voxelization/update;
				</p>
			</li>
			<li>
				<p>
                    The UE5 presentation does make a good argument against cone tracing for generic use (which unreal engine targets), there will always be some artifacts and those will be super bad in specific scenes.
				</p>
			</li>
			<li>
				<p>
                    Comparing to RTGI, voxel based is faster but less accurate and less dynamic.
				</p>
			</li>
			<li>
				<p>
                    Voxel-based lighting is only faster than ray traced global illumination solutions that favor quality over performance. RTGI solutions that prioritize performance are faster than Voxel cone tracing.
				</p>
			</li>
			<li>
				<p>
                    Indiana Jones runs at 1080 60FPS on Series S with RTGI and KCD2 runs at 1080P 30 FPS on Series S.
				</p>
			</li>
			<li>
				<p>
                    Voxel cone tracing is an obsolete technological dead end - too heavy to run on last-gen consoles and mobile, inferior to RT on modern desktop and console hardware.
				</p>
			</li>
			<li>
				<p>
                    KCD2 (Kingdom Come: Deliverance II) only uses it because CryEngine supports it but doesn‚Äôt support ray tracing. And CryEngine is still stuck using voxel lighting because the Star Citizen devs poached CryTek‚Äôs best graphics engineers, and those engineers then proceeded to add ray tracing to Star Citizen‚Äôs fork of CryEngine.
				</p>
			</li>
			<li>
				<p>
                    SVOGI is inaccurate as balls.
				</p>
			</li>
			<li>
				<p>
                    You must love light leaking through every room corner. It's cheap crappy raytracing and it looks like cheap crappy raytracing.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Implementation
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
					<a
						href="https://wickedengine.net/2017/08/voxel-based-global-illumination/" 
						class="external-link" 
						target="_blank" >
                        Wicked Engine Implementation
					</a>
                    .
				</p>
			</li>
			<li>
				<p>
                    First, we have our scene model with polygonal meshes. We need to convert it to a voxel representation. The voxel structure is a 3D texture which holds the direct illumination of the voxelized geometries in each pixel. There is an optional step here which I describe later. Once we have this, we can pre-integrate it by creating a mipmap chain for the resource. This is essential for cone tracing because we want to ray-march the texture with quadrilinear interpolation (sampling a 3D texture with min-mag-mip-linear filtering). We can then retrieve the bounced direct illumination in a final screen space cone tracing pass. The additional step in the middle is relevant if we want more bounces, because we can dispatch additional cone tracing compute shader passes for the whole structure (not in screen space).
				</p>
			</li>
			<li>
				<p>
					<strong>
                        1.) Voxelization
					</strong>
                    :
				</p>
				<ul>
					<li>
						<p>
                            The most involving part is definitely the first one, the voxelization step. It involves making use of advanced graphics API features like geometry shaders, abandoning the output merger and writing into resources ‚Äúby hand‚Äù. We can also make use of new hardware features like conservative rasterization and rasterizer ordered views, but we will implement them in the shaders as well.
						</p>
					</li>
					<li>
						<p>
                            The main trick to be able to run this in real time is that we need to parallelize the process well. For that, we will exploit the fixed function rasterization hardware, and we will get a pixel shader invocation for each voxel which will be rendered. We also do only a single render pass for every object.
						</p>
					</li>
					<li>
						<p>
                            We need to integrate the following pipeline to our scene rendering algorithm:
						</p>
					</li>
					<li>
						<p>
							<strong>
                                1.) Vertex shader
							</strong>
						</p>
					</li>
					<li>
						<p>
                            The voxelizing vertex shader needs to transform vertices into world space and pass through the attributes to the geometry shader stage. Or just do a pass through and transform to world space in the GS, doesn‚Äôt matter.
						</p>
					</li>
					<li>
						<p>
							<strong>
                                2.) Geometry shader
							</strong>
						</p>
					</li>
					<li>
						<p>
                            This will be responsible to select the best facing axis of each triangle received from the vertex shader. This is important because we want to voxelize each triangle once, on the axis it is best visible, otherwise we would get seams and bad looking results.
						</p>
					</li>
				</ul>
<pre><code class="language-glsl" data-lang="glsl">// select the greatest component of the face normal input[3] is the input array of three vertices
float3 facenormal = abs(input[0].nor + input[1].nor + input[2].nor);
uint maxi = facenormal[1] &gt; facenormal[0] ? 1 : 0;
maxi = facenormal[2] &gt; facenormal[maxi] ? 2 : maxi;
</code></pre>
				<ul>
					<li>
						<p>
                            After we determined the dominant axis, we need to project to it orthogonally by swizzling the position‚Äôs xyz components, then setting the z component to 1 and scaling it to clip space.
						</p>
					</li>
				</ul>
<pre><code class="language-glsl" data-lang="glsl">for (uint i = 0; i &lt; 3; ++i)
{
// voxel space pos:
&nbsp;&nbsp;&nbsp;&nbsp;output[i].pos = float4((input[i].pos.xyz - g_xWorld_VoxelRadianceDataCenter) / g_xWorld_VoxelRadianceDataSize, 1);
&nbsp;&nbsp;&nbsp;&nbsp;// Project onto dominant axis:
&nbsp;&nbsp;&nbsp;&nbsp;if (maxi == 0)
&nbsp;&nbsp;&nbsp;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output[i].pos.xyz = output[i].pos.zyx;
&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;else if (maxi == 1)
&nbsp;&nbsp;&nbsp;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output[i].pos.xyz = output[i].pos.xzy;
&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;// projected pos:
&nbsp;&nbsp;&nbsp;&nbsp;output[i].pos.xy /= g_xWorld_VoxelRadianceDataRes;
&nbsp;&nbsp;&nbsp;&nbsp;output[i].pos.z = 1;
&nbsp;&nbsp;&nbsp;&nbsp;output[i].N = input[i].nor;
&nbsp;&nbsp;&nbsp;&nbsp;output[i].tex = input[i].tex;
&nbsp;&nbsp;&nbsp;&nbsp;output[i].P = input[i].pos.xyz;
&nbsp;&nbsp;&nbsp;&nbsp;output[i].instanceColor = input[i].instanceColor;
}
</code></pre>
				<ul>
					<li>
						<p>
                            At the end, we could also expand our triangle a bit to be more conservative to avoid gaps. We could also just be setting a conservative rasterizer state if we have hardware support for it and avoid the expansion here.
						</p>
					</li>
				</ul>
<pre><code class="language-glsl" data-lang="glsl">
// Conservative Rasterization setup:
float2 side0N = normalize(output[1].pos.xy - output[0].pos.xy);
float2 side1N = normalize(output[2].pos.xy - output[1].pos.xy);
float2 side2N = normalize(output[0].pos.xy - output[2].pos.xy);
const float texelSize = 1.0f / g_xWorld_VoxelRadianceDataRes;
output[0].pos.xy += normalize(-side0N + side2N)*texelSize;
output[1].pos.xy += normalize(side0N - side1N)*texelSize;
output[2].pos.xy += normalize(side1N - side2N)*texelSize;
</code></pre>
				<ul>
					<li>
						<p>
                            It is important to pass the vertices‚Äô world position to the pixel shader, because we will use that directly to index into our voxel grid data structure and write into it. We will also need texture coords and normals for correct diffuse color and lighting.
						</p>
					</li>
					<li>
						<p>
							<strong>
                                3.) Pixel shader
							</strong>
						</p>
					</li>
					<li>
						<p>
                            After the geometry shader, the rasterizer unit schedules some pixel shader invocations for our voxels, so in the pixel shader we determine the color of the voxel and write it into our data structure. We probably need to sample our base texture of the surface and evaluate direct lighting which affects the fragment (the voxel). While evaluating the lighting, use a forward rendering approach, so iterate through the nearby lights for the fragment and do the light calculations for the diffuse part of the light. Leave the specular out of it, because we don‚Äôt care about the view dependent part now, we want to be able to query lighting from any direction anyway later. I recommend using a simplified lighting model, but try to keep it somewhat consistent with your main lighting model which is probably a physically based model (at least it is for me and you should also have one) and account for the energy loss caused by leaving out the specularity.
						</p>
					</li>
					<li>
						<p>
                            When you calculated the color of the voxel, write it out by using the following trick: I didn‚Äôt bind a render target for the render pass, but I have set an Unordered Access View by calling OMSetRenderTargetsAndUnorderedAccessViews(). So the shader returns nothing, but we write into our voxel grid in the shader code. My voxel grid is a RWStructuredBuffer here to be able to support atomic operations easily, but later it will be converted to a 3D texture for easier filtering and better cache utilization. The Structured buffer is a linear array of VoxelType of size gridDimensions X
							<em>
                                Y
							</em>
                            Z. VoxelType is a structure holding a 32 bit uint for the voxel color (packed HDR color with 0-255 RGB, an emissive multiplier in 7 bits and the last bit indicates if the voxel is empty or not). The structure also contains a normal vector packed into a uint. Our interpolated 3D world position comes in handy when determining the write position into the buffer, just truncate and flatten the interpolated world position which you received from the geometry shader. For writing the results, you must use atomic max operations on the voxel uints. You could be writing to a texture here without atomic operations, but using rasterizer ordered views, but they don‚Äôt support volume resources, so a multi pass approach would be necessary for the individual slices of the texture.
						</p>
					</li>
					<li>
						<p>
                            An additional note: If you have generated shadow maps, you can use them in your lighting calculations here to get more proper illumination when cone tracing. If you don‚Äôt have shadow maps, you can even use the voxel grid to retrieve (soft) shadow information for the scene later.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<strong>
                        2.) Filtering the data
					</strong>
                    :
				</p>
			</li>
			<li>
				<p>
					<strong>
                        3.) Cone Tracing
					</strong>
                    :
				</p>
				<ul>
					<li>
						<p>
                            We have the voxel scene ready for our needs, so let‚Äôs query it for information. To gather the global illumination for the scene, we have to run the cone tracing in screen space for every pixel on the screen once. This can happen in the forward rendering object shaders or against the gbuffer in a deferred renderer, when rendering a full screen quad, or in a compute shader. In forward rendering, we may lose some performance because of the worse thread utilization if we have many small triangles. A Z-prepass is an absolute must have if we are doing this in forward rendering. We don‚Äôt want to shade a pixel multiple times because this is a heavy computation.
						</p>
					</li>
					<li>
						<p>
                            For diffuse light bounces, we need the pixel‚Äôs surface normal and world position at minimum. From the world position, calculate the voxel grid coordinate, then shoot rays in the direction of the normal and around the normal in a hemisphere. But the ray should not start at the surface voxel, but the next voxel along the ray, so we don‚Äôt accumulate the current surface‚Äôs lighting. Begin ray marching, and each step sample your voxel from increasing mip levels, accumulate color and alpha and when alpha reaches 1, exit and divide the distance travelled. Do this for each ray, and in the end divide the accumulated result with the number of rays as well. Now you have light bounce information and ambient occlusion information as well, just add it to your diffuse light buffer.
						</p>
					</li>
					<li>
						<p>
                            Assembling the hemisphere: You can create a hemisphere on a surface by using a static array of precomputed randomized positions on a sphere and the surface normal. First, if you do a 
							<em>
                                reflect(surfaceNormal, randomPointOnSphere),
							</em>
                            &nbsp;you get a random point on a sphere with variance added by the normal vector. This helps with banding as discrete precomputed points get modulated by surface normal. We still have a sphere, but we want the upper half of it, so check if a point goes below the ‚Äúhorizon‚Äù and force it to go to the other direction if it does:
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://docs.godotengine.org/en/4.4/tutorials/3d/global_illumination/using_voxel_gi.html" 
				class="external-link" 
				target="_blank" >
                VoxelGI - Godot
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    Good reflections and indirect lighting, but beware of leaks.
				</p>
			</li>
			<li>
				<p>
                    Due to its voxel-based nature, VoxelGI will exhibit light leaks if walls and floors are too thin. It's recommended to make sure all solid surfaces are at least as thick as one voxel.
				</p>
			</li>
			<li>
				<p>
                    Streaking artifacts may also be visible on sloped surfaces. In this case, tweaking the bias properties or rotating the VoxelGI node can help combat this.
				</p>
			</li>
			<li>
				<p>
					<em>
                        Performance
					</em>
                    :
				</p>
				<ul>
					<li>
						<p>
                            The bake's number of subdivisions can be adjusted to balance between performance and quality. The VoxelGI rendering quality can be adjusted in the Project Settings. The rendering can optionally be performed at half resolution (and then linearly scaled) to improve performance significantly.
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://www.cryengine.com/docs/static/engines/cryengine-5/categories/23756816/pages/25535599" 
				class="external-link" 
				target="_blank" >
                SVOGI - CryEngine
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=dyH0W-AVqu8" 
				class="external-link" 
				target="_blank" >
                SVOGI - Demo in Kingdom Come: Deliverance
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    The voxel based approach from the &quot;old&quot; CryEngine 3 was fascinating and it's a real shame very few games effectively supported it.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=4-KSMRjUqGU" 
				class="external-link" 
				target="_blank" >
                Voxel Cone Tracing GI - Demo 2011
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    I think it's actually viable for a commercial project. At very least as a fallback for RTX (where it's unsupported or inefficient). I also working on porting it to Unity HDRP.
				</p>
			</li>
			<li>
				<p>
                    Shame Voxel Cone Tracing/VXGI is not even used as a fallback whenever DXR (DirectX Raytracing) is not supported.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="voxel-cone-traced-reflections" >
    Voxel Cone Traced Reflections
</h5>
<ul>
	<li>
		<p>
            Sometimes used for specular approximation.
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=3tDLFceRXoM" 
				class="external-link" 
				target="_blank" >
                Voxel Traced GI/Reflections - Demo 2019
			</a>
            .
		</p>
	</li>
</ul>
<h5
	id="global-illumination-based-on-surfels-gibs-surfel-gi" >
    Global Illumination Based on Surfels (GIBS) (Surfel GI)
</h5>
<ul>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=h1ocYFrtsM4" 
				class="external-link" 
				target="_blank" >
                Global Illumination Based on Surfels - EA 2021
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<input
				type="checkbox" 
				disabled=""
>
            
			<a
				href="https://m4xc.dev/blog/surfel-maintenance/" 
				class="external-link" 
				target="_blank" >
                Surfel Maintenance for Global Illumination - January 2025
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<em>
                Impressions
			</em>
            :
		</p>
		<ul>
			<li>
				<p>
                    It is a technique different from the classic ones currently used. It does not use probes or voxels.
				</p>
			</li>
			<li>
				<p>
                    It is a somewhat elegant technique.
				</p>
			</li>
			<li>
				<p>
                    The presentation covers many of the points I had doubts about, it seems well developed.
				</p>
			</li>
			<li>
				<p>
                    I believe that if the camera moves very quickly, or a new scene appears in front of the player quickly, the technique suffers because it has to update all the surfels.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            Surfel-based GI is not inherently ray-traced, however, EA GIBS implementation (and several production uses) do use ray queries/ray tracing as the primary means to place surfels and to evaluate visibility/irradiance, so practical surfel-GI systems are often implemented with ray tracing.
		</p>
	</li>
	<li>
		<p>
            It is completely geared toward Raytracing, given its nature.
		</p>
	</li>
	<li>
		<p>
            Frostbite Engine / EA.
		</p>
	</li>
	<li>
		<p>
            An image is discretized in pixels, a surface is discretized in surfels.
		</p>
	</li>
	<li>
		<p>
            It's an interactive screen space gap filling.
		</p>
	</li>
	<li>
		<p>
            The screen is split into 16x16 tiles and the algorithm finds the tile with the lowest surfel coverage.
		</p>
	</li>
	<li>
		<p>
            If a tile passes a randomized threshold, spawn a surfel.
		</p>
	</li>
	<li>
		<p>
            The result is cached for further use.
		</p>
	</li>
	<li>
		<p>
            Uses Radial Gaussian Depth, inspired by DDGI.
		</p>
	</li>
	<li>
		<p>
            One surfel shares the irradiance with another surfel.
		</p>
	</li>
</ul>
<h5
	id="screen-space-indirect-lighting-ssil" >
    Screen-Space Indirect Lighting (SSIL)
</h5>
<ul>
	<li>
		<p>
            ‚ÄúWhere should I 
			<em>
                add
			</em>
            &nbsp;light because nearby surfaces reflect it?‚Äù
		</p>
	</li>
	<li>
		<p>
            SSIL tries to estimate the contribution of nearby surfaces reflecting light into shaded areas, using only the information already available in the screen-space buffers (depth, normals, and sometimes color).
		</p>
	</li>
	<li>
		<p>
            Can both darken (occlusion) and brighten areas by adding bounced light.
		</p>
	</li>
	<li>
		<p>
            Think of SSIL as color bleeding + soft bounce lighting in screen space.
		</p>
	</li>
	<li>
		<p>
            Many SSIL techniques use analytic / horizon-based or hemisphere sampling approaches, and some SSIL variants use screen-space ray-marching (ray-marching against the depth buffer). SSIL only becomes hardware/world-space ray-tracing if you explicitly add a BVH-trace pass (i.e. a hybrid that is no longer ‚Äúpure‚Äù screen-space).
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://docs.godotengine.org/en/4.4/tutorials/3d/environment_and_post_processing.html#screen-space-indirect-lighting-ssil" 
				class="external-link" 
				target="_blank" >
                SSIL - Godot
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    SSIL provides indirect lighting for small details or dynamic geometry that other global illumination techniques cannot cover. This applies to bounced diffuse lighting, but also emissive materials.
				</p>
			</li>
			<li>
				<p>
                    SSIL also provides a subtle ambient occlusion effect, similar to SSAO, but with less detail.
				</p>
			</li>
			<li>
				<p>
                    Good 
					<em>
                        secondary
					</em>
                    &nbsp;source of indirect lighting, but no reflections.
				</p>
			</li>
			<li>
				<p>
                    SSIL works best for small-scale details, as it cannot provide accurate indirect lighting for large structures on its own. SSIL can provide real-time indirect lighting in situations where other GI techniques fail to capture small-scale details or dynamic objects. Its screen-space nature will result in some artifacts, especially when objects enter and leave the screen. SSIL works using the last frame's color (before post-processing) which means that emissive decals and custom shaders are included (as long as they're present on screen).
				</p>
			</li>
			<li>
				<p>
					<em>
                        Usage
					</em>
                    :
				</p>
				<ul>
					<li>
						<p>
                            This feature only provides 
							<strong>
                                indirect lighting
							</strong>
                            . It is 
							<em>
                                not
							</em>
                            &nbsp;a full global illumination solution.
						</p>
						<ul>
							<li>
								<p>
                                    This makes it different from screen-space global illumination (SSGI) offered by other 3D engines.
								</p>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            SSIL is meant to be used as a complement to other global illumination techniques such as VoxelGI, SDFGI and LightmapGI.
						</p>
					</li>
					<li>
						<p>
                            SSIL can be combined with SSR and/or SSAO for greater visual quality (at the cost of performance).
						</p>
					</li>
					<li>
						<p>
                            When SSIL is enabled on its own, the effect may not be that noticeable, which is intended.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<em>
                        Performance
					</em>
                    :
				</p>
				<ul>
					<li>
						<p>
                            The SSIL quality and number of blur passes can be adjusted in the Project Settings. By default, SSIL rendering is performed at half resolution (and then linearly scaled) to ensure a reasonable performance level.
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="ssgi-screen-space-global-illumination" >
    SSGI (Screen-Space Global Illumination)
</h5>
<ul>
	<li>
		<p>
            &quot;Standard raytracing in screenspace&quot;.
		</p>
	</li>
	<li>
		<p>
            It is an image-space approximation that often uses screen-space ray-marching or hemisphere sampling (i.e. ‚Äúrays‚Äù marched through the depth buffer), which is different from hardware/true world-space ray tracing.
		</p>
	</li>
	<li>
		<p>
            Computes GI from information available in the screen buffer (depth, normals). Fast but limited to visible surfaces and can produce artifacts.
		</p>
	</li>
	<li>
		<p>
            Screen-space sampling / ray marching in screen-space. Not a full ray tracer.
		</p>
	</li>
	<li>
		<p>
            Most real-time SSGI implementations use temporal and/or spatial denoising (or temporal accumulation) because the raw results are noisy or contain high-frequency sampling error unless you pay a high sampling cost.
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=dmdyqzelBIY" 
				class="external-link" 
				target="_blank" >
                Implementing SSGI with Joint Bileteral Filtering as a Denoiser
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    It also discusses techniques for reducing GI resolution and using image upscaling; this improves performance with practically no visual difference.
				</p>
			</li>
			<li>
				<p>
                    Etc; several other small techniques are discussed.
				</p>
			</li>
			<li>
				<p>
                    All of this in the video was made to apply a 
					<a
						href="https://reshade.me/" 
						class="external-link" 
						target="_blank" >
                        ReShade
					</a>
                    &nbsp;to Skyrim; in the final seconds of the video the before-and-after difference with the new GI technique is shown.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://dev.epicgames.com/documentation/en-us/unreal-engine/screen-space-global-illumination-in-unreal-engine" 
				class="external-link" 
				target="_blank" >
                SSGI in Unreal Engine 5
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    Is a feature that aims to create natural-looking lighting by adding dynamic indirect lighting to objects within the screen view. SSGI also makes it possible to have dynamic lighting from emissive surfaces, such as neon lights or other bright surfaces.
				</p>
			</li>
			<li>
				<p>
                    Screen Space Global Illumination works best as a supplimental indirect lighting illumination method to precomputed lighting from 
					<a
						href="https://dev.epicgames.com/documentation/en-us/unreal-engine/global-illumination-in-unreal-engine" 
						class="external-link" 
						target="_blank" >
                        Lightmass
					</a>
                    .
				</p>
			</li>
			<li>
				<p>
                    SSGI, like other screen space effects, is best used in conjunction with other indirect lighting techniques, such as 
					<a
						href="https://dev.epicgames.com/documentation/en-us/unreal-engine/global-illumination-in-unreal-engine" 
						class="external-link" 
						target="_blank" >
                        precomputed lighting from lightmass
					</a>
                    . When you have large objects that block portions of the screen, SSGI becomes apparent when it's being used as the sole indirect lighting illumination for the scene. For example, using baked lighting reduces screen space artifacts when transitioning behind a large occluder where a bright object may be located. SSGI is recommended as a means to improve indirect lighting illumination in your scene but not as a sole indirect lighting method.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://docs.unity3d.com/Packages/com.unity.render-pipelines.high-definition@14.0/manual/Override-Screen-Space-GI.html" 
				class="external-link" 
				target="_blank" >
                SSGI in Unity
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://wickedengine.net/2024/12/wicked-engines-graphics-in-2024/" 
				class="external-link" 
				target="_blank" >
                Wicked Engine 2024
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    I also experimented with a screen space global illumination (SSGI) that I wanted to base on the ‚Äúmulti scale screen space ambient occlusion‚Äù (MSAO) that I got from the 
					<a
						href="https://github.com/microsoft/DirectX-Graphics-Samples/tree/master/MiniEngine" 
						class="external-link" 
						target="_blank" >
                        DirectX Miniengine
					</a>
                    .
				</p>
			</li>
			<li>
				<p>
                    Just like MSAO, I wanted SSGI to not have to use any temporal accumulation.
				</p>
			</li>
			<li>
				<p>
                    This technique currently can only add lighting, not remove it, but it‚Äôs meant to be used together with MSAO which handles only ambient occlusion. However, I might revisit and improve this because in real scenes I didn‚Äôt find its quality good enough, especially on small scale as I had to use a lot of blur to hide the sub-sampling.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            SSGI only provides bounce lighting for objects that are on the screen, meaning if you have a large red object that is bouncing red light into the scene, and you turn away, all the red light disappears.
		</p>
	</li>
	<li>
		<p>
            That‚Äôs a pretty big limitation but it can work in some cases, such as top-down camera views. Those usually don‚Äôt have significant overlapping elements and objects take up less of the screen space so lighting changes usually aren‚Äôt as jarring.
		</p>
	</li>
	<li>
		<p>
            I wouldn‚Äôt use it as the sole source of GI for a first/third person project, but it can be used to augment baked lighting, as it will provide bounce light and occlusion for movable objects/lights that otherwise wouldn‚Äôt.
		</p>
	</li>
	<li>
		<p>
            Think of SSGI as a subset/approximation of what 
			<em>
                path tracing
			</em>
            &nbsp;would compute, but with significant limitations:
		</p>
		<ul>
			<li>
				<p>
                    Path tracing can see the entire scene, SSGI only sees what‚Äôs on screen.
				</p>
			</li>
			<li>
				<p>
                    Path tracing accounts for multiple bounces, SSGI usually only approximates one diffuse bounce.
				</p>
			</li>
			<li>
				<p>
                    Path tracing is unbiased (given enough samples), SSGI is inherently biased.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            Bad implementations of SSGI can run worse than some Path Tracing implementations.
		</p>
	</li>
</ul>
<h5
	id="ssrtgi-screen-space-ray-traced-global-illumination" >
    SSRTGI (Screen-Space Ray-Traced Global Illumination)
</h5>
<ul>
	<li>
		<p>
			<em>
                SSGI vs SRTGI
			</em>
            :
		</p>
		<ul>
			<li>
				<p>
                    All SSRTGI implementations are SSGI (because they use screen buffers), but not all SSGI implementations are SSRTGI (because some use hemisphere sampling, cone approximations, analytic occlusion, or other non-ray-march methods).
				</p>
			</li>
			<li>
				<p>
                    Instead of sampling nearby pixels like SSGI does, SSRTGI performs actual ray marching or ray queries in screen space to simulate diffuse rays bouncing off surfaces.
				</p>
			</li>
			<li>
				<p>
                    Honestly... it seems the same thing, the term sounds interchangeable.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            Still limited to screen-space data (can‚Äôt see off-screen geometry), but captures more accurate light transport within the visible region.
		</p>
	</li>
	<li>
		<p>
            Usually produces better spatial coherence and more accurate occlusion than SSGI. Can also support multiple samples per pixel for more realistic diffuse scattering.
		</p>
	</li>
	<li>
		<p>
            Heavier than SSGI because of ray marching and denoising. However, still cheaper than full path tracing since it avoids tracing into the full scene BVH.
		</p>
	</li>
</ul>
<h5
	id="ray-traced-ambient-occlusion-rtao" >
    Ray-Traced Ambient Occlusion (RTAO)
</h5>
<ul>
	<li>
		<p>
			<a
				href="https://github.com/boksajak/RTAO" 
				class="external-link" 
				target="_blank" >
                RTAO Implementation
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    Ray Traced Ambient Occlusion (RTAO) implemented using DirectX Raytracing (DXR)
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="ray-traced-global-illumination-rtxgi" >
    Ray-Traced Global Illumination (RTXGI)
</h5>
<ul>
	<li>
		<p>
            NVIDIA‚Äôs RTXGI is implemented as volumes of probes (probe grids/DDGI-style) where probes are updated (ray-traced) and store irradiance/distance-to-geometry for shading.
		</p>
	</li>
	<li>
		<p>
            RTXGI fits into the modern game engine by directly replacing existing indirect lighting approaches such as screen-space ray casting, precomputed lightmaps, and baked irradiance probes. We combine ray tracing, fast irradiance updates, and a moment-based depth scheme for occlusion calculations to create a scalable system without bake times or light leaks. RTXGI is supported on any DXR-enabled GPU and provides developers with an ideal starting point to bring the benefits of real-time ray tracing to their existing tools, knowledge, and capabilities.
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.nvidia.com/en-us/on-demand/session/gtcsj20-s22692/" 
				class="external-link" 
				target="_blank" >
                RTXGI
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://github.com/NVIDIA-RTX/RTXGI" 
				class="external-link" 
				target="_blank" >
                RTXGI
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    RTXGI v2.0 Update including Neural Radiance Cache and Spatial Hash Radiance Cache
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://github.com/NVIDIAGameWorks/RTXGI-DDGI" 
				class="external-link" 
				target="_blank" >
                RTXGI
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<strong>
                Usage
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    RTXGI is less prominent as a mainstream, engine-integrated solution today.
				</p>
			</li>
			<li>
				<p>
                    Industry momentum has shifted toward hybrid ray-tracing + sampling/resampling approaches (Lumen, ReSTIR/RTXDI, hardware path tracing) and improved probe/volume variants (DDGI/modern probe grids).
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<em>
                Unreal Engine
			</em>
            :
		</p>
		<ul>
			<li>
				<p>
                    Available on UE4 with the RTXGI Plugin and NVIDIA maintained an RTX-focused UE branch, and some community forks claim UE5 support, but 
					<em>
                        RTXGI is not the default/integrated GI in mainline Unreal Engine 5
					</em>
                    &nbsp;and NVIDIA‚Äôs official plugin work was effectively put on hold/limited after early UE5 versions.
				</p>
			</li>
			<li>
				<p>
                    Unreal Engine 5 uses Lumen.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="ground-truth-ambient-occlusion-gtao" >
    Ground Truth Ambient Occlusion (GTAO)
</h5>
<ul>
	<li>
		<p>
			<a
				href="https://www.iryoku.com/downloads/Practical-Realtime-Strategies-for-Accurate-Indirect-Occlusion.pdf" 
				class="external-link" 
				target="_blank" >
                ‚ÄúPractical Realtime Strategies for Accurate Indirect Occlusion&quot; - 2016
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            Works almost identically to HBAO, but with a few key differences:
		</p>
		<ul>
			<li>
				<p>
                    The heavy math is moved outside of the loop, needed to be calculated once per slice, so the performance is comparable with HBAO+.
				</p>
			</li>
			<li>
				<p>
                    Consider the cosine of the angle, just like HBAO+ does.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="distance-field-ambient-occlusion-dfao" >
    Distance Field Ambient Occlusion (DFAO)
</h5>
<ul>
	<li>
		<p>
			<a
				href="https://advances.realtimerendering.com/s2015/DynamicOcclusionWithSignedDistanceFields.ppt" 
				class="external-link" 
				target="_blank" >
                Dynamic Occlusion with Signed Distance Fields - Unreal Engine 2015
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://dev.epicgames.com/documentation/en-us/unreal-engine/distance-field-ambient-occlusion-in-unreal-engine" 
				class="external-link" 
				target="_blank" >
                Unreal 5 DFAO
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            Unlike SSAO, occlusion is computed from world-space occluders, so there are no artifacts from missing data off-screen.
		</p>
	</li>
	<li>
		<p>
            It supports dynamic scene changes; the rigid meshes can be moved or hidden, and it will affect the occlusion.
		</p>
	</li>
	<li>
		<p>
            Distance Field AO quality is determined by the resolution of the Mesh Distance Field it represents. Since AO is very soft shadowing, so even if the surfaces aren't represented properly, occlusion further from the surface will be accurate. It's often not noticeable with sky occlusion. However, make sure that the larger details of the mesh are well represented in the Mesh Distance Field for good results.
		</p>
	</li>
	<li>
		<p>
            The cost of Distance Field AO is primarily GPU time and video memory. DFAO has been optimized such that it can run on medium-spec PC, PlayStation 4, and Xbox One. Currently, it has a much more reliable cost so that it's mostly constant (with a slight dependency on object-density).
		</p>
	</li>
	<li>
		<p>
            In cases with a static camera and mostly flat surfaces, DFAO is 1.6x faster when compared to earlier implementations. In complex scenes with foliage and a fast moving camera, the latest optimizations are 5.5x faster. The cost of Distance Field AO on PlayStation 4 for a full game scene is around 3.7ms.
		</p>
	</li>
	<li>
		<p>
            DFAO relies on an SDF that encodes the minimum signed distance from any point in space to the nearest surface (positive outside, negative inside, or a variant with only unsigned distances).
		</p>
	</li>
</ul>
<h5
	id="scalable-ambient-obscurance-hbao-sao" >
    Scalable Ambient Obscurance (HBAO+ / SAO)
</h5>
<ul>
	<li>
		<p>
			<a
				href="https://research.nvidia.com/sites/default/files/pubs/2012-06_Scalable-Ambient-Obscurance/McGuire12SAO.pdf" 
				class="external-link" 
				target="_blank" >
                &quot;Scalable Ambient Obscurance&quot; - 2012
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            It had nothing to do with HBAO, it's actually an optimization of Alchemy Screen-Space Ambient Occlusion.
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250919082620.png" width="450" >
            .
		</p>
	</li>
</ul>
<h5
	id="hierarhcial-digital-differential-analyzer-global-illumination-hddagi" >
    Hierarhcial Digital Differential Analyzer Global Illumination (HDDAGI)
</h5>
<ul>
	<li>
		<p>
			<a
				href="https://research.dreamworks.com/wp-content/uploads/2018/07/talk_hierarchical_digital_OpenVDB_v4-Edited.pdf" 
				class="external-link" 
				target="_blank" >
                Hierarhcial Digital Differential Analyzer for Efficient Ray-Marching in OpenVDB - 2013
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://github.com/godotengine/godot/pull/86267" 
				class="external-link" 
				target="_blank" >
                PR para HDDAGI para Godot
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=9Dj9lvBkY-o" 
				class="external-link" 
				target="_blank" >
                HDDAGI - Godot Demo
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            This is a new global illumination system meant to supersede SDFGI.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Key advantages are
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Much, much faster. Significantly lower frame time, orders of magnitude faster cascad
				</p>
			</li>
			<li>
				<p>
                    Generally higher quality (less arctifacting).
				</p>
			</li>
			<li>
				<p>
                    Much better occclusion (a¬†
					<em>
                        lot
					</em>
                    ¬†less light leaked).
				</p>
			</li>
			<li>
				<p>
                    Less memory usage.
                    <br>
                    It is meant as a drop-in replacement. Should work as a replacement for SDFGI.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Known issues
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    ¬†For some reason gets DEVICE LOST on Intel GPUs. No idea why.
				</p>
			</li>
			<li>
				<p>
                    ¬†Sharp Reflections not always play good with TAA (wobbly).
				</p>
			</li>
			<li>
				<p>
                    ¬†Darkening (occlusion) on some corners, just like SDFGI. I tried different techniques to see if any worked better. DDGI Octahedral VSM gets rid of them, but also leaks a lot more light, so I am unconvinced. Have other ideas to try, but I don‚Äôt have infinite time üôÅ
				</p>
			</li>
			<li>
				<p>
                    ¬†SDFGI spherical harmonics turned out to be buggy and not energy conserving. This makes GI look more saturated and have more light than in HDDAGI (which some people may appreciate more), but It‚Äôs a bug üò¢. Wondering how this can be compensated.
				</p>
			</li>
			<li>
				<p>
                    ¬†Still some further pending optimizations.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Future
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    ¬†Dynamic object support.
				</p>
			</li>
			<li>
				<p>
                    ¬†High density mode (sub-probes).
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="sdfgi-signed-distance-field-global-illumination" >
    SDFGI (Signed Distance Field Global Illumination)
</h5>
<ul>
	<li>
		<p>
			<a
				href="https://www.docdroid.net/YNntL0e/godot-sdfgi-pdf" 
				class="external-link" 
				target="_blank" >
                &quot;SDFGI Solving the accessible Global Illumination problem in Godot&quot; - Outubro 2022
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    This paper is 
					<em>
                        absolutely
					</em>
                    &nbsp;important to understand how SDFGI was made.
				</p>
			</li>
			<li>
				<p>
                    Requirements
				</p>
				<ul>
					<li>
						<p>
                            Easy to use (no scene or object setup at import time, no setting up SDF, cards,
						</p>
					</li>
					<li>
						<p>
                            lightmaps, etc). Ideally enable with one click, no set-up.
						</p>
					</li>
					<li>
						<p>
                            Real-time (or at least fast updates).
						</p>
					</li>
					<li>
						<p>
                            Good enough quality (no light leaks -or keep to minimum-).
						</p>
					</li>
					<li>
						<p>
                            Supports both diffuse and reflected light.
						</p>
					</li>
					<li>
						<p>
                            Supports light into transparent objects.
						</p>
					</li>
					<li>
						<p>
                            Works as source of light for volumetric fog.
						</p>
					</li>
					<li>
						<p>
                            Works in all hardware that supports Vulkan, even IGP.
						</p>
					</li>
					<li>
						<p>
                            Can work in VR (so, using TAA is not required).
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Sacrifices
				</p>
				<ul>
					<li>
						<p>
                            Not the best possible quality (high frequency GI missing, has to be&nbsp;&nbsp;compensated with screen space lighting).
						</p>
					</li>
					<li>
						<p>
                            Poor dynamic object support (dynamic objects get light from environment, but don't contribute to it). Light blocking may be added to some extent in the future.
						</p>
					</li>
					<li>
						<p>
                            Needs to use cascades.
						</p>
					</li>
					<li>
						<p>
                            Limited amount of samples means small emissive objects are spotty.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Previous work
				</p>
				<ul>
					<li>
						<p>
                            Uses DDGI by Morgan McGuire as a base.
						</p>
					</li>
					<li>
						<p>
                            Uses Signed Distance Fields generated with 
							<a
								href="https://www.comp.nus.edu.sg/~tants/jfa.html" 
								class="external-link" 
								target="_blank" >
                                Jump Flood
							</a>
                            .
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://arxiv.org/pdf/2007.14394" 
				class="external-link" 
				target="_blank" >
                Signed Distance Fields Dynamic Diffuse Global Illumination - 2020
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    Global Illumination (GI) is of utmost importance in the field of photo-realistic rendering. However, its computation has always been very complex, especially diffuse GI. State of the art real-time GI methods have limitations of different nature, such as light leaking, performance issues, special hardware requirements, noise corruption, bounce number limitations, among others. To overcome these limitations, we propose a novel approach of computing dynamic diffuse GI with a signed distance fields approximation of the scene and discretizing the space domain of the irradiance function. With this approach, we are able to estimate real-time diffuse GI for dynamic lighting and geometry, without any precomputations and supporting multi-bounce GI, providing good quality lighting and high performance at the same time. Our algorithm is also able to achieve better scalability, and manage both large open scenes and indoor high-detailed scenes without being corrupted by noise.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<em>
                My opinions
			</em>
            :
		</p>
		<ul>
			<li>
				<p>
                    Knowing the Godot implementation, I find the visuals simply uninspiring.
				</p>
			</li>
			<li>
				<p>
                    It sounds bad for me to go for this solution when I didn't even use this for Godot games.
				</p>
			</li>
			<li>
				<p>
                    I've always found the performance somewhat poor, but it could simply be the low-quality implementation in the editor.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            It‚Äôs not screen-space (since it doesn‚Äôt rely only on what‚Äôs visible on screen).
		</p>
	</li>
	<li>
		<p>
            It‚Äôs also not a full path-traced solution; instead it‚Äôs a form of voxel-based GI accelerated by signed distance fields.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Future in Godot
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    SDFGI will be replaced either way as it has many limitations that can't be resolved (such as slow cascade generation speed).
				</p>
			</li>
			<li>
				<p>
                    A bounded GI implementation to supersede VoxelGI may be added in the future, but it's not guaranteed.
				</p>
			</li>
			<li>
				<p>
                    There are plans to replace it with HDDAGI.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                SDFGI vs VoxelGI
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    SDFGI provides better real-time ability than baked lightmaps, but worse real-time ability than VoxelGI.
				</p>
				<ul>
					<li>
						<p>
                            SDFGI supports dynamic lights, but 
							<em>
                                not
							</em>
                            &nbsp;dynamic occluders or dynamic emissive surfaces.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Using 
					<em>
                        Use Occlusion
					</em>
                    &nbsp;has a small performance cost, but it often results in fewer leaks compared to VoxelGI.
				</p>
			</li>
			<li>
				<p>
                    Newer on Godot, when compared to VoxelGI. SDFGI was implemented as a new feature in Godot 4.0 release.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://docs.godotengine.org/en/4.4/tutorials/3d/global_illumination/using_sdfgi.html#doc-using-sdfgi" 
				class="external-link" 
				target="_blank" >
                SDFGI - Godot
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
					<a
						href="https://www.youtube.com/watch?v=QFKPrDv-Ue8" 
						class="external-link" 
						target="_blank" >
                        SDFGI - Godot Demo
					</a>
                    .
				</p>
			</li>
			<li>
				<p>
                    SDFGI is something akin to a dynamic real-time lightmap (but it does not requiere unwrapping, nor does it use textures). It‚Äôs enabled and it automatically works by generating global illumination for static objects. It does not require raytracing, and it runs in most current (and some years old) dedicated GPUs, even medium-end budget CPUs from some years ago (SDFGI was developed and tested on a GeForce 1060, running at a stable 60 FPS).
				</p>
			</li>
			<li>
				<p>
                    Light changes 
					<strong>
                        are real-time
					</strong>
                    , meaning any change in lighting conditions will result in an 
					<strong>
                        immediate update
					</strong>
                    . Dynamic objects are supported only for receiving light from the environment, but they don‚Äôt contribute to lighting. Some degree of support is planned for this eventually, but not immediately.
				</p>
			</li>
			<li>
				<p>
                    SDFGI also supports specular reflections, 
					<strong>
                        both sharp and rough
					</strong>
                    , so full PBR scenes should ‚Äújust work‚Äù. In the image below you can see both of them in checkerboard roughness texture.
				</p>
			</li>
			<li>
				<p>
                    SDFGI is mostly leak free, unlike VCT techniques which are the most common in use today (like SVOGI/GIProbe/etc). As long as walls are thicker than a voxel for a given cascade, light won‚Äôt go through.
				</p>
				<ul>
					<li>
						<p>
                            Leaks can be reduced significantly by enabling the 
							<em>
                                Use Occlusion
							</em>
                            &nbsp;property.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    GI level of detail varies depending on the distance between the camera and surface.
				</p>
			</li>
			<li>
				<p>
					<em>
                        Caviats / Issues
					</em>
                    :
				</p>
				<ul>
					<li>
						<p>
                            Cascade shifts may be visible when the camera moves fast. This can be made less noticeable by adjusting the cascade sizes or using fog.
						</p>
					</li>
					<li>
						<p>
                            Good reflections and indirect lighting, but beware of leaks and visible cascade shifts.
						</p>
					</li>
					<li>
						<p>
                            SDFGI has some downsides due to its cascaded nature. When the camera moves, cascade shifts may be visible in indirect lighting.
						</p>
						<ul>
							<li>
								<p>
                                    This can be alleviated by adjusting the cascade size, but also by adding fog (which will make distant cascade shifts less noticeable).
								</p>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            Performance will suffer if the camera moves too fast. This can be fixed in two ways:
						</p>
						<ul>
							<li>
								<p>
                                    Ensuring the camera doesn't move too fast in any given situation.
								</p>
							</li>
							<li>
								<p>
                                    Temporarily disabling SDFGI in the Environment resource if the camera needs to be moved at a high speed, then enabling SDFGI once the camera speed slows down.
								</p>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            When SDFGI is enabled, it will also take some time for global illumination to be fully converged (25 frames by default). This can create a noticeable transition effect while GI is still converging.
						</p>
						<ul>
							<li>
								<p>
                                    To hide this, you can use a ColorRect node that spans the whole viewport and fade it out when switching scenes using an AnimationPlayer node.
								</p>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            The signed distance field is only updated when the camera moves in and out of a cascade. This means that if geometry is modified in the distance, the global illumination appearance will be correct once the camera gets closer. However, if a nearby object with a bake mode set to Static or Dynamic is moved (such as a door), the global illumination will appear incorrect until the camera moves away from the object.
						</p>
					</li>
					<li>
						<p>
                            SDFGI's sharp reflections are only visible on opaque materials. Transparent materials will only use rough reflections, even if the material's roughness is lower than 0.2.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<em>
                        Performance
					</em>
                    :
				</p>
				<ul>
					<li>
						<p>
                            The number of cascades can be adjusted to balance performance and quality. The number of rays thrown per frame can be adjusted in the Project Settings. The rendering can optionally be performed at half resolution (and then linearly scaled) to improve performance significantly.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<em>
                        Setting up
					</em>
                    :
				</p>
				<ol>
					<li>
						<p>
                            Make sure your MeshInstance nodes have their 
							<strong>
                                Global Illumination &gt; Mode
							</strong>
                            &nbsp;property set to 
							<strong>
                                Static
							</strong>
                            &nbsp;in the inspector.
						</p>
						<ul>
							<li>
								<p>
                                    Any Mesh can receive Indirect Lighting, but only static meshes can contribute to Indirect Lighting.
								</p>
							</li>
							<li>
								<p>
                                    For meshes:
								</p>
								<ul>
									<li>
										<p>
											<strong>
                                                Disabled:
											</strong>
										</p>
										<ul>
											<li>
												<p>
                                                    The mesh won't be taken into account in SDFGI generation. The mesh will receive indirect lighting from the scene, but it will not contribute indirect lighting to the scene.
												</p>
											</li>
										</ul>
									</li>
									<li>
										<p>
											<strong>
                                                Static (default):
											</strong>
										</p>
										<ul>
											<li>
												<p>
                                                    The mesh will be taken into account in SDFGI generation. The mesh will both receive 
													<em>
                                                        and
													</em>
                                                    &nbsp;contribute indirect lighting to the scene. If the mesh is changed in any way after SDFGI is generated, the camera must move away from the object then move back close to it for SDFGI to regenerate. Alternatively, SDFGI can be toggled off and back on. If neither is done, indirect lighting will look incorrect.
												</p>
											</li>
										</ul>
									</li>
									<li>
										<p>
											<strong>
                                                Dynamic (not supported with SDFGI):
											</strong>
										</p>
										<ul>
											<li>
												<p>
                                                    The mesh won't be taken into account in SDFGI generation. The mesh will receive indirect lighting from the scene, but it will not contribute indirect lighting to the scene.
												</p>
											</li>
											<li>
												<p>
                                                    This acts 
													<em>
                                                        identical
													</em>
                                                    &nbsp;to the Disabled bake mode when using SDFGI.
												</p>
											</li>
										</ul>
									</li>
								</ul>
							</li>
							<li>
								<p>
                                    For lights:
								</p>
								<ul>
									<li>
										<p>
											<strong>
                                                Disabled:
											</strong>
										</p>
										<ul>
											<li>
												<p>
                                                    The light will 
													<em>
                                                        not
													</em>
                                                    &nbsp;be taken into account for SDFGI baking. The light won't contribute indirect lighting to the scene.
												</p>
											</li>
										</ul>
									</li>
									<li>
										<p>
											<strong>
                                                Static:
											</strong>
										</p>
										<ul>
											<li>
												<p>
                                                    The light will be taken into account for SDFGI baking. The light will contribute indirect lighting to the scene. If the light is changed in any way after baking, indirect lighting will look incorrect until the camera moves away from the light and back (which causes SDFGI to be baked again). will look incorrect. If in doubt, use this mode for level lighting.
												</p>
											</li>
										</ul>
									</li>
									<li>
										<p>
											<strong>
                                                Dynamic (default):
											</strong>
										</p>
										<ul>
											<li>
												<p>
                                                    The light won't be taken into account for SDFGI baking, but it will still contribute indirect lighting to the scene in real-time. This option is slower compared to 
													<strong>
                                                        Static
													</strong>
                                                    . Only use the 
													<strong>
                                                        Dynamic
													</strong>
                                                    &nbsp;global illumination mode on lights that will change significantly during gameplay.
												</p>
											</li>
										</ul>
									</li>
								</ul>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            Add a WorldEnvironment node and create an Environment resource for it.
						</p>
					</li>
					<li>
						<p>
                            Edit the Environment resource, scroll down to the 
							<strong>
                                SDFGI
							</strong>
                            &nbsp;section and unfold it.
						</p>
					</li>
					<li>
						<p>
                            Enable 
							<strong>
                                SDFGI &gt; Enabled
							</strong>
                            .
						</p>
					</li>
				</ol>
				<ul>
					<li>
						<p>
                            SDFGI will automatically follow the camera when it moves, so you do not need to configure extents (unlike VoxelGI).
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<a
						href="https://docs.godotengine.org/en/4.4/tutorials/3d/global_illumination/using_sdfgi.html#environment-sdfgi-properties" 
						class="external-link" 
						target="_blank" >
                        Properties
					</a>
                    .
				</p>
			</li>
			<li>
				<p>
					<a
						href="https://docs.godotengine.org/en/4.4/tutorials/3d/global_illumination/using_sdfgi.html#adjusting-sdfgi-performance-and-quality" 
						class="external-link" 
						target="_blank" >
                        Performance and Quality
					</a>
                    .
				</p>
			</li>
			<li>
				<p>
                    To make a specific light emit more or less indirect energy without affecting the amount of direct light emitted by the light, adjust the 
					<strong>
                        Indirect Energy
					</strong>
                    &nbsp;property in the Light3D inspector.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            It is not a screen-space effect, so it can provide global illumination for off-screen elements (unlike SSIL).
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250918145006.png" width="400" >
            .
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250918145148.png" width="400" >
            .
		</p>
	</li>
</ul>
<h5
	id="lighting-grid-hierarchy" >
    Lighting Grid Hierarchy
</h5>
<ul>
	<li>
		<p>
			<a
				href="https://www.cemyuksel.com/research/lgh/real-time_rendering_with_lgh_i3d2019.pdf" 
				class="external-link" 
				target="_blank" >
                Real-Time Rendering with Lighting Grid Hierarchy - 2019
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.cemyuksel.com/research/lgh/lgh.pdf" 
				class="external-link" 
				target="_blank" >
                Lighting Grid Hierarchy for Self-illuminating Explosions - Siggraph 2017
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.cemyuksel.com/research/lgh/" 
				class="external-link" 
				target="_blank" >
                Lighting Grid Hierarchy
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            Focusing on temporal coherency to avoid flickering in animations, we introduce 
			<em>
                lighting grid hierarchy
			</em>
            &nbsp;for approximating the volumetric illumination at different resolutions. Using this structure we can efficiently approximate the lighting at any point inside or outside of the explosion volume as a mixture of lighting contributions from all levels of the hierarchy. As a result, we are able to capture high-frequency details of local illumination, as well as the potentially strong impact of distant illumination. Most importantly, this hierarchical structure allows us to efficiently precompute volumetric shadows, which substantially accelerates the lighting computation. Finally, we provide a scalable approach for computing the multiple scattering of light within the smoke volume using our lighting grid hierarchy.
		</p>
	</li>
	<li>
		<p>
            Ray tracing 
			<em>
                can
			</em>
            &nbsp;be layered on top for visibility injection, but the technique itself does not depend on it.
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=BHQ4RhWl-vc" 
				class="external-link" 
				target="_blank" >
                Lighting Grid Hierarchy with Raytracing Hardware Demo
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    100k VPL (100.000 Virtual Point Lights).
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="screen-space-directional-occlusion-ssdo" >
    Screen-Space Directional Occlusion (SSDO)
</h5>
<ul>
	<li>
		<p>
			<a
				href="https://ascane.github.io/projects/06_ssdo/report.pdf" 
				class="external-link" 
				target="_blank" >
                Real-time Approximated Global Illumination From SSAO to SSDO - 2016
			</a>
            .
		</p>
	</li>
</ul>
<h5
	id="precomputed-radiance-transfer-prt" >
    Precomputed Radiance Transfer (PRT)
</h5>
<ul>
	<li>
		<p>
			<strong>
                My understanding
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Based on the explanations in &quot;A Data-Driven Paradigm for Precomputed Radiance Transfer - 2022&quot; and &quot;Neural Precomputed Radiance Transfer - 2022&quot;, I understood that this strategy is an intermediate between baking and dynamic.
				</p>
			</li>
			<li>
				<p>
                    The idea would be to compute different types of lighting on a mesh, choosing which to apply based on the mesh's current direct lighting conditions.
				</p>
			</li>
			<li>
				<p>
                    It uses Spherical Harmonics directly, but another basis can be used.
				</p>
			</li>
			<li>
				<p>
                    ChatGPT:
				</p>
				<ul>
					<li>
						<p>
                            PRT is a technique that moves expensive, direction-dependent light transport calculations offline so that, at runtime, shading under complex (often dynamic) lighting can be evaluated with a small number of dot-products.
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://jankautz.com/publications/prtSIG02.pdf" 
				class="external-link" 
				target="_blank" >
                Precomputed Radiance Transfer for Real-Time Rendering in Dynamic, Low-Frequency Lighting Environments - Siggraph 2002
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250921164439.png" width="400" >
            .
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250921154944.png" width="400" >
            .
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250921160349.png" width="350" >
            .
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250921160533.png" width="350" >
            .
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250921162247.png" width="500" >
            .
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250921162207.png" width="450" >
            .
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250921162225.png" width="450" >
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=k3hzcrjoFbc" 
				class="external-link" 
				target="_blank" >
                A Data-Driven Paradigm for Precomputed Radiance Transfer - Unity 2022
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
					<a
						href="https://arxiv.org/pdf/2206.13112" 
						class="external-link" 
						target="_blank" >
                        A Data-Driven Paradigm for Precomputed Radiance Transfer - Unity 2022
					</a>
                    .
				</p>
			</li>
			<li>
				<p>
                    Very interesting.
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250921162915.png" width="450" >
                    .
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250921162927.png" width="450" >
                    .
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=voyOq7VJfl0" 
				class="external-link" 
				target="_blank" >
                Neural Precomputed Radiance Transfer - 2022
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    Neural PRT:
				</p>
				<ul>
					<li>
						<p>
                            <img src="assets/image_20250921160243.png" width="500" >
                            .
						</p>
					</li>
					<li>
						<p>
                            A CNN encoder is the part of a convolutional neural network (CNN) that compresses input data into a compact, meaningful representation called a latent representation. It functions by progressively down-sampling and extracting hierarchical features from the input, such as an image, through a series of convolutional and pooling layers. The goal of the encoder is to reduce the input's dimensionality while retaining essential information, making it a compressed version of the original data that can then be used by a 
							<a
								href="https://www.google.com/search?client=firefox-b-d&cs=1&sca_esv=730c08a984f2ad03&q=decoder&sa=X&ved=2ahUKEwjG282txeqPAxU_DLkGHUz-FOgQxccNegQIBRAB&mstk=AUtExfAVbBz0aoEqs_sqNypyuG83mjn4wS_fhnPddU8y0J29NIhf6yRTh4vEVUQ_JRo4od2X2VFHzl7uOlei5viFjVOo2ubHYl-lOzoZVzSoyHn-pwoefuSFLzyUbrZRSyraUUPn1UR73Cjq92zGAU1UFYc2dLo2WUIHl5vl17vIr3vnnj4&csui=3" 
								class="external-link" 
								target="_blank" >
                                decoder
							</a>
                            &nbsp;for tasks like reconstruction or pixel-level prediction
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250921161241.png" width="500" >
                    .
				</p>
				<ul>
					<li>
						<p>
                            &quot;Even with 5 circle harmonic bands (25 coefficients), spherical harmonics tend to cutoff the high frequency angular signals, this is visible on the mirror, for example&quot;.
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
	<li>
		<p>
            Instead of dealing with rays, PRT deals with functions on a sphere.
		</p>
	</li>
	<li>
		<p>
            Traditional PRT suggests as choice of basis:
		</p>
		<ul>
			<li>
				<p>
                    Spherical Harmonics
				</p>
			</li>
			<li>
				<p>
                    Haar Wavelets.
				</p>
			</li>
			<li>
				<p>
                    Spherical Gaussians.
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250921155754.png" width="500" >
                    .
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            The choice of basis has been one of the main domains of research.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Usage
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    It is designed for 
					<em>
                        static
					</em>
                    &nbsp;geometry (or limited deformation) and is primarily targeted at low-frequency environment lighting and shadowing / soft interreflection effects that can be precomputed.
				</p>
			</li>
			<li>
				<p>
                    Partially replaced by real-time ray-tracing / dynamic probe systems for workflows that require runtime changes. PRT is efficient for low-frequency, mostly-static content (precomputation), but for highly dynamic environments engines increasingly use real-time methods (RT + denoising, probe volumes) to handle scene changes.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=04YUZ3bWAyg" 
				class="external-link" 
				target="_blank" >
                PRT Probes - The Division 1 - 2016
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    It is not normal PRT, but rather PRT Probes.
				</p>
			</li>
			<li>
				<p>
                    I watched about the first 20 minutes and some other segments of the video.
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250921163440.png" width="450" >
                    .
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250921164329.png" width="400" >
                    <img src="assets/image_20250921164606.png" width="450" >
                    .
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250921164546.png" width="450" >
                    .
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250921165103.png" width="450" >
                    .
				</p>
				<ul>
					<li>
						<p>
                            After considerations, we settled on the Half Life 2 Ambient Cube Basis (HL2), which is not a real basis but 6 vectores aligned; so it requires only six floats.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    The probes are placed automatically via a 4x4 meters raycast grid, spawning a probe on every ray hit; also, spawn probes alongside building walls, to avoid them looking flat.
				</p>
			</li>
			<li>
				<p>
                    The storage on disk is via a 2D grid with 64x64 meters, with maximum of 1000 probes, but usually 200 probes per sector. The sectors are streamed in and out as the player moves.
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250921165504.png" width="500" >
                    .
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250921165706.png" width="450" >
                    .
				</p>
				<ul>
					<li>
						<p>
                            Manhattan == Manhattan city map.
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="voxel-ambient-occlusion-vxao" >
    Voxel Ambient Occlusion (VXAO)
</h5>
<ul>
	<li>
		<p>
            Not in screen space.
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.nvidia.com/en-us/geforce/news/nvidia-vxao-voxel-ambient-occlusion/" 
				class="external-link" 
				target="_blank" >
                Nvidia VXAO - 2016
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            Unlike VXGI which controls all illumination, VXAO is only utilized for Ambient Occlusion, enabling us to integrate it into a wider array of games and game engines that use traditional illumination technologies.
		</p>
	</li>
	<li>
		<p>
            It‚Äôs more accurate than SSAO and its derivatives, casts deeper, richer shadows that account for even the smallest details in a scene, and runs faster than other competing effects when they‚Äôre rendered at a similar quality.
		</p>
	</li>
	<li>
		<p>
            Even still, HBAO+‚Äôs Ambient Occlusion shadowing is far from the level of fidelity offered by VXAO, which avoids the caveats of screen space techniques, enabling us to deliver the most accurate and realistic Ambient Occlusion shadowing seen to date.
		</p>
	</li>
	<li>
		<p>
            With VXAO, occlusion and lighting information is gathered from a ‚Äòworld space‚Äô voxel representation of the scene, which takes into account a large area around the viewer. Included in this voxelization are objects and details currently invisible to the viewer, and those behind the viewer, too. The result is scene-wide Ambient Occlusion shadowing, instead of ‚Äòscreen space‚Äô shadowing based on what you can currently see. This allows AO shadows to be cast into a scene from objects near to the player but just outside of their view, and from occluded objects in the distance large enough to affect the appearance of the scene.
		</p>
	</li>
	<li>
		<p>
            Use on:
		</p>
		<ul>
			<li>
				<p>
                    Rise of the Tomb Raider.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=VWiSex0NhlI" 
				class="external-link" 
				target="_blank" >
                SSAO vs HBAO+ vs VXAO, Rise of the Tomb Raider
			</a>
            .
		</p>
	</li>
</ul>
<h5
	id="mssao-multi-resolution-screen-space-ambient-occlusion" >
    MSSAO (Multi-Resolution Screen-Space Ambient Occlusion)
</h5>
<ul>
	<li>
		<p>
			<a
				href="https://www.comp.nus.edu.sg/~lowkl/publications/mssao_cgi2011.pdf" 
				class="external-link" 
				target="_blank" >
                MSSAO 2011
			</a>
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250917182637.png" width="400" >
            .
		</p>
	</li>
	<li>
		<p>
            You can combine results from different occlusion radius.
		</p>
	</li>
	<li>
		<p>
            It is a technique from a 2010 paper.
		</p>
	</li>
</ul>
<h5
	id="ssao-screen-space-ambient-occlusion" >
    SSAO (Screen-Space Ambient Occlusion)
</h5>
<ul>
	<li>
		<p>
            ‚ÄúWhere should I 
			<em>
                remove
			</em>
            &nbsp;light because it‚Äôs blocked?‚Äù
		</p>
	</li>
	<li>
		<p>
            Ray Traced Ambient Occlusion itself is an approximation of Indirect Light, and SSAO is an approximation of Ray Traced Ambient Occlusion.
		</p>
		<ul>
			<li>
				<p>
                    So it's an approximation of an approximation.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            SSAO only acts on 
			<em>
                ambient light
			</em>
            . It does not affect direct light.
		</p>
	</li>
	<li>
		<p>
            Think of SSAO as a shadow-only pass, faking soft contact shadows.
		</p>
	</li>
	<li>
		<p>
            Ray Traced AO would be the &quot;correct&quot; ambient occlusion, but that is kinda difficult to say as Ambient Occlusion itself is a &quot;fake term&quot;; SSAO approximates it.
		</p>
	</li>
	<li>
		<p>
            It's much faster then Ray Traced AO.
		</p>
	</li>
	<li>
		<p>
            Approximates ambient occlusion as a cheap GI term; bent normals can guide diffuse light injection.
		</p>
	</li>
	<li>
		<p>
            Screen-space sampling; rasterization-based.
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://docs.godotengine.org/en/4.4/tutorials/3d/environment_and_post_processing.html#screen-space-ambient-occlusion-ssao" 
				class="external-link" 
				target="_blank" >
                SSAO - Godot
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    If you want to force SSAO to work with direct light too, use the 
					<strong>
                        Light Affect
					</strong>
                    &nbsp;parameter. Even though this is not physically correct, some artists like how it looks.
				</p>
			</li>
			<li>
				<p>
                    SSAO looks best when combined with a real source of indirect light, like 
					<em>
                        VoxelGI
					</em>
                    .
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            <img src="assets/image_20250917181805.png" width="450" >
            .
		</p>
	</li>
</ul>
<h5
	id="combined-adaptive-compute-ambient-occlusion-cacao" >
    Combined Adaptive Compute Ambient Occlusion (CACAO)
</h5>
<ul>
	<li>
		<p>
			<a
				href="https://gpuopen.com/fidelityfx-cacao/" 
				class="external-link" 
				target="_blank" >
                AMD FidelityFX Combined Adaptive Compute Ambient Occlusion
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            Released in May 2020.
		</p>
	</li>
	<li>
		<p>
            As of 2023, it became part of the AMD FidelityFX SDK.
		</p>
	</li>
	<li>
		<p>
            Updated in May 2025.
		</p>
	</li>
	<li>
		<p>
            &quot;Artist control, etc, but deviates from the rendering equation&quot;.
		</p>
	</li>
</ul>
<h5
	id="screen-space-global-illumination-based-invariance-ssvgi" >
    Screen-Space Global Illumination Based-Invariance (SSVGI)
</h5>
<ul>
	<li>
		<p>
            The author of Radiance Cascading worked on this technique for PoE1.
		</p>
	</li>
	<li>
		<p>
            Uses exclusively image space data.
		</p>
	</li>
	<li>
		<p>
            Calculates GI for every point from scratch.
		</p>
	</li>
	<li>
		<p>
            Still needs denoising.
		</p>
	</li>
	<li>
		<p>
            Uses Screen Space Shadow Hierarchy.
		</p>
		<ul>
			<li>
				<p>
                    &quot;Shadow cascade&quot;.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="light-propagation-volumes-lpv" >
    Light Propagation Volumes (LPV)
</h5>
<ul>
	<li>
		<p>
            First introduced by Crytek in 2009.
		</p>
	</li>
	<li>
		<p>
            LV calculation of global illumination consists of three steps:
		</p>
		<ul>
			<li>
				<p>
                    Injection virtual points lights obtained from Reflective Shadow Maps into LPV 3D grid.
				</p>
			</li>
			<li>
				<p>
                    Propagation of light intensity in grid stored in spherical harmonics coefficients.
				</p>
			</li>
			<li>
				<p>
                    Lookup for light intensity in LPV while scene rendering.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            LPV sits in the same family as Voxel-GI (both use a 3D grid) and is functionally closer to runtime probe/volume methods like DDGI than to precomputed PRT ‚Äî but LPV‚Äôs propagation approach and data layout make its behavior and trade-offs distinct.
		</p>
		<ol>
			<li>
				<p>
                    Inject light into a 3D regular grid (the ‚Äúvolume‚Äù) from direct light sources or from virtual point lights / reflective shadow maps. The injected values are usually stored as low-order spherical harmonics (SH) or simple directional bands per cell.
				</p>
			</li>
			<li>
				<p>
                    Iteratively propagate those values between neighboring grid cells (a diffusion / scattering sweep). The propagation step moves energy through the grid and approximates multiple diffuse bounces.
				</p>
			</li>
			<li>
				<p>
                    At shading time, the renderer samples the grid (trilinear / tetrahedral interpolation) and uses the sampled radiance/SH to illuminate surfaces (usually only the diffuse term).
				</p>
			</li>
		</ol>
	</li>
	<li>
		<p>
            Key implementation notes: LPV normally stores very low angular detail (few SH bands or directional channels) and relies on repeated propagation iterations to spread light. It does not explicitly store full scene geometry inside the grid (although depth/normal heuristics can be used to reduce obvious leakage).
		</p>
	</li>
	<li>
		<p>
			<strong>
                Usage
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    LPV fell out of favor in many production engines because its core design produces persistent, hard-to-fix artifacts (notably light bleeding and poor directional fidelity) and because alternative runtime GI approaches (probe-based DDGI variants, voxel-cone tracing, and hardware-accelerated ray-traced GI) offer better trade-offs for modern, dynamic scenes and artist workflows. The choice is engineering- and platform-dependent; LPV still makes sense in limited cases (very low-cost, low-frequency indirect lighting), but it is no longer the common ‚Äúgo-to‚Äù for high-quality dynamic GI in AAA engines.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://dev.epicgames.com/documentation/en-us/unreal-engine/light-propagation-volumes?application_version=4.27" 
				class="external-link" 
				target="_blank" >
                LPV - Unreal Engine 5
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    Deprecated in UE5, used in UE4.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://cg.ivd.kit.edu/publications/p2009/LPVIC3_Kaplanyan_2009/LPVIC3_Kaplanyan_2009.pdf?utm_source=chatgpt.com" 
				class="external-link" 
				target="_blank" >
                Light Propagation Volumes - Cry Engine 3 - 2009
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://github.com/djbozkosz/Light-Propagation-Volumes" 
				class="external-link" 
				target="_blank" >
                LPV Implementation Demo
			</a>
            .
		</p>
	</li>
</ul>
<h5
	id="ambient-color" >
    Ambient Color
</h5>
<ul>
	<li>
		<p>
            <img src="assets/image_20250918204343.png" width="450" >
            .
		</p>
	</li>
	<li>
		<p>
            Usually 2 colors, applied depending on the normal of the surface.
		</p>
	</li>
</ul>
<h5
	id="instant-radiosity-quotevirtual-lightsquote" >
    Instant Radiosity (&quot;Virtual Lights&quot;)
</h5>
<ul>
	<li>
		<p>
            It's not instant, and has nothing to do with the 'Radiosity' method.
		</p>
	</li>
	<li>
		<ol>
			<li>
			</li>
		</ol>
	</li>
	<li>
		<p>
            Approximates GI by spawning virtual point lights (VPLs) from primary light bounces.
		</p>
	</li>
	<li>
		<p>
            Then renders the scene lit by these many point lights (with importance sampling and clamping to reduce artifacts).
		</p>
	</li>
	<li>
		<p>
            It‚Äôs an approximation to many-bounce light transport.
		</p>
	</li>
	<li>
		<p>
            The method requires finding secondary bounces to spawn virtual point lights (VPLs). This is traditionally done with ray tracing.
		</p>
		<ul>
			<li>
				<p>
                    However, simplified rasterization approximations (e.g., reflective shadow maps) exist that avoid explicit ray casting.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            A &quot;Light Source&quot; is the Sun, and a &quot;Virtual Light&quot; is the Moon.
		</p>
	</li>
	<li>
		<p>
            Starting from the light source, generate virtual lights placed where the light illuminates a surface, by sampling random directions from the light source.
		</p>
	</li>
	<li>
		<p>
            The virtual lights account for indirect illumination; the indirect illumination becomes direct illumination from the virtual lights.
		</p>
	</li>
	<li>
		<p>
            It converts the problem of indirect illumination into the problem of rendering many light sources.
		</p>
	</li>
	<li>
		<p>
            It can account for one or many bounces of light; you just need to keep creating virtual lights.
		</p>
	</li>
	<li>
		<p>
            This is the opposite of Path Tracing; it's called 
			<em>
                Light Tracing
			</em>
            .
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250918112329.png" width="400" >
            .
		</p>
	</li>
	<li>
		<p>
            CryEngine 3:
		</p>
		<ul>
			<li>
				<p>
                    This class of approaches is based on the idea of representing indirect lighting as a cloud set of virtual point light sources (VPL). Consequently, this technique has a great potential to speed up with GPU. Its main advantages are&nbsp;&nbsp;good veracity and absence of any scene/lighting/camera constraints. Unfortunately, the main disadvantage of these methods is inadequate performance primarily because of the necessity to render at least 300-400 shadow- casting VPLs for an arbitrary scene to represent the precise solution without artifacts and flickering.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="horizon-based-ambient-occlusion-hbao" >
    Horizon-Based Ambient Occlusion (HBAO)
</h5>
<ul>
	<li>
		<p>
            &quot;Image-Space Horizon-based Ambient Occlusion&quot; - Nvidia Siggraph 2008.
		</p>
	</li>
	<li>
		<p>
            Very expensive trigonometry operations and too slow at the time.
		</p>
	</li>
</ul>
<h5
	id="alchemy-screen-space-ambient-obscurance-alchemyao" >
    Alchemy Screen-Space Ambient Obscurance (AlchemyAO)
</h5>
<ul>
	<li>
		<p>
            HBAO+ is an optimization of this technique.
		</p>
	</li>
</ul>
<h5
	id="metropolis-light-transport-mlt" >
    Metropolis Light Transport (MLT)
</h5>
<ul>
	<li>
		<p>
			<a
				href="https://graphics.stanford.edu/papers/metro/metro.pdf" 
				class="external-link" 
				target="_blank" >
                Metropolis Light Transport
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            Siggraph 1997.
		</p>
	</li>
	<li>
		<p>
            Monte Carlo method that explores light paths in a ‚Äúmutation‚Äù process, emphasizing important contributions (e.g., caustics).
		</p>
	</li>
	<li>
		<p>
            Ray tracing-based Monte Carlo.
		</p>
	</li>
</ul>
<h5
	id="path-tracing" >
    Path Tracing
</h5>
<ul>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=iOlehM5kNSk" 
				class="external-link" 
				target="_blank" >
                Ray Tracing, Path Tracing, Global Illumination, BVH
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    Great video. Very illustrative.
				</p>
			</li>
			<li>
				<p>
                    Path Tracing and Global Illumination:
				</p>
				<ul>
					<li>
						<p>
                            Nothing &quot;new&quot; in the explanations. It's based on a lot of material I studied.
						</p>
					</li>
					<li>
						<p>
                            Sometimes it complicates some explanations, making things seem a bit magical and &quot;untouchable&quot;
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    BVH: Binary Volume Hierarchy.
				</p>
				<ul>
					<li>
						<p>
                            Maybe it's the most relevant explanation.
						</p>
					</li>
					<li>
						<p>
                            <img src="assets/image_20251006231423.png" width="400" >
                            .
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    <img src="assets/image_20251006231924.png" width="400" >
                    .
				</p>
				<ul>
					<li>
						<p>
                            Interesting performance statistics.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    The video ends at 22:15.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<input
				type="checkbox" 
				disabled=""
>
            
			<a
				href="https://www.scratchapixel.com/lessons/3d-basic-rendering/global-illumination-path-tracing/introduction-global-illumination-path-tracing.html" 
				class="external-link" 
				target="_blank" >
                Global Illumination and Path Tracing
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            The standard method for computing global illumination today.
		</p>
	</li>
	<li>
		<p>
            Finds light paths starting from the 
			<em>
                camera
			</em>
            .
		</p>
	</li>
	<li>
		<p>
            Project a ray and pick a random direction.
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250918104831.png" width="250" >
            &nbsp;
            <img src="assets/image_20250918105000.png" width="300" >
            .
		</p>
	</li>
	<li>
		<p>
            It tries to find all possible light paths starting from the camera to the light source.
		</p>
	</li>
	<li>
		<p>
            The amount of light paths consider is indicated by the SPP (Samples Per pixel).
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250918105138.png" width="250" >
            &nbsp;
            <img src="assets/image_20250918105219.png" width="250" >
            &nbsp;
            <img src="assets/image_20250918105246.png" width="250" >
            &nbsp;
            <img src="assets/image_20250918105337.png" width="250" >
            &nbsp;
            <img src="assets/image_20250918105419.png" width="250" >
            .
		</p>
	</li>
	<li>
		<p>
			<strong>
                Denoiser
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    <img src="assets/image_20250918105955.png" width="400" >
                    .
				</p>
			</li>
			<li>
				<ol>
					<li>
					</li>
				</ol>
			</li>
			<li>
				<p>
                    Use the samples from the neighboring pixels, to estimate the indirect illumination for the current pixel.
				</p>
			</li>
			<li>
				<p>
                    AI and Deep Learning is used today.
				</p>
			</li>
			<li>
				<p>
					<em>
                        Spacial Denoising
					</em>
                    :
				</p>
			</li>
			<li>
				<p>
					<em>
                        Temporal Denoising
					</em>
                    :
				</p>
				<ul>
					<li>
						<p>
                            Cheaper and it doesn't blur.
						</p>
					</li>
					<li>
						<p>
                            For a game that has a camera moving all the time, you'll have to use:
						</p>
						<ul>
							<li>
								<p>
                                    Motion Vectors.
								</p>
								<ul>
									<li>
										<p>
                                            How things moved between frames.
										</p>
									</li>
									<li>
										<p>
                                            <img src="assets/image_20250918121801.png" width="300" >
                                            .
										</p>
									</li>
								</ul>
							</li>
							<li>
								<p>
                                    Reject samples that were reprojected incorrectly.
								</p>
							</li>
							<li>
								<p>
                                    <img src="assets/image_20250918121813.png" width="300" >
                                    .
								</p>
							</li>
						</ul>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<strong>
                        Joint Bileteral Filter
					</strong>
                    :
				</p>
				<ul>
					<li>
						<p>
                            <img src="assets/image_20250918121942.png" width="350" >
                            .
						</p>
					</li>
					<li>
						<p>
                            Takes into account the depth and normals.
						</p>
					</li>
					<li>
						<p>
                            <img src="assets/image_20250918122051.png" width="350" >
                            .
						</p>
						<ul>
							<li>
								<p>
                                    My understanding is that there is less blur based on the depth buffer, in the image on the left.
								</p>
							</li>
						</ul>
					</li>
				</ul>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="photon-mapping" >
    Photon Mapping
</h5>
<ul>
	<li>
		<p>
            1995 to 2001.
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://web.cs.wpi.edu/~emmanuel/courses/cs563/write_ups/zackw/photon_mapping/PhotonMapping.html" 
				class="external-link" 
				target="_blank" >
                Photon Mapping
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            Two-pass algorithm: first, photons are traced from light sources and stored; second, the radiance is gathered at surfaces to compute illumination.
		</p>
	</li>
	<li>
		<p>
            Handles caustics and diffuse interreflections.
		</p>
	</li>
	<li>
		<p>
            Uses Ray tracing (photon tracing) + data structure (k-d tree for photon storage).
		</p>
	</li>
	<li>
		<p>
            CryEngine 3:
		</p>
		<ul>
			<li>
				<p>
                    These methods are less popular than the others in real-time graphics because of their performance issues. Usually this class of techniques is based on the classical photon-mapping approach. These methods usually use GPU texture fetching and rendering units to accelerate the photon map evaluation. The usual optimizations for these techniques are irradiance caching, importance sampling, and the incremental approach. One drawback of these methods is that the scene needs to be preprocessed to get the unique representation for the photon map. Another problem is photon map updates caused by scene and lighting changes, which leads to highly inconsistent performance and intermittent stalls.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="radiosity" >
    Radiosity
</h5>
<ul>
	<li>
		<ol>
			<li>
			</li>
		</ol>
	</li>
	<li>
		<p>
            Origin of the 
			<em>
                Cornell Box
			</em>
            .
		</p>
	</li>
	<li>
		<p>
            Radiosity is a global illumination method that solves light transport between diffuse-only surfaces by discretizing geometry into patches and solving a linear system of energy exchange.
		</p>
	</li>
	<li>
		<p>
            Produces smooth, view-independent GI (often offline or precomputed).
		</p>
	</li>
	<li>
		<p>
            Surface-based; patch-to-patch energy exchange (matrix solve). Ray tracing is optional for visibility (hemicube or ray casting).
		</p>
	</li>
	<li>
		<p>
            It doesn't use ray tracing inherently. Radiosity is based on solving a radiosity matrix (energy exchange between surface patches).
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250918104614.png" width="300" >
            .
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250918104634.png" width="300" >
            .
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250918104652.png" width="350" >
            .
		</p>
	</li>
</ul>
<h3
	id="lightmaps" >
    Lightmaps
</h3>
<ul>
	<li>
		<p>
            <img src="assets/image_20250918111953.png" width="400" >
            .
		</p>
	</li>
	<li>
		<p>
            It doesn't matter what method we use for computing the GI.
		</p>
	</li>
	<li>
		<p>
            Access the data from a texture; trilinear filter it and we are done.
		</p>
	</li>
	<li>
		<p>
            Easy and cheap.
		</p>
	</li>
	<li>
		<p>
            Only works for static scenes.
		</p>
	</li>
</ul>
<h5
	id="light-maps" >
    Light Maps
</h5>
<ul>
	<li>
		<p>
            <img src="assets/image_20250918112212.png" width="350" >
            .
		</p>
		<ul>
			<li>
				<p>
                    <img src="assets/image_20250918112234.png" width="174" >
                    .
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=39iBnWTXR8M" 
				class="external-link" 
				target="_blank" >
                Lightmaps Demo - Wicked Engine
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="http://the-witness.net/news/2010/03/graphics-tech-texture-parameterization/" 
				class="external-link" 
				target="_blank" >
                Lightmap Parameterization - The Witness
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="http://the-witness.net/news/2010/09/hemicube-rendering-and-integration/" 
				class="external-link" 
				target="_blank" >
                Hemicube Rendering and Integration - The Witness
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="http://the-witness.net/news/2011/07/irradiance-caching-part-1/" 
				class="external-link" 
				target="_blank" >
                Irradiance Caching Pt1 - The Witness
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.ludicon.com/castano/blog/2014/07/irradiance-caching-continued/" 
				class="external-link" 
				target="_blank" >
                Irradiance Caching Pt2 - The Witness
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<input
				type="checkbox" 
				disabled=""
>
            
			<a
				href="https://www.RayLib.com/examples/shaders/loader.html?name=shaders_lightmap_rendering" 
				class="external-link" 
				target="_blank" >
                Lightmap in RayLib
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    See the source in VSCode.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Limitations
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Specularity changes based on the angle of the camera, so it cannot be baked into the geometry; this is view dependent.
				</p>
			</li>
			<li>
				<p>
                    No specular indirect lighting baked.
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250918203936.png" width="500" >
                    .
				</p>
			</li>
		</ul>
	</li>
</ul>
<h5
	id="light-bleeding" >
    Light Bleeding
</h5>
<ul>
	<li>
		<p>
			<a
				href="https://docs.unity3d.com/6000.1/Documentation/Manual/ProgressiveLightmapper-UVOverlap.html" 
				class="external-link" 
				target="_blank" >
                Fixing light bleeding in Lightmaps - Unity
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
					<em>
                        Solutions
					</em>
                    :
				</p>
				<ul>
					<li>
						<p>
                            If you provide lightmap UVs yourself, add margins using your modeling package.
						</p>
					</li>
					<li>
						<p>
                            ~If Unity automatically generates the lightmap UVs for a Model, you can tell Unity to increase the pack margin. Bla bla bla.
						</p>
					</li>
					<li>
						<p>
                            Increase the resolution:
						</p>
						<ul>
							<li>
								<p>
                                    Of the entire lightmap. This will increase the number of pixels between the charts and therefore reduce the likelihood of bleeding. The downside is that your lightmap may become too large.
								</p>
							</li>
							<li>
								<p>
                                    Of a single GameObject. This allows you to increase lightmap resolution only for GameObjects that have overlapping UVs. Though less likely, this can also increase your lightmap size.
								</p>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            I believe what Unity means is:
						</p>
						<ul>
							<li>
								<p>
                                    It must use a different texture for lightmaps, overlaid on top of the albedo.
								</p>
							</li>
							<li>
								<p>
                                    Therefore, it makes sense to talk about &quot;increasing the lightmap resolution&quot; since it's a separate texture.
								</p>
							</li>
						</ul>
					</li>
				</ul>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://helpx.adobe.com/substance-3d-bake/common-issues/aliasing-on-uv-seams.html" 
				class="external-link" 
				target="_blank" >
                Aliasing on UV Seams - Adobe
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
					<em>
                        Solutions
					</em>
                    :
				</p>
				<ul>
					<li>
						<p>
                            Increase the output texture resolution of the Bakers.
						</p>
					</li>
					<li>
						<p>
                            Increase the Anti-aliasing setting (note: it may take more time to compute).
						</p>
					</li>
					<li>
						<p>
                            Align the UVs to the pixel grid in the UV editor of the 3D modeling software.
						</p>
					</li>
					<li>
						<p>
                            Give a better Texel Ratio to UVs.
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                My question
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Caio:
				</p>
				<ul>
					<li>
						<p>
                            the light is baked in, so I guess it makes sense to be a UV issue, causing the texture bleeding
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    devsh:
				</p>
				<ul>
					<li>
						<p>
                            <img src="assets/image_20250901174203.png" width="250" >
                            .
						</p>
					</li>
					<li>
						<p>
                            welcome to the fundamental problem with lightmapping
						</p>
					</li>
					<li>
						<p>
                            Pixar solves it with 
							<a
								href="https://github.com/wdas/ptex" 
								class="external-link" 
								target="_blank" >
                                PTEX
							</a>
                            .
						</p>
						<ul>
							<li>
								<p>
                                    &quot;Per-Face Texture Mapping for Production Rendering&quot;.
								</p>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            as a mere mortal you need to clamp the max mip level, anisotropy, and add padding texels around every UV island
						</p>
					</li>
					<li>
						<p>
                            you may also want to adjust the way you island your faces
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Caio:
				</p>
				<ul>
					<li>
						<p>
                            Interesting, I've never heard of this. I'm not using mipmap nor AA for now, I'm still setting things up
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Devsh:
				</p>
				<ul>
					<li>
						<p>
                            well then it's bilinear interpolation bleed
						</p>
					</li>
					<li>
						<p>
                            your pixel centers are not on the edge, there are no padding/border pixels.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Caio:
				</p>
				<ul>
					<li>
						<p>
                            so it's not a problem with the UVs?
						</p>
					</li>
					<li>
						<p>
                            I'm a bit confused, is this fixed by implementing AA or mipmap?
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Devsh:
				</p>
				<ul>
					<li>
						<p>
                            no, it's made worse by mip-mapping
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Caio:
				</p>
				<ul>
					<li>
						<p>
                            so, from what I'm seeing, this is caused when generating the lightmap and baking the light color into the albedo texture? The lack of padding or UV overlap, etc. So, from what I'm understanding, this isn't caused by some oversight on my part, but this is an external issue that my render engine could fix at runtime? You said something about &quot;bilinear interpolation bleed&quot; &quot;clamp the max mip level, anisotropy and add padding texels around every UV island&quot;, so I got confused about who's to blame for the artifact, my rendering engine or external software used to generate the lightmap?
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Devsh:
				</p>
				<ul>
					<li>
						<p>
                            both, you need to generate the input data better, and your engine needs to take care not to accidentally enable mip-mapping later on
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Caio:
				</p>
				<ul>
					<li>
						<p>
                            just a final clarification: are you considering that the lightmap is from a separate texture from the albedo, or the light is baked into the albedo? This is a free model, I didn't make the light baking myself. So when you say &quot;generate the input data better&quot;, are you referring to the texture the light was baked into, or some other &quot;input&quot;? Finally, if it is the case that I can't rebake the texture, is there something I can do to reduce the artifact, or are all my options just to keep it from getting worse with mipmapping?
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
</ul>

					</div>
					<footer
						id="previous-next" >
					</footer>
				</article>
			</main>
			<footer
				id="central-footer" >
                üßë‚Äçüíª built by and copyright
				<a
					href="https://github.com/caioraphael1" 
					target="_blank" >
                    Caio Raphael
				</a>
                üìÖ 2025-10-21 .&nbsp;&nbsp;2026-01-21 üöÄ
			</footer>
		</div>
		<script
			src="/static/studies.36380.js" >
		</script>
	</body>
</html>
