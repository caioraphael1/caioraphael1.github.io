<!doctype html>
<html
	lang="en" >
	<head>
		<title>
            Caio Raphael
		</title>
		<meta
			charset="utf-8" >
		<meta
			name="viewport" 
			content="width=device-width, initial-scale=1" >
		<meta
			name="description" 
			content="Senior Game Developer, Engine Developer, Low-Level Network, Low-Level Systems, Physicist" >
		<meta
			name="author" 
			content="Caio Raphael" >
		<meta
			name="theme-color" 
			content="#ffffff" 
			media="(prefers-color-scheme: light)" >
		<meta
			name="theme-color" 
			content="#101010" 
			media="(prefers-color-scheme: dark)" >
		<link
			rel="icon" 
			href="/assets/icon.ico" >
		<link
			rel="icon" 
			href="/assets/icon-16x16.png" 
			sizes="16x16" 
			type="image/png" >
		<link
			rel="icon" 
			href="/assets/icon-32x32.png" 
			sizes="32x32" 
			type="image/png" >
		<script>
window.MathJax = {
                tex: {
                    inlineMath: [['$', '$']],
                    displayMath: [['$$', '$$']]
                }
                };
		</script>
		<script
			src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" >
		</script>
		<script
			type="module" >

                    import hljs from 'https://unpkg.com/@highlightjs/cdn-assets@11.11.1/es/highlight.min.js';
                    import hljs_odin from 'https://unpkg.com/highlightjs-odinlang@1.4.0/dist/odin.es.min.js';
                    import hljs_glsl from 'https://unpkg.com/@highlightjs/cdn-assets@11.11.1/es/languages/glsl.min.js';
                    import hljs_swift  from 'https://unpkg.com/@highlightjs/cdn-assets@11.11.1/es/languages/swift.min.js';
                    hljs.registerLanguage('odin', hljs_odin);
                    hljs.registerLanguage('glsl', hljs_glsl);
                    hljs.registerLanguage('gdscript', hljs_swift);
                    hljs.highlightAll();
                
		</script>
		<link
			rel="stylesheet" 
			href="/static/studies.70566.css" >
	</head>
	<body>
		<aside
			id="left-sidebar-wrapper" >
			<div
				id="left-sidebar" >
				<header>
					<a
						href="/" 
						class="site-logo" >
                        Caio Raphael
					</a>
					<p
						class="breadcrums-division" >
                        /
					</p>
					<a
						href="/studies/_index.html" 
						class="breadcrumbs-studies" >
                        Studies
					</a>
				</header>
				<nav>
					<details
>
						<summary>
                            Vulkan
						</summary>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Vulkan/Vulkan-basic.html" >
                                    Basic
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Vulkan/Vulkan-samples.html" >
                                    Samples
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Vulkan/Vulkan-core.html" >
                                    Core
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Vulkan/Vulkan-render-loop.html" >
                                    Render Loop
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Vulkan/Vulkan-synchronization-and-cache-control.html" >
                                    Synchronization and Cache Control
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Vulkan/Vulkan-command-buffers.html" >
                                    Command Buffers
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Vulkan/Vulkan-pipelines.html" >
                                    Pipelines
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Vulkan/Vulkan-graphics-pipeline.html" >
                                    Graphics Pipeline
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Vulkan/Vulkan-compute-pipeline.html" >
                                    Compute Pipeline
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Vulkan/Vulkan-resources.html" >
                                    Resources
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Vulkan/Vulkan-depth.html" >
                                    Depth
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Vulkan/Vulkan-mapping-data-to-shaders.html" >
                                    Mapping Data to Shaders
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Vulkan/Vulkan-memory-allocation.html" >
                                    Memory Allocation
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Vulkan/Vulkan-hdr-support.html" >
                                    HDR Support
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Vulkan/Vulkan-profiling.html" >
                                    Profiling
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Vulkan/Vulkan-mobile.html" >
                                    Mobile
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Vulkan/Vulkan-vr.html" >
                                    VR
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Vulkan/Vulkan-video-decoding.html" >
                                    Video Decoding
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Vulkan/Vulkan-spir-v.html" >
                                    SPIR-V
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Vulkan/Vulkan-web.html" >
                                    Web
								</a>
							</li>
						</ul>
					</details>
					<details
						open="">
						<summary>
                            Render Engineering
						</summary>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Render Engineering/Render Engineering-graphics-apis.html" >
                                    Graphics APIs
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Render Engineering/Render Engineering-shader-languages.html" >
                                    Shader Languages
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Render Engineering/Render Engineering-tools.html" >
                                    Tools
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Render Engineering/Render Engineering-choosing-the-space-to-compute-lighting.html" >
                                    Choosing the Space to compute Lighting
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Render Engineering/Render Engineering-bsdf-bidirectional-scattering-distribution-function.html" >
                                    BSDF (Bidirectional Scattering Distribution Function)
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Render Engineering/Render Engineering-material.html" >
                                    Material
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Render Engineering/Render Engineering-direct-lighting.html" >
                                    Direct Lighting
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Render Engineering/Render Engineering-shadows.html" >
                                    Shadows
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Render Engineering/Render Engineering-skybox-skydome.html" >
                                    Skybox / Skydome
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Render Engineering/Render Engineering-transparency.html" >
                                    Transparency
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Render Engineering/Render Engineering-global-illumination-indirect-lighting.html" >
                                    Global Illumination / Indirect Lighting
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Render Engineering/Render Engineering-global-illumination-solutions.html" >
                                    Global Illumination - Solutions
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="active" 
									href="/studies/Graphics Programming/Render Engineering/Render Engineering-global-illumination-discarded-solutions.html" >
                                    Global Illumination - Discarded Solutions
								</a>
								<ul>
									<li>
										<a
											href="#voxel-cone-tracing-gi-vct-voxel-gi-sparse-voxel-octree-gi-svogi-voxel-traced-global-illumination-vtgi" >
                                            Voxel Cone Tracing GI (VCT) / Voxel GI / Sparse Voxel Octree GI (SVOGI) / Voxel Traced Global Illumination (VTGI)
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#voxel-cone-traced-reflections" >
                                            Voxel Cone Traced Reflections
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#global-illumination-based-on-surfels-gibs-surfel-gi" >
                                            Global Illumination Based on Surfels (GIBS) (Surfel GI)
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#screen-space-indirect-lighting-ssil" >
                                            Screen-Space Indirect Lighting (SSIL)
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#ssgi-screen-space-global-illumination" >
                                            SSGI (Screen-Space Global Illumination)
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#ssrtgi-screen-space-ray-traced-global-illumination" >
                                            SSRTGI (Screen-Space Ray-Traced Global Illumination)
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#ray-traced-ambient-occlusion-rtao" >
                                            Ray-Traced Ambient Occlusion (RTAO)
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#ray-traced-global-illumination-rtxgi" >
                                            Ray-Traced Global Illumination (RTXGI)
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#ground-truth-ambient-occlusion-gtao" >
                                            Ground Truth Ambient Occlusion (GTAO)
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#distance-field-ambient-occlusion-dfao" >
                                            Distance Field Ambient Occlusion (DFAO)
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#scalable-ambient-obscurance-hbao-sao" >
                                            Scalable Ambient Obscurance (HBAO+ / SAO)
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#hierarhcial-digital-differential-analyzer-global-illumination-hddagi" >
                                            Hierarhcial Digital Differential Analyzer Global Illumination (HDDAGI)
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#sdfgi-signed-distance-field-global-illumination" >
                                            SDFGI (Signed Distance Field Global Illumination)
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#lighting-grid-hierarchy" >
                                            Lighting Grid Hierarchy
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#screen-space-directional-occlusion-ssdo" >
                                            Screen-Space Directional Occlusion (SSDO)
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#precomputed-radiance-transfer-prt" >
                                            Precomputed Radiance Transfer (PRT)
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#voxel-ambient-occlusion-vxao" >
                                            Voxel Ambient Occlusion (VXAO)
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#mssao-multi-resolution-screen-space-ambient-occlusion" >
                                            MSSAO (Multi-Resolution Screen-Space Ambient Occlusion)
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#ssao-screen-space-ambient-occlusion" >
                                            SSAO (Screen-Space Ambient Occlusion)
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#combined-adaptive-compute-ambient-occlusion-cacao" >
                                            Combined Adaptive Compute Ambient Occlusion (CACAO)
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#screen-space-global-illumination-based-invariance-ssvgi" >
                                            Screen-Space Global Illumination Based-Invariance (SSVGI)
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#light-propagation-volumes-lpv" >
                                            Light Propagation Volumes (LPV)
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#ambient-color" >
                                            Ambient Color
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#instant-radiosity-quotevirtual-lightsquote" >
                                            Instant Radiosity (&quot;Virtual Lights&quot;)
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#horizon-based-ambient-occlusion-hbao" >
                                            Horizon-Based Ambient Occlusion (HBAO)
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#alchemy-screen-space-ambient-obscurance-alchemyao" >
                                            Alchemy Screen-Space Ambient Obscurance (AlchemyAO)
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#metropolis-light-transport-mlt" >
                                            Metropolis Light Transport (MLT)
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#path-tracing" >
                                            Path Tracing
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#photon-mapping" >
                                            Photon Mapping
										</a>
										<ul>
										</ul>
									</li>
									<li>
										<a
											href="#radiosity" >
                                            Radiosity
										</a>
									</li>
								</ul>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Render Engineering/Render Engineering-lightmaps.html" >
                                    Lightmaps
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Render Engineering/Render Engineering-post-processing.html" >
                                    Post-Processing
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Render Engineering/Render Engineering-anti-aliasing.html" >
                                    Anti-Aliasing
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Render Engineering/Render Engineering-light-path-rendering-method.html" >
                                    Light Path / Rendering Method
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Render Engineering/Render Engineering-optimization-techniques.html" >
                                    Optimization Techniques
								</a>
							</li>
						</ul>
					</details>
					<details
>
						<summary>
                            Graphics and Shaders
						</summary>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Graphics and Shaders/Graphics and Shaders-sources.html" >
                                    Sources
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Graphics and Shaders/Graphics and Shaders-math-linear-algebra.html" >
                                    Math, Linear Algebra
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Graphics and Shaders/Graphics and Shaders-spaces-transformations-and-graphics-pipeline.html" >
                                    Spaces, Transformations and Graphics Pipeline
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Graphics and Shaders/Graphics and Shaders-sizes.html" >
                                    Sizes
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Graphics and Shaders/Graphics and Shaders-common-techniques.html" >
                                    Common Techniques
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Graphics and Shaders/Graphics and Shaders-shaders.html" >
                                    Shaders
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Graphics and Shaders/Graphics and Shaders-fixing-artifacts.html" >
                                    Fixing Artifacts
								</a>
							</li>
						</ul>
					</details>
					<details
>
						<summary>
                            GLSL
						</summary>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/GLSL/GLSL-basic.html" >
                                    Basic
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/GLSL/GLSL-storage-qualifiers.html" >
                                    Storage Qualifiers
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/GLSL/GLSL-layout-qualifiers.html" >
                                    Layout Qualifiers
								</a>
							</li>
						</ul>
					</details>
					<details
>
						<summary>
                            GPU
						</summary>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/GPU/GPU-execution-building-blocks.html" >
                                    Execution Building Blocks
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/GPU/GPU-specialized-units-and-instructions.html" >
                                    Specialized units &amp; instructions
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/GPU/GPU-memory.html" >
                                    Memory
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/GPU/GPU-cache.html" >
                                    Cache
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/GPU/GPU-gpu-va-virtual-address.html" >
                                    GPU VA (Virtual Address)
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/GPU/GPU-tiled-gpus.html" >
                                    Tiled-GPUs
								</a>
							</li>
						</ul>
					</details>
					<details
>
						<summary>
                            Slang
						</summary>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Slang-slang.html" >
                                    Slang
								</a>
							</li>
						</ul>
					</details>
					<details
>
						<summary>
                            Font Rendering
						</summary>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Font Rendering-techniques.html" >
                                    Techniques
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Font Rendering-text-processing-pipeline.html" >
                                    Text Processing Pipeline
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Font Rendering-concepts.html" >
                                    Concepts
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Font Rendering-formats.html" >
                                    Formats
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Font Rendering-libs.html" >
                                    Libs
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Font Rendering-tools.html" >
                                    Tools
								</a>
							</li>
						</ul>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/Font Rendering-fonts.html" >
                                    Fonts
								</a>
							</li>
						</ul>
					</details>
					<details
>
						<summary>
                            OpenGL
						</summary>
						<ul>
							<li>
								<a
									class="" 
									href="/studies/Graphics Programming/OpenGL/OpenGL-about.html" >
                                    About
								</a>
							</li>
						</ul>
					</details>
				</nav>
			</div>
		</aside>
		<main
			id="central-wrapper" >
			<main
				id="note-wrapper" >
				<header
					id="note-header" >
					<h1>
                        Global Illumination - Discarded Solutions
					</h1>
					<p>
						<time
							datetime="2025-07-03" >
                            üïí Created: 2025-07-03
						</time>
						<time
							datetime="2026-02-15" >
                            | Updated: 2026-02-15
						</time>
					</p>
				</header>
				<article
					id="note-content" >
<h3
	id="voxel-cone-tracing-gi-vct-voxel-gi-sparse-voxel-octree-gi-svogi-voxel-traced-global-illumination-vtgi" >
    Voxel Cone Tracing GI (VCT) / Voxel GI / Sparse Voxel Octree GI (SVOGI) / Voxel Traced Global Illumination (VTGI)
</h3>
<ul>
	<li>
		<p>
			<input
				type="checkbox" 
				disabled=""
>
            
			<a
				href="https://research.nvidia.com/sites/default/files/pubs/2011-09_Interactive-Indirect-Illumination/GIVoxels-pg2011-authors.pdf" 
				class="external-link" 
				target="_blank" >
                Interactive Indirect Illumination Using Voxel Cone Tracing - 2011
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<input
				type="checkbox" 
				disabled=""
>
            
			<a
				href="https://onlinelibrary.wiley.com/doi/10.1111/cgf.15262" 
				class="external-link" 
				target="_blank" >
                Dynamic Voxel Based Global Illumination
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            Instead of using rays, we use voxel cones.
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250918110456.png" width="450" >
            &nbsp;
            <img src="assets/image_20250918110515.png" width="450" >
            .
		</p>
		<ul>
			<li>
				<p>
                    This is a high resolution of voxelization.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            Just like mipmaps, we generate lower and lower resolutions of this.
		</p>
	</li>
	<li>
		<p>
            And cones are used, where each cone accounts for a range of directions, instead of a single direction.
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250918110613.png" width="400" >
            .
		</p>
	</li>
	<li>
		<p>
            The cone tracing technique works best with a regular voxel grid because we perform ray-marching against the data like with screen space reflections for example.
		</p>
	</li>
	<li>
		<p>
            A regular voxel grid consumes more memory, but it is faster to create (voxelize), and more cache efficient to traverse (ray-march).
		</p>
	</li>
	<li>
		<p>
            The nice thing about this technique is that we can retrieve all sorts of effects. We have ‚Äúfree‚Äù ambient occlusion by default when doing this cone tracing, light bouncing, but we can retrieve reflections, refractions and shadows as well from this voxel structure with additional ray march steps. We can have a configurable amount of light bounces. Cone tracing code can be shared between the bouncing and querying shader and different types of rays as well. The entire thing remains fully on the GPU, the CPU is only responsible for command buffer generation.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Limitations / Drawbacks
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    The main drawback being the limited resolution.
				</p>
			</li>
			<li>
				<p>
                    Voxel GI gives plausible multi-bounce GI but costs in memory, bandwidth and costly voxelization/update;
				</p>
			</li>
			<li>
				<p>
                    The UE5 presentation does make a good argument against cone tracing for generic use (which unreal engine targets), there will always be some artifacts and those will be super bad in specific scenes.
				</p>
			</li>
			<li>
				<p>
                    Comparing to RTGI, voxel based is faster but less accurate and less dynamic.
				</p>
			</li>
			<li>
				<p>
                    Voxel-based lighting is only faster than ray traced global illumination solutions that favor quality over performance. RTGI solutions that prioritize performance are faster than Voxel cone tracing.
				</p>
			</li>
			<li>
				<p>
                    Indiana Jones runs at 1080 60FPS on Series S with RTGI and KCD2 runs at 1080P 30 FPS on Series S.
				</p>
			</li>
			<li>
				<p>
                    Voxel cone tracing is an obsolete technological dead end - too heavy to run on last-gen consoles and mobile, inferior to RT on modern desktop and console hardware.
				</p>
			</li>
			<li>
				<p>
                    KCD2 (Kingdom Come: Deliverance II) only uses it because CryEngine supports it but doesn‚Äôt support ray tracing. And CryEngine is still stuck using voxel lighting because the Star Citizen devs poached CryTek‚Äôs best graphics engineers, and those engineers then proceeded to add ray tracing to Star Citizen‚Äôs fork of CryEngine.
				</p>
			</li>
			<li>
				<p>
                    SVOGI is inaccurate as balls.
				</p>
			</li>
			<li>
				<p>
                    You must love light leaking through every room corner. It's cheap crappy raytracing and it looks like cheap crappy raytracing.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Implementation
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
					<a
						href="https://wickedengine.net/2017/08/voxel-based-global-illumination/" 
						class="external-link" 
						target="_blank" >
                        Wicked Engine Implementation
					</a>
                    .
				</p>
			</li>
			<li>
				<p>
                    First, we have our scene model with polygonal meshes. We need to convert it to a voxel representation. The voxel structure is a 3D texture which holds the direct illumination of the voxelized geometries in each pixel. There is an optional step here which I describe later. Once we have this, we can pre-integrate it by creating a mipmap chain for the resource. This is essential for cone tracing because we want to ray-march the texture with quadrilinear interpolation (sampling a 3D texture with min-mag-mip-linear filtering). We can then retrieve the bounced direct illumination in a final screen space cone tracing pass. The additional step in the middle is relevant if we want more bounces, because we can dispatch additional cone tracing compute shader passes for the whole structure (not in screen space).
				</p>
			</li>
			<li>
				<p>
					<strong>
                        1.) Voxelization
					</strong>
                    :
				</p>
				<ul>
					<li>
						<p>
                            The most involving part is definitely the first one, the voxelization step. It involves making use of advanced graphics API features like geometry shaders, abandoning the output merger and writing into resources ‚Äúby hand‚Äù. We can also make use of new hardware features like conservative rasterization and rasterizer ordered views, but we will implement them in the shaders as well.
						</p>
					</li>
					<li>
						<p>
                            The main trick to be able to run this in real time is that we need to parallelize the process well. For that, we will exploit the fixed function rasterization hardware, and we will get a pixel shader invocation for each voxel which will be rendered. We also do only a single render pass for every object.
						</p>
					</li>
					<li>
						<p>
                            We need to integrate the following pipeline to our scene rendering algorithm:
						</p>
					</li>
					<li>
						<p>
							<strong>
                                1.) Vertex shader
							</strong>
						</p>
					</li>
					<li>
						<p>
                            The voxelizing vertex shader needs to transform vertices into world space and pass through the attributes to the geometry shader stage. Or just do a pass through and transform to world space in the GS, doesn‚Äôt matter.
						</p>
					</li>
					<li>
						<p>
							<strong>
                                2.) Geometry shader
							</strong>
						</p>
					</li>
					<li>
						<p>
                            This will be responsible to select the best facing axis of each triangle received from the vertex shader. This is important because we want to voxelize each triangle once, on the axis it is best visible, otherwise we would get seams and bad looking results.
						</p>
					</li>
				</ul>
<pre><code class="language-glsl" data-lang="glsl">// select the greatest component of the face normal input[3] is the input array of three vertices
float3 facenormal = abs(input[0].nor + input[1].nor + input[2].nor);
uint maxi = facenormal[1] &gt; facenormal[0] ? 1 : 0;
maxi = facenormal[2] &gt; facenormal[maxi] ? 2 : maxi;
</code></pre>
				<ul>
					<li>
						<p>
                            After we determined the dominant axis, we need to project to it orthogonally by swizzling the position‚Äôs xyz components, then setting the z component to 1 and scaling it to clip space.
						</p>
					</li>
				</ul>
<pre><code class="language-glsl" data-lang="glsl">for (uint i = 0; i &lt; 3; ++i)
{
// voxel space pos:
&nbsp;&nbsp;&nbsp;&nbsp;output[i].pos = float4((input[i].pos.xyz - g_xWorld_VoxelRadianceDataCenter) / g_xWorld_VoxelRadianceDataSize, 1);
&nbsp;&nbsp;&nbsp;&nbsp;// Project onto dominant axis:
&nbsp;&nbsp;&nbsp;&nbsp;if (maxi == 0)
&nbsp;&nbsp;&nbsp;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output[i].pos.xyz = output[i].pos.zyx;
&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;else if (maxi == 1)
&nbsp;&nbsp;&nbsp;&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output[i].pos.xyz = output[i].pos.xzy;
&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;// projected pos:
&nbsp;&nbsp;&nbsp;&nbsp;output[i].pos.xy /= g_xWorld_VoxelRadianceDataRes;
&nbsp;&nbsp;&nbsp;&nbsp;output[i].pos.z = 1;
&nbsp;&nbsp;&nbsp;&nbsp;output[i].N = input[i].nor;
&nbsp;&nbsp;&nbsp;&nbsp;output[i].tex = input[i].tex;
&nbsp;&nbsp;&nbsp;&nbsp;output[i].P = input[i].pos.xyz;
&nbsp;&nbsp;&nbsp;&nbsp;output[i].instanceColor = input[i].instanceColor;
}
</code></pre>
				<ul>
					<li>
						<p>
                            At the end, we could also expand our triangle a bit to be more conservative to avoid gaps. We could also just be setting a conservative rasterizer state if we have hardware support for it and avoid the expansion here.
						</p>
					</li>
				</ul>
<pre><code class="language-glsl" data-lang="glsl">
// Conservative Rasterization setup:
float2 side0N = normalize(output[1].pos.xy - output[0].pos.xy);
float2 side1N = normalize(output[2].pos.xy - output[1].pos.xy);
float2 side2N = normalize(output[0].pos.xy - output[2].pos.xy);
const float texelSize = 1.0f / g_xWorld_VoxelRadianceDataRes;
output[0].pos.xy += normalize(-side0N + side2N)*texelSize;
output[1].pos.xy += normalize(side0N - side1N)*texelSize;
output[2].pos.xy += normalize(side1N - side2N)*texelSize;
</code></pre>
				<ul>
					<li>
						<p>
                            It is important to pass the vertices‚Äô world position to the pixel shader, because we will use that directly to index into our voxel grid data structure and write into it. We will also need texture coords and normals for correct diffuse color and lighting.
						</p>
					</li>
					<li>
						<p>
							<strong>
                                3.) Pixel shader
							</strong>
						</p>
					</li>
					<li>
						<p>
                            After the geometry shader, the rasterizer unit schedules some pixel shader invocations for our voxels, so in the pixel shader we determine the color of the voxel and write it into our data structure. We probably need to sample our base texture of the surface and evaluate direct lighting which affects the fragment (the voxel). While evaluating the lighting, use a forward rendering approach, so iterate through the nearby lights for the fragment and do the light calculations for the diffuse part of the light. Leave the specular out of it, because we don‚Äôt care about the view dependent part now, we want to be able to query lighting from any direction anyway later. I recommend using a simplified lighting model, but try to keep it somewhat consistent with your main lighting model which is probably a physically based model (at least it is for me and you should also have one) and account for the energy loss caused by leaving out the specularity.
						</p>
					</li>
					<li>
						<p>
                            When you calculated the color of the voxel, write it out by using the following trick: I didn‚Äôt bind a render target for the render pass, but I have set an Unordered Access View by calling OMSetRenderTargetsAndUnorderedAccessViews(). So the shader returns nothing, but we write into our voxel grid in the shader code. My voxel grid is a RWStructuredBuffer here to be able to support atomic operations easily, but later it will be converted to a 3D texture for easier filtering and better cache utilization. The Structured buffer is a linear array of VoxelType of size gridDimensions X
							<em>
                                Y
							</em>
                            Z. VoxelType is a structure holding a 32 bit uint for the voxel color (packed HDR color with 0-255 RGB, an emissive multiplier in 7 bits and the last bit indicates if the voxel is empty or not). The structure also contains a normal vector packed into a uint. Our interpolated 3D world position comes in handy when determining the write position into the buffer, just truncate and flatten the interpolated world position which you received from the geometry shader. For writing the results, you must use atomic max operations on the voxel uints. You could be writing to a texture here without atomic operations, but using rasterizer ordered views, but they don‚Äôt support volume resources, so a multi pass approach would be necessary for the individual slices of the texture.
						</p>
					</li>
					<li>
						<p>
                            An additional note: If you have generated shadow maps, you can use them in your lighting calculations here to get more proper illumination when cone tracing. If you don‚Äôt have shadow maps, you can even use the voxel grid to retrieve (soft) shadow information for the scene later.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<strong>
                        2.) Filtering the data
					</strong>
                    :
				</p>
			</li>
			<li>
				<p>
					<strong>
                        3.) Cone Tracing
					</strong>
                    :
				</p>
				<ul>
					<li>
						<p>
                            We have the voxel scene ready for our needs, so let‚Äôs query it for information. To gather the global illumination for the scene, we have to run the cone tracing in screen space for every pixel on the screen once. This can happen in the forward rendering object shaders or against the gbuffer in a deferred renderer, when rendering a full screen quad, or in a compute shader. In forward rendering, we may lose some performance because of the worse thread utilization if we have many small triangles. A Z-prepass is an absolute must have if we are doing this in forward rendering. We don‚Äôt want to shade a pixel multiple times because this is a heavy computation.
						</p>
					</li>
					<li>
						<p>
                            For diffuse light bounces, we need the pixel‚Äôs surface normal and world position at minimum. From the world position, calculate the voxel grid coordinate, then shoot rays in the direction of the normal and around the normal in a hemisphere. But the ray should not start at the surface voxel, but the next voxel along the ray, so we don‚Äôt accumulate the current surface‚Äôs lighting. Begin ray marching, and each step sample your voxel from increasing mip levels, accumulate color and alpha and when alpha reaches 1, exit and divide the distance travelled. Do this for each ray, and in the end divide the accumulated result with the number of rays as well. Now you have light bounce information and ambient occlusion information as well, just add it to your diffuse light buffer.
						</p>
					</li>
					<li>
						<p>
                            Assembling the hemisphere: You can create a hemisphere on a surface by using a static array of precomputed randomized positions on a sphere and the surface normal. First, if you do a 
							<em>
                                reflect(surfaceNormal, randomPointOnSphere),
							</em>
                            &nbsp;you get a random point on a sphere with variance added by the normal vector. This helps with banding as discrete precomputed points get modulated by surface normal. We still have a sphere, but we want the upper half of it, so check if a point goes below the ‚Äúhorizon‚Äù and force it to go to the other direction if it does:
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://docs.godotengine.org/en/4.4/tutorials/3d/global_illumination/using_voxel_gi.html" 
				class="external-link" 
				target="_blank" >
                VoxelGI - Godot
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    Good reflections and indirect lighting, but beware of leaks.
				</p>
			</li>
			<li>
				<p>
                    Due to its voxel-based nature, VoxelGI will exhibit light leaks if walls and floors are too thin. It's recommended to make sure all solid surfaces are at least as thick as one voxel.
				</p>
			</li>
			<li>
				<p>
                    Streaking artifacts may also be visible on sloped surfaces. In this case, tweaking the bias properties or rotating the VoxelGI node can help combat this.
				</p>
			</li>
			<li>
				<p>
					<em>
                        Performance
					</em>
                    :
				</p>
				<ul>
					<li>
						<p>
                            The bake's number of subdivisions can be adjusted to balance between performance and quality. The VoxelGI rendering quality can be adjusted in the Project Settings. The rendering can optionally be performed at half resolution (and then linearly scaled) to improve performance significantly.
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://www.cryengine.com/docs/static/engines/cryengine-5/categories/23756816/pages/25535599" 
				class="external-link" 
				target="_blank" >
                SVOGI - CryEngine
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=dyH0W-AVqu8" 
				class="external-link" 
				target="_blank" >
                SVOGI - Demo in Kingdom Come: Deliverance
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    The voxel based approach from the &quot;old&quot; CryEngine 3 was fascinating and it's a real shame very few games effectively supported it.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=4-KSMRjUqGU" 
				class="external-link" 
				target="_blank" >
                Voxel Cone Tracing GI - Demo 2011
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    I think it's actually viable for a commercial project. At very least as a fallback for RTX (where it's unsupported or inefficient). I also working on porting it to Unity HDRP.
				</p>
			</li>
			<li>
				<p>
                    Shame Voxel Cone Tracing/VXGI is not even used as a fallback whenever DXR (DirectX Raytracing) is not supported.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="voxel-cone-traced-reflections" >
    Voxel Cone Traced Reflections
</h3>
<ul>
	<li>
		<p>
            Sometimes used for specular approximation.
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=3tDLFceRXoM" 
				class="external-link" 
				target="_blank" >
                Voxel Traced GI/Reflections - Demo 2019
			</a>
            .
		</p>
	</li>
</ul>
<h3
	id="global-illumination-based-on-surfels-gibs-surfel-gi" >
    Global Illumination Based on Surfels (GIBS) (Surfel GI)
</h3>
<ul>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=h1ocYFrtsM4" 
				class="external-link" 
				target="_blank" >
                Global Illumination Based on Surfels - EA 2021
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<input
				type="checkbox" 
				disabled=""
>
            
			<a
				href="https://m4xc.dev/blog/surfel-maintenance/" 
				class="external-link" 
				target="_blank" >
                Surfel Maintenance for Global Illumination - January 2025
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<em>
                Impressions
			</em>
            :
		</p>
		<ul>
			<li>
				<p>
                    It is a technique different from the classic ones currently used. It does not use probes or voxels.
				</p>
			</li>
			<li>
				<p>
                    It is a somewhat elegant technique.
				</p>
			</li>
			<li>
				<p>
                    The presentation covers many of the points I had doubts about, it seems well developed.
				</p>
			</li>
			<li>
				<p>
                    I believe that if the camera moves very quickly, or a new scene appears in front of the player quickly, the technique suffers because it has to update all the surfels.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            Surfel-based GI is not inherently ray-traced, however, EA GIBS implementation (and several production uses) do use ray queries/ray tracing as the primary means to place surfels and to evaluate visibility/irradiance, so practical surfel-GI systems are often implemented with ray tracing.
		</p>
	</li>
	<li>
		<p>
            It is completely geared toward Raytracing, given its nature.
		</p>
	</li>
	<li>
		<p>
            Frostbite Engine / EA.
		</p>
	</li>
	<li>
		<p>
            An image is discretized in pixels, a surface is discretized in surfels.
		</p>
	</li>
	<li>
		<p>
            It's an interactive screen space gap filling.
		</p>
	</li>
	<li>
		<p>
            The screen is split into 16x16 tiles and the algorithm finds the tile with the lowest surfel coverage.
		</p>
	</li>
	<li>
		<p>
            If a tile passes a randomized threshold, spawn a surfel.
		</p>
	</li>
	<li>
		<p>
            The result is cached for further use.
		</p>
	</li>
	<li>
		<p>
            Uses Radial Gaussian Depth, inspired by DDGI.
		</p>
	</li>
	<li>
		<p>
            One surfel shares the irradiance with another surfel.
		</p>
	</li>
</ul>
<h3
	id="screen-space-indirect-lighting-ssil" >
    Screen-Space Indirect Lighting (SSIL)
</h3>
<ul>
	<li>
		<p>
            ‚ÄúWhere should I 
			<em>
                add
			</em>
            &nbsp;light because nearby surfaces reflect it?‚Äù
		</p>
	</li>
	<li>
		<p>
            SSIL tries to estimate the contribution of nearby surfaces reflecting light into shaded areas, using only the information already available in the screen-space buffers (depth, normals, and sometimes color).
		</p>
	</li>
	<li>
		<p>
            Can both darken (occlusion) and brighten areas by adding bounced light.
		</p>
	</li>
	<li>
		<p>
            Think of SSIL as color bleeding + soft bounce lighting in screen space.
		</p>
	</li>
	<li>
		<p>
            Many SSIL techniques use analytic / horizon-based or hemisphere sampling approaches, and some SSIL variants use screen-space ray-marching (ray-marching against the depth buffer). SSIL only becomes hardware/world-space ray-tracing if you explicitly add a BVH-trace pass (i.e. a hybrid that is no longer ‚Äúpure‚Äù screen-space).
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://docs.godotengine.org/en/4.4/tutorials/3d/environment_and_post_processing.html#screen-space-indirect-lighting-ssil" 
				class="external-link" 
				target="_blank" >
                SSIL - Godot
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    SSIL provides indirect lighting for small details or dynamic geometry that other global illumination techniques cannot cover. This applies to bounced diffuse lighting, but also emissive materials.
				</p>
			</li>
			<li>
				<p>
                    SSIL also provides a subtle ambient occlusion effect, similar to SSAO, but with less detail.
				</p>
			</li>
			<li>
				<p>
                    Good 
					<em>
                        secondary
					</em>
                    &nbsp;source of indirect lighting, but no reflections.
				</p>
			</li>
			<li>
				<p>
                    SSIL works best for small-scale details, as it cannot provide accurate indirect lighting for large structures on its own. SSIL can provide real-time indirect lighting in situations where other GI techniques fail to capture small-scale details or dynamic objects. Its screen-space nature will result in some artifacts, especially when objects enter and leave the screen. SSIL works using the last frame's color (before post-processing) which means that emissive decals and custom shaders are included (as long as they're present on screen).
				</p>
			</li>
			<li>
				<p>
					<em>
                        Usage
					</em>
                    :
				</p>
				<ul>
					<li>
						<p>
                            This feature only provides 
							<strong>
                                indirect lighting
							</strong>
                            . It is 
							<em>
                                not
							</em>
                            &nbsp;a full global illumination solution.
						</p>
						<ul>
							<li>
								<p>
                                    This makes it different from screen-space global illumination (SSGI) offered by other 3D engines.
								</p>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            SSIL is meant to be used as a complement to other global illumination techniques such as VoxelGI, SDFGI and LightmapGI.
						</p>
					</li>
					<li>
						<p>
                            SSIL can be combined with SSR and/or SSAO for greater visual quality (at the cost of performance).
						</p>
					</li>
					<li>
						<p>
                            When SSIL is enabled on its own, the effect may not be that noticeable, which is intended.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<em>
                        Performance
					</em>
                    :
				</p>
				<ul>
					<li>
						<p>
                            The SSIL quality and number of blur passes can be adjusted in the Project Settings. By default, SSIL rendering is performed at half resolution (and then linearly scaled) to ensure a reasonable performance level.
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="ssgi-screen-space-global-illumination" >
    SSGI (Screen-Space Global Illumination)
</h3>
<ul>
	<li>
		<p>
            &quot;Standard raytracing in screenspace&quot;.
		</p>
	</li>
	<li>
		<p>
            It is an image-space approximation that often uses screen-space ray-marching or hemisphere sampling (i.e. ‚Äúrays‚Äù marched through the depth buffer), which is different from hardware/true world-space ray tracing.
		</p>
	</li>
	<li>
		<p>
            Computes GI from information available in the screen buffer (depth, normals). Fast but limited to visible surfaces and can produce artifacts.
		</p>
	</li>
	<li>
		<p>
            Screen-space sampling / ray marching in screen-space. Not a full ray tracer.
		</p>
	</li>
	<li>
		<p>
            Most real-time SSGI implementations use temporal and/or spatial denoising (or temporal accumulation) because the raw results are noisy or contain high-frequency sampling error unless you pay a high sampling cost.
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=dmdyqzelBIY" 
				class="external-link" 
				target="_blank" >
                Implementing SSGI with Joint Bileteral Filtering as a Denoiser
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    It also discusses techniques for reducing GI resolution and using image upscaling; this improves performance with practically no visual difference.
				</p>
			</li>
			<li>
				<p>
                    Etc; several other small techniques are discussed.
				</p>
			</li>
			<li>
				<p>
                    All of this in the video was made to apply a 
					<a
						href="https://reshade.me/" 
						class="external-link" 
						target="_blank" >
                        ReShade
					</a>
                    &nbsp;to Skyrim; in the final seconds of the video the before-and-after difference with the new GI technique is shown.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://dev.epicgames.com/documentation/en-us/unreal-engine/screen-space-global-illumination-in-unreal-engine" 
				class="external-link" 
				target="_blank" >
                SSGI in Unreal Engine 5
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    Is a feature that aims to create natural-looking lighting by adding dynamic indirect lighting to objects within the screen view. SSGI also makes it possible to have dynamic lighting from emissive surfaces, such as neon lights or other bright surfaces.
				</p>
			</li>
			<li>
				<p>
                    Screen Space Global Illumination works best as a supplimental indirect lighting illumination method to precomputed lighting from 
					<a
						href="https://dev.epicgames.com/documentation/en-us/unreal-engine/global-illumination-in-unreal-engine" 
						class="external-link" 
						target="_blank" >
                        Lightmass
					</a>
                    .
				</p>
			</li>
			<li>
				<p>
                    SSGI, like other screen space effects, is best used in conjunction with other indirect lighting techniques, such as 
					<a
						href="https://dev.epicgames.com/documentation/en-us/unreal-engine/global-illumination-in-unreal-engine" 
						class="external-link" 
						target="_blank" >
                        precomputed lighting from lightmass
					</a>
                    . When you have large objects that block portions of the screen, SSGI becomes apparent when it's being used as the sole indirect lighting illumination for the scene. For example, using baked lighting reduces screen space artifacts when transitioning behind a large occluder where a bright object may be located. SSGI is recommended as a means to improve indirect lighting illumination in your scene but not as a sole indirect lighting method.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://docs.unity3d.com/Packages/com.unity.render-pipelines.high-definition@14.0/manual/Override-Screen-Space-GI.html" 
				class="external-link" 
				target="_blank" >
                SSGI in Unity
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://wickedengine.net/2024/12/wicked-engines-graphics-in-2024/" 
				class="external-link" 
				target="_blank" >
                Wicked Engine 2024
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    I also experimented with a screen space global illumination (SSGI) that I wanted to base on the ‚Äúmulti scale screen space ambient occlusion‚Äù (MSAO) that I got from the 
					<a
						href="https://github.com/microsoft/DirectX-Graphics-Samples/tree/master/MiniEngine" 
						class="external-link" 
						target="_blank" >
                        DirectX Miniengine
					</a>
                    .
				</p>
			</li>
			<li>
				<p>
                    Just like MSAO, I wanted SSGI to not have to use any temporal accumulation.
				</p>
			</li>
			<li>
				<p>
                    This technique currently can only add lighting, not remove it, but it‚Äôs meant to be used together with MSAO which handles only ambient occlusion. However, I might revisit and improve this because in real scenes I didn‚Äôt find its quality good enough, especially on small scale as I had to use a lot of blur to hide the sub-sampling.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            SSGI only provides bounce lighting for objects that are on the screen, meaning if you have a large red object that is bouncing red light into the scene, and you turn away, all the red light disappears.
		</p>
	</li>
	<li>
		<p>
            That‚Äôs a pretty big limitation but it can work in some cases, such as top-down camera views. Those usually don‚Äôt have significant overlapping elements and objects take up less of the screen space so lighting changes usually aren‚Äôt as jarring.
		</p>
	</li>
	<li>
		<p>
            I wouldn‚Äôt use it as the sole source of GI for a first/third person project, but it can be used to augment baked lighting, as it will provide bounce light and occlusion for movable objects/lights that otherwise wouldn‚Äôt.
		</p>
	</li>
	<li>
		<p>
            Think of SSGI as a subset/approximation of what 
			<em>
                path tracing
			</em>
            &nbsp;would compute, but with significant limitations:
		</p>
		<ul>
			<li>
				<p>
                    Path tracing can see the entire scene, SSGI only sees what‚Äôs on screen.
				</p>
			</li>
			<li>
				<p>
                    Path tracing accounts for multiple bounces, SSGI usually only approximates one diffuse bounce.
				</p>
			</li>
			<li>
				<p>
                    Path tracing is unbiased (given enough samples), SSGI is inherently biased.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            Bad implementations of SSGI can run worse than some Path Tracing implementations.
		</p>
	</li>
</ul>
<h3
	id="ssrtgi-screen-space-ray-traced-global-illumination" >
    SSRTGI (Screen-Space Ray-Traced Global Illumination)
</h3>
<ul>
	<li>
		<p>
			<em>
                SSGI vs SRTGI
			</em>
            :
		</p>
		<ul>
			<li>
				<p>
                    All SSRTGI implementations are SSGI (because they use screen buffers), but not all SSGI implementations are SSRTGI (because some use hemisphere sampling, cone approximations, analytic occlusion, or other non-ray-march methods).
				</p>
			</li>
			<li>
				<p>
                    Instead of sampling nearby pixels like SSGI does, SSRTGI performs actual ray marching or ray queries in screen space to simulate diffuse rays bouncing off surfaces.
				</p>
			</li>
			<li>
				<p>
                    Honestly... it seems the same thing, the term sounds interchangeable.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            Still limited to screen-space data (can‚Äôt see off-screen geometry), but captures more accurate light transport within the visible region.
		</p>
	</li>
	<li>
		<p>
            Usually produces better spatial coherence and more accurate occlusion than SSGI. Can also support multiple samples per pixel for more realistic diffuse scattering.
		</p>
	</li>
	<li>
		<p>
            Heavier than SSGI because of ray marching and denoising. However, still cheaper than full path tracing since it avoids tracing into the full scene BVH.
		</p>
	</li>
</ul>
<h3
	id="ray-traced-ambient-occlusion-rtao" >
    Ray-Traced Ambient Occlusion (RTAO)
</h3>
<ul>
	<li>
		<p>
			<a
				href="https://github.com/boksajak/RTAO" 
				class="external-link" 
				target="_blank" >
                RTAO Implementation
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    Ray Traced Ambient Occlusion (RTAO) implemented using DirectX Raytracing (DXR)
				</p>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="ray-traced-global-illumination-rtxgi" >
    Ray-Traced Global Illumination (RTXGI)
</h3>
<ul>
	<li>
		<p>
            NVIDIA‚Äôs RTXGI is implemented as volumes of probes (probe grids/DDGI-style) where probes are updated (ray-traced) and store irradiance/distance-to-geometry for shading.
		</p>
	</li>
	<li>
		<p>
            RTXGI fits into the modern game engine by directly replacing existing indirect lighting approaches such as screen-space ray casting, precomputed lightmaps, and baked irradiance probes. We combine ray tracing, fast irradiance updates, and a moment-based depth scheme for occlusion calculations to create a scalable system without bake times or light leaks. RTXGI is supported on any DXR-enabled GPU and provides developers with an ideal starting point to bring the benefits of real-time ray tracing to their existing tools, knowledge, and capabilities.
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.nvidia.com/en-us/on-demand/session/gtcsj20-s22692/" 
				class="external-link" 
				target="_blank" >
                RTXGI
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://github.com/NVIDIA-RTX/RTXGI" 
				class="external-link" 
				target="_blank" >
                RTXGI
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    RTXGI v2.0 Update including Neural Radiance Cache and Spatial Hash Radiance Cache
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://github.com/NVIDIAGameWorks/RTXGI-DDGI" 
				class="external-link" 
				target="_blank" >
                RTXGI
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<strong>
                Usage
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    RTXGI is less prominent as a mainstream, engine-integrated solution today.
				</p>
			</li>
			<li>
				<p>
                    Industry momentum has shifted toward hybrid ray-tracing + sampling/resampling approaches (Lumen, ReSTIR/RTXDI, hardware path tracing) and improved probe/volume variants (DDGI/modern probe grids).
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<em>
                Unreal Engine
			</em>
            :
		</p>
		<ul>
			<li>
				<p>
                    Available on UE4 with the RTXGI Plugin and NVIDIA maintained an RTX-focused UE branch, and some community forks claim UE5 support, but 
					<em>
                        RTXGI is not the default/integrated GI in mainline Unreal Engine 5
					</em>
                    &nbsp;and NVIDIA‚Äôs official plugin work was effectively put on hold/limited after early UE5 versions.
				</p>
			</li>
			<li>
				<p>
                    Unreal Engine 5 uses Lumen.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="ground-truth-ambient-occlusion-gtao" >
    Ground Truth Ambient Occlusion (GTAO)
</h3>
<ul>
	<li>
		<p>
			<a
				href="https://www.iryoku.com/downloads/Practical-Realtime-Strategies-for-Accurate-Indirect-Occlusion.pdf" 
				class="external-link" 
				target="_blank" >
                ‚ÄúPractical Realtime Strategies for Accurate Indirect Occlusion&quot; - 2016
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            Works almost identically to HBAO, but with a few key differences:
		</p>
		<ul>
			<li>
				<p>
                    The heavy math is moved outside of the loop, needed to be calculated once per slice, so the performance is comparable with HBAO+.
				</p>
			</li>
			<li>
				<p>
                    Consider the cosine of the angle, just like HBAO+ does.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="distance-field-ambient-occlusion-dfao" >
    Distance Field Ambient Occlusion (DFAO)
</h3>
<ul>
	<li>
		<p>
			<a
				href="https://advances.realtimerendering.com/s2015/DynamicOcclusionWithSignedDistanceFields.ppt" 
				class="external-link" 
				target="_blank" >
                Dynamic Occlusion with Signed Distance Fields - Unreal Engine 2015
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://dev.epicgames.com/documentation/en-us/unreal-engine/distance-field-ambient-occlusion-in-unreal-engine" 
				class="external-link" 
				target="_blank" >
                Unreal 5 DFAO
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            Unlike SSAO, occlusion is computed from world-space occluders, so there are no artifacts from missing data off-screen.
		</p>
	</li>
	<li>
		<p>
            It supports dynamic scene changes; the rigid meshes can be moved or hidden, and it will affect the occlusion.
		</p>
	</li>
	<li>
		<p>
            Distance Field AO quality is determined by the resolution of the Mesh Distance Field it represents. Since AO is very soft shadowing, so even if the surfaces aren't represented properly, occlusion further from the surface will be accurate. It's often not noticeable with sky occlusion. However, make sure that the larger details of the mesh are well represented in the Mesh Distance Field for good results.
		</p>
	</li>
	<li>
		<p>
            The cost of Distance Field AO is primarily GPU time and video memory. DFAO has been optimized such that it can run on medium-spec PC, PlayStation 4, and Xbox One. Currently, it has a much more reliable cost so that it's mostly constant (with a slight dependency on object-density).
		</p>
	</li>
	<li>
		<p>
            In cases with a static camera and mostly flat surfaces, DFAO is 1.6x faster when compared to earlier implementations. In complex scenes with foliage and a fast moving camera, the latest optimizations are 5.5x faster. The cost of Distance Field AO on PlayStation 4 for a full game scene is around 3.7ms.
		</p>
	</li>
	<li>
		<p>
            DFAO relies on an SDF that encodes the minimum signed distance from any point in space to the nearest surface (positive outside, negative inside, or a variant with only unsigned distances).
		</p>
	</li>
</ul>
<h3
	id="scalable-ambient-obscurance-hbao-sao" >
    Scalable Ambient Obscurance (HBAO+ / SAO)
</h3>
<ul>
	<li>
		<p>
			<a
				href="https://research.nvidia.com/sites/default/files/pubs/2012-06_Scalable-Ambient-Obscurance/McGuire12SAO.pdf" 
				class="external-link" 
				target="_blank" >
                &quot;Scalable Ambient Obscurance&quot; - 2012
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            It had nothing to do with HBAO, it's actually an optimization of Alchemy Screen-Space Ambient Occlusion.
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250919082620.png" width="450" >
            .
		</p>
	</li>
</ul>
<h3
	id="hierarhcial-digital-differential-analyzer-global-illumination-hddagi" >
    Hierarhcial Digital Differential Analyzer Global Illumination (HDDAGI)
</h3>
<ul>
	<li>
		<p>
			<a
				href="https://research.dreamworks.com/wp-content/uploads/2018/07/talk_hierarchical_digital_OpenVDB_v4-Edited.pdf" 
				class="external-link" 
				target="_blank" >
                Hierarhcial Digital Differential Analyzer for Efficient Ray-Marching in OpenVDB - 2013
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://github.com/godotengine/godot/pull/86267" 
				class="external-link" 
				target="_blank" >
                PR para HDDAGI para Godot
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=9Dj9lvBkY-o" 
				class="external-link" 
				target="_blank" >
                HDDAGI - Godot Demo
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            This is a new global illumination system meant to supersede SDFGI.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Key advantages are
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Much, much faster. Significantly lower frame time, orders of magnitude faster cascad
				</p>
			</li>
			<li>
				<p>
                    Generally higher quality (less arctifacting).
				</p>
			</li>
			<li>
				<p>
                    Much better occclusion (a¬†
					<em>
                        lot
					</em>
                    ¬†less light leaked).
				</p>
			</li>
			<li>
				<p>
                    Less memory usage.
                    <br>
                    It is meant as a drop-in replacement. Should work as a replacement for SDFGI.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Known issues
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    ¬†For some reason gets DEVICE LOST on Intel GPUs. No idea why.
				</p>
			</li>
			<li>
				<p>
                    ¬†Sharp Reflections not always play good with TAA (wobbly).
				</p>
			</li>
			<li>
				<p>
                    ¬†Darkening (occlusion) on some corners, just like SDFGI. I tried different techniques to see if any worked better. DDGI Octahedral VSM gets rid of them, but also leaks a lot more light, so I am unconvinced. Have other ideas to try, but I don‚Äôt have infinite time üôÅ
				</p>
			</li>
			<li>
				<p>
                    ¬†SDFGI spherical harmonics turned out to be buggy and not energy conserving. This makes GI look more saturated and have more light than in HDDAGI (which some people may appreciate more), but It‚Äôs a bug üò¢. Wondering how this can be compensated.
				</p>
			</li>
			<li>
				<p>
                    ¬†Still some further pending optimizations.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                Future
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    ¬†Dynamic object support.
				</p>
			</li>
			<li>
				<p>
                    ¬†High density mode (sub-probes).
				</p>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="sdfgi-signed-distance-field-global-illumination" >
    SDFGI (Signed Distance Field Global Illumination)
</h3>
<ul>
	<li>
		<p>
			<a
				href="https://www.docdroid.net/YNntL0e/godot-sdfgi-pdf" 
				class="external-link" 
				target="_blank" >
                &quot;SDFGI Solving the accessible Global Illumination problem in Godot&quot; - Outubro 2022
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    This paper is 
					<em>
                        absolutely
					</em>
                    &nbsp;important to understand how SDFGI was made.
				</p>
			</li>
			<li>
				<p>
                    Requirements
				</p>
				<ul>
					<li>
						<p>
                            Easy to use (no scene or object setup at import time, no setting up SDF, cards,
						</p>
					</li>
					<li>
						<p>
                            lightmaps, etc). Ideally enable with one click, no set-up.
						</p>
					</li>
					<li>
						<p>
                            Real-time (or at least fast updates).
						</p>
					</li>
					<li>
						<p>
                            Good enough quality (no light leaks -or keep to minimum-).
						</p>
					</li>
					<li>
						<p>
                            Supports both diffuse and reflected light.
						</p>
					</li>
					<li>
						<p>
                            Supports light into transparent objects.
						</p>
					</li>
					<li>
						<p>
                            Works as source of light for volumetric fog.
						</p>
					</li>
					<li>
						<p>
                            Works in all hardware that supports Vulkan, even IGP.
						</p>
					</li>
					<li>
						<p>
                            Can work in VR (so, using TAA is not required).
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Sacrifices
				</p>
				<ul>
					<li>
						<p>
                            Not the best possible quality (high frequency GI missing, has to be&nbsp;&nbsp;compensated with screen space lighting).
						</p>
					</li>
					<li>
						<p>
                            Poor dynamic object support (dynamic objects get light from environment, but don't contribute to it). Light blocking may be added to some extent in the future.
						</p>
					</li>
					<li>
						<p>
                            Needs to use cascades.
						</p>
					</li>
					<li>
						<p>
                            Limited amount of samples means small emissive objects are spotty.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Previous work
				</p>
				<ul>
					<li>
						<p>
                            Uses DDGI by Morgan McGuire as a base.
						</p>
					</li>
					<li>
						<p>
                            Uses Signed Distance Fields generated with 
							<a
								href="https://www.comp.nus.edu.sg/~tants/jfa.html" 
								class="external-link" 
								target="_blank" >
                                Jump Flood
							</a>
                            .
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://arxiv.org/pdf/2007.14394" 
				class="external-link" 
				target="_blank" >
                Signed Distance Fields Dynamic Diffuse Global Illumination - 2020
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    Global Illumination (GI) is of utmost importance in the field of photo-realistic rendering. However, its computation has always been very complex, especially diffuse GI. State of the art real-time GI methods have limitations of different nature, such as light leaking, performance issues, special hardware requirements, noise corruption, bounce number limitations, among others. To overcome these limitations, we propose a novel approach of computing dynamic diffuse GI with a signed distance fields approximation of the scene and discretizing the space domain of the irradiance function. With this approach, we are able to estimate real-time diffuse GI for dynamic lighting and geometry, without any precomputations and supporting multi-bounce GI, providing good quality lighting and high performance at the same time. Our algorithm is also able to achieve better scalability, and manage both large open scenes and indoor high-detailed scenes without being corrupted by noise.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<em>
                My opinions
			</em>
            :
		</p>
		<ul>
			<li>
				<p>
                    Knowing the Godot implementation, I find the visuals simply uninspiring.
				</p>
			</li>
			<li>
				<p>
                    It sounds bad for me to go for this solution when I didn't even use this for Godot games.
				</p>
			</li>
			<li>
				<p>
                    I've always found the performance somewhat poor, but it could simply be the low-quality implementation in the editor.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            It‚Äôs not screen-space (since it doesn‚Äôt rely only on what‚Äôs visible on screen).
		</p>
	</li>
	<li>
		<p>
            It‚Äôs also not a full path-traced solution; instead it‚Äôs a form of voxel-based GI accelerated by signed distance fields.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Future in Godot
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    SDFGI will be replaced either way as it has many limitations that can't be resolved (such as slow cascade generation speed).
				</p>
			</li>
			<li>
				<p>
                    A bounded GI implementation to supersede VoxelGI may be added in the future, but it's not guaranteed.
				</p>
			</li>
			<li>
				<p>
                    There are plans to replace it with HDDAGI.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<strong>
                SDFGI vs VoxelGI
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    SDFGI provides better real-time ability than baked lightmaps, but worse real-time ability than VoxelGI.
				</p>
				<ul>
					<li>
						<p>
                            SDFGI supports dynamic lights, but 
							<em>
                                not
							</em>
                            &nbsp;dynamic occluders or dynamic emissive surfaces.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    Using 
					<em>
                        Use Occlusion
					</em>
                    &nbsp;has a small performance cost, but it often results in fewer leaks compared to VoxelGI.
				</p>
			</li>
			<li>
				<p>
                    Newer on Godot, when compared to VoxelGI. SDFGI was implemented as a new feature in Godot 4.0 release.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://docs.godotengine.org/en/4.4/tutorials/3d/global_illumination/using_sdfgi.html#doc-using-sdfgi" 
				class="external-link" 
				target="_blank" >
                SDFGI - Godot
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
					<a
						href="https://www.youtube.com/watch?v=QFKPrDv-Ue8" 
						class="external-link" 
						target="_blank" >
                        SDFGI - Godot Demo
					</a>
                    .
				</p>
			</li>
			<li>
				<p>
                    SDFGI is something akin to a dynamic real-time lightmap (but it does not requiere unwrapping, nor does it use textures). It‚Äôs enabled and it automatically works by generating global illumination for static objects. It does not require raytracing, and it runs in most current (and some years old) dedicated GPUs, even medium-end budget CPUs from some years ago (SDFGI was developed and tested on a GeForce 1060, running at a stable 60 FPS).
				</p>
			</li>
			<li>
				<p>
                    Light changes 
					<strong>
                        are real-time
					</strong>
                    , meaning any change in lighting conditions will result in an 
					<strong>
                        immediate update
					</strong>
                    . Dynamic objects are supported only for receiving light from the environment, but they don‚Äôt contribute to lighting. Some degree of support is planned for this eventually, but not immediately.
				</p>
			</li>
			<li>
				<p>
                    SDFGI also supports specular reflections, 
					<strong>
                        both sharp and rough
					</strong>
                    , so full PBR scenes should ‚Äújust work‚Äù. In the image below you can see both of them in checkerboard roughness texture.
				</p>
			</li>
			<li>
				<p>
                    SDFGI is mostly leak free, unlike VCT techniques which are the most common in use today (like SVOGI/GIProbe/etc). As long as walls are thicker than a voxel for a given cascade, light won‚Äôt go through.
				</p>
				<ul>
					<li>
						<p>
                            Leaks can be reduced significantly by enabling the 
							<em>
                                Use Occlusion
							</em>
                            &nbsp;property.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    GI level of detail varies depending on the distance between the camera and surface.
				</p>
			</li>
			<li>
				<p>
					<em>
                        Caviats / Issues
					</em>
                    :
				</p>
				<ul>
					<li>
						<p>
                            Cascade shifts may be visible when the camera moves fast. This can be made less noticeable by adjusting the cascade sizes or using fog.
						</p>
					</li>
					<li>
						<p>
                            Good reflections and indirect lighting, but beware of leaks and visible cascade shifts.
						</p>
					</li>
					<li>
						<p>
                            SDFGI has some downsides due to its cascaded nature. When the camera moves, cascade shifts may be visible in indirect lighting.
						</p>
						<ul>
							<li>
								<p>
                                    This can be alleviated by adjusting the cascade size, but also by adding fog (which will make distant cascade shifts less noticeable).
								</p>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            Performance will suffer if the camera moves too fast. This can be fixed in two ways:
						</p>
						<ul>
							<li>
								<p>
                                    Ensuring the camera doesn't move too fast in any given situation.
								</p>
							</li>
							<li>
								<p>
                                    Temporarily disabling SDFGI in the Environment resource if the camera needs to be moved at a high speed, then enabling SDFGI once the camera speed slows down.
								</p>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            When SDFGI is enabled, it will also take some time for global illumination to be fully converged (25 frames by default). This can create a noticeable transition effect while GI is still converging.
						</p>
						<ul>
							<li>
								<p>
                                    To hide this, you can use a ColorRect node that spans the whole viewport and fade it out when switching scenes using an AnimationPlayer node.
								</p>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            The signed distance field is only updated when the camera moves in and out of a cascade. This means that if geometry is modified in the distance, the global illumination appearance will be correct once the camera gets closer. However, if a nearby object with a bake mode set to Static or Dynamic is moved (such as a door), the global illumination will appear incorrect until the camera moves away from the object.
						</p>
					</li>
					<li>
						<p>
                            SDFGI's sharp reflections are only visible on opaque materials. Transparent materials will only use rough reflections, even if the material's roughness is lower than 0.2.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<em>
                        Performance
					</em>
                    :
				</p>
				<ul>
					<li>
						<p>
                            The number of cascades can be adjusted to balance performance and quality. The number of rays thrown per frame can be adjusted in the Project Settings. The rendering can optionally be performed at half resolution (and then linearly scaled) to improve performance significantly.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<em>
                        Setting up
					</em>
                    :
				</p>
				<ol>
					<li>
						<p>
                            Make sure your MeshInstance nodes have their 
							<strong>
                                Global Illumination &gt; Mode
							</strong>
                            &nbsp;property set to 
							<strong>
                                Static
							</strong>
                            &nbsp;in the inspector.
						</p>
						<ul>
							<li>
								<p>
                                    Any Mesh can receive Indirect Lighting, but only static meshes can contribute to Indirect Lighting.
								</p>
							</li>
							<li>
								<p>
                                    For meshes:
								</p>
								<ul>
									<li>
										<p>
											<strong>
                                                Disabled:
											</strong>
										</p>
										<ul>
											<li>
												<p>
                                                    The mesh won't be taken into account in SDFGI generation. The mesh will receive indirect lighting from the scene, but it will not contribute indirect lighting to the scene.
												</p>
											</li>
										</ul>
									</li>
									<li>
										<p>
											<strong>
                                                Static (default):
											</strong>
										</p>
										<ul>
											<li>
												<p>
                                                    The mesh will be taken into account in SDFGI generation. The mesh will both receive 
													<em>
                                                        and
													</em>
                                                    &nbsp;contribute indirect lighting to the scene. If the mesh is changed in any way after SDFGI is generated, the camera must move away from the object then move back close to it for SDFGI to regenerate. Alternatively, SDFGI can be toggled off and back on. If neither is done, indirect lighting will look incorrect.
												</p>
											</li>
										</ul>
									</li>
									<li>
										<p>
											<strong>
                                                Dynamic (not supported with SDFGI):
											</strong>
										</p>
										<ul>
											<li>
												<p>
                                                    The mesh won't be taken into account in SDFGI generation. The mesh will receive indirect lighting from the scene, but it will not contribute indirect lighting to the scene.
												</p>
											</li>
											<li>
												<p>
                                                    This acts 
													<em>
                                                        identical
													</em>
                                                    &nbsp;to the Disabled bake mode when using SDFGI.
												</p>
											</li>
										</ul>
									</li>
								</ul>
							</li>
							<li>
								<p>
                                    For lights:
								</p>
								<ul>
									<li>
										<p>
											<strong>
                                                Disabled:
											</strong>
										</p>
										<ul>
											<li>
												<p>
                                                    The light will 
													<em>
                                                        not
													</em>
                                                    &nbsp;be taken into account for SDFGI baking. The light won't contribute indirect lighting to the scene.
												</p>
											</li>
										</ul>
									</li>
									<li>
										<p>
											<strong>
                                                Static:
											</strong>
										</p>
										<ul>
											<li>
												<p>
                                                    The light will be taken into account for SDFGI baking. The light will contribute indirect lighting to the scene. If the light is changed in any way after baking, indirect lighting will look incorrect until the camera moves away from the light and back (which causes SDFGI to be baked again). will look incorrect. If in doubt, use this mode for level lighting.
												</p>
											</li>
										</ul>
									</li>
									<li>
										<p>
											<strong>
                                                Dynamic (default):
											</strong>
										</p>
										<ul>
											<li>
												<p>
                                                    The light won't be taken into account for SDFGI baking, but it will still contribute indirect lighting to the scene in real-time. This option is slower compared to 
													<strong>
                                                        Static
													</strong>
                                                    . Only use the 
													<strong>
                                                        Dynamic
													</strong>
                                                    &nbsp;global illumination mode on lights that will change significantly during gameplay.
												</p>
											</li>
										</ul>
									</li>
								</ul>
							</li>
						</ul>
					</li>
					<li>
						<p>
                            Add a WorldEnvironment node and create an Environment resource for it.
						</p>
					</li>
					<li>
						<p>
                            Edit the Environment resource, scroll down to the 
							<strong>
                                SDFGI
							</strong>
                            &nbsp;section and unfold it.
						</p>
					</li>
					<li>
						<p>
                            Enable 
							<strong>
                                SDFGI &gt; Enabled
							</strong>
                            .
						</p>
					</li>
				</ol>
				<ul>
					<li>
						<p>
                            SDFGI will automatically follow the camera when it moves, so you do not need to configure extents (unlike VoxelGI).
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<a
						href="https://docs.godotengine.org/en/4.4/tutorials/3d/global_illumination/using_sdfgi.html#environment-sdfgi-properties" 
						class="external-link" 
						target="_blank" >
                        Properties
					</a>
                    .
				</p>
			</li>
			<li>
				<p>
					<a
						href="https://docs.godotengine.org/en/4.4/tutorials/3d/global_illumination/using_sdfgi.html#adjusting-sdfgi-performance-and-quality" 
						class="external-link" 
						target="_blank" >
                        Performance and Quality
					</a>
                    .
				</p>
			</li>
			<li>
				<p>
                    To make a specific light emit more or less indirect energy without affecting the amount of direct light emitted by the light, adjust the 
					<strong>
                        Indirect Energy
					</strong>
                    &nbsp;property in the Light3D inspector.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            It is not a screen-space effect, so it can provide global illumination for off-screen elements (unlike SSIL).
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250918145006.png" width="400" >
            .
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250918145148.png" width="400" >
            .
		</p>
	</li>
</ul>
<h3
	id="lighting-grid-hierarchy" >
    Lighting Grid Hierarchy
</h3>
<ul>
	<li>
		<p>
			<a
				href="https://www.cemyuksel.com/research/lgh/real-time_rendering_with_lgh_i3d2019.pdf" 
				class="external-link" 
				target="_blank" >
                Real-Time Rendering with Lighting Grid Hierarchy - 2019
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.cemyuksel.com/research/lgh/lgh.pdf" 
				class="external-link" 
				target="_blank" >
                Lighting Grid Hierarchy for Self-illuminating Explosions - Siggraph 2017
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.cemyuksel.com/research/lgh/" 
				class="external-link" 
				target="_blank" >
                Lighting Grid Hierarchy
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            Focusing on temporal coherency to avoid flickering in animations, we introduce 
			<em>
                lighting grid hierarchy
			</em>
            &nbsp;for approximating the volumetric illumination at different resolutions. Using this structure we can efficiently approximate the lighting at any point inside or outside of the explosion volume as a mixture of lighting contributions from all levels of the hierarchy. As a result, we are able to capture high-frequency details of local illumination, as well as the potentially strong impact of distant illumination. Most importantly, this hierarchical structure allows us to efficiently precompute volumetric shadows, which substantially accelerates the lighting computation. Finally, we provide a scalable approach for computing the multiple scattering of light within the smoke volume using our lighting grid hierarchy.
		</p>
	</li>
	<li>
		<p>
            Ray tracing 
			<em>
                can
			</em>
            &nbsp;be layered on top for visibility injection, but the technique itself does not depend on it.
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=BHQ4RhWl-vc" 
				class="external-link" 
				target="_blank" >
                Lighting Grid Hierarchy with Raytracing Hardware Demo
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    100k VPL (100.000 Virtual Point Lights).
				</p>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="screen-space-directional-occlusion-ssdo" >
    Screen-Space Directional Occlusion (SSDO)
</h3>
<ul>
	<li>
		<p>
			<a
				href="https://ascane.github.io/projects/06_ssdo/report.pdf" 
				class="external-link" 
				target="_blank" >
                Real-time Approximated Global Illumination From SSAO to SSDO - 2016
			</a>
            .
		</p>
	</li>
</ul>
<h3
	id="precomputed-radiance-transfer-prt" >
    Precomputed Radiance Transfer (PRT)
</h3>
<ul>
	<li>
		<p>
			<strong>
                My understanding
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    Based on the explanations in &quot;A Data-Driven Paradigm for Precomputed Radiance Transfer - 2022&quot; and &quot;Neural Precomputed Radiance Transfer - 2022&quot;, I understood that this strategy is an intermediate between baking and dynamic.
				</p>
			</li>
			<li>
				<p>
                    The idea would be to compute different types of lighting on a mesh, choosing which to apply based on the mesh's current direct lighting conditions.
				</p>
			</li>
			<li>
				<p>
                    It uses Spherical Harmonics directly, but another basis can be used.
				</p>
			</li>
			<li>
				<p>
                    ChatGPT:
				</p>
				<ul>
					<li>
						<p>
                            PRT is a technique that moves expensive, direction-dependent light transport calculations offline so that, at runtime, shading under complex (often dynamic) lighting can be evaluated with a small number of dot-products.
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://jankautz.com/publications/prtSIG02.pdf" 
				class="external-link" 
				target="_blank" >
                Precomputed Radiance Transfer for Real-Time Rendering in Dynamic, Low-Frequency Lighting Environments - Siggraph 2002
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250921164439.png" width="400" >
            .
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250921154944.png" width="400" >
            .
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250921160349.png" width="350" >
            .
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250921160533.png" width="350" >
            .
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250921162247.png" width="500" >
            .
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250921162207.png" width="450" >
            .
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250921162225.png" width="450" >
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=k3hzcrjoFbc" 
				class="external-link" 
				target="_blank" >
                A Data-Driven Paradigm for Precomputed Radiance Transfer - Unity 2022
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
					<a
						href="https://arxiv.org/pdf/2206.13112" 
						class="external-link" 
						target="_blank" >
                        A Data-Driven Paradigm for Precomputed Radiance Transfer - Unity 2022
					</a>
                    .
				</p>
			</li>
			<li>
				<p>
                    Very interesting.
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250921162915.png" width="450" >
                    .
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250921162927.png" width="450" >
                    .
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=voyOq7VJfl0" 
				class="external-link" 
				target="_blank" >
                Neural Precomputed Radiance Transfer - 2022
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    Neural PRT:
				</p>
				<ul>
					<li>
						<p>
                            <img src="assets/image_20250921160243.png" width="500" >
                            .
						</p>
					</li>
					<li>
						<p>
                            A CNN encoder is the part of a convolutional neural network (CNN) that compresses input data into a compact, meaningful representation called a latent representation. It functions by progressively down-sampling and extracting hierarchical features from the input, such as an image, through a series of convolutional and pooling layers. The goal of the encoder is to reduce the input's dimensionality while retaining essential information, making it a compressed version of the original data that can then be used by a 
							<a
								href="https://www.google.com/search?client=firefox-b-d&cs=1&sca_esv=730c08a984f2ad03&q=decoder&sa=X&ved=2ahUKEwjG282txeqPAxU_DLkGHUz-FOgQxccNegQIBRAB&mstk=AUtExfAVbBz0aoEqs_sqNypyuG83mjn4wS_fhnPddU8y0J29NIhf6yRTh4vEVUQ_JRo4od2X2VFHzl7uOlei5viFjVOo2ubHYl-lOzoZVzSoyHn-pwoefuSFLzyUbrZRSyraUUPn1UR73Cjq92zGAU1UFYc2dLo2WUIHl5vl17vIr3vnnj4&csui=3" 
								class="external-link" 
								target="_blank" >
                                decoder
							</a>
                            &nbsp;for tasks like reconstruction or pixel-level prediction
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250921161241.png" width="500" >
                    .
				</p>
				<ul>
					<li>
						<p>
                            &quot;Even with 5 circle harmonic bands (25 coefficients), spherical harmonics tend to cutoff the high frequency angular signals, this is visible on the mirror, for example&quot;.
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
	<li>
		<p>
            Instead of dealing with rays, PRT deals with functions on a sphere.
		</p>
	</li>
	<li>
		<p>
            Traditional PRT suggests as choice of basis:
		</p>
		<ul>
			<li>
				<p>
                    Spherical Harmonics
				</p>
			</li>
			<li>
				<p>
                    Haar Wavelets.
				</p>
			</li>
			<li>
				<p>
                    Spherical Gaussians.
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250921155754.png" width="500" >
                    .
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            The choice of basis has been one of the main domains of research.
		</p>
	</li>
	<li>
		<p>
			<strong>
                Usage
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    It is designed for 
					<em>
                        static
					</em>
                    &nbsp;geometry (or limited deformation) and is primarily targeted at low-frequency environment lighting and shadowing / soft interreflection effects that can be precomputed.
				</p>
			</li>
			<li>
				<p>
                    Partially replaced by real-time ray-tracing / dynamic probe systems for workflows that require runtime changes. PRT is efficient for low-frequency, mostly-static content (precomputation), but for highly dynamic environments engines increasingly use real-time methods (RT + denoising, probe volumes) to handle scene changes.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=04YUZ3bWAyg" 
				class="external-link" 
				target="_blank" >
                PRT Probes - The Division 1 - 2016
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    It is not normal PRT, but rather PRT Probes.
				</p>
			</li>
			<li>
				<p>
                    I watched about the first 20 minutes and some other segments of the video.
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250921163440.png" width="450" >
                    .
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250921164329.png" width="400" >
                    <img src="assets/image_20250921164606.png" width="450" >
                    .
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250921164546.png" width="450" >
                    .
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250921165103.png" width="450" >
                    .
				</p>
				<ul>
					<li>
						<p>
                            After considerations, we settled on the Half Life 2 Ambient Cube Basis (HL2), which is not a real basis but 6 vectores aligned; so it requires only six floats.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    The probes are placed automatically via a 4x4 meters raycast grid, spawning a probe on every ray hit; also, spawn probes alongside building walls, to avoid them looking flat.
				</p>
			</li>
			<li>
				<p>
                    The storage on disk is via a 2D grid with 64x64 meters, with maximum of 1000 probes, but usually 200 probes per sector. The sectors are streamed in and out as the player moves.
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250921165504.png" width="500" >
                    .
				</p>
			</li>
			<li>
				<p>
                    <img src="assets/image_20250921165706.png" width="450" >
                    .
				</p>
				<ul>
					<li>
						<p>
                            Manhattan == Manhattan city map.
						</p>
					</li>
				</ul>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="voxel-ambient-occlusion-vxao" >
    Voxel Ambient Occlusion (VXAO)
</h3>
<ul>
	<li>
		<p>
            Not in screen space.
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://www.nvidia.com/en-us/geforce/news/nvidia-vxao-voxel-ambient-occlusion/" 
				class="external-link" 
				target="_blank" >
                Nvidia VXAO - 2016
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            Unlike VXGI which controls all illumination, VXAO is only utilized for Ambient Occlusion, enabling us to integrate it into a wider array of games and game engines that use traditional illumination technologies.
		</p>
	</li>
	<li>
		<p>
            It‚Äôs more accurate than SSAO and its derivatives, casts deeper, richer shadows that account for even the smallest details in a scene, and runs faster than other competing effects when they‚Äôre rendered at a similar quality.
		</p>
	</li>
	<li>
		<p>
            Even still, HBAO+‚Äôs Ambient Occlusion shadowing is far from the level of fidelity offered by VXAO, which avoids the caveats of screen space techniques, enabling us to deliver the most accurate and realistic Ambient Occlusion shadowing seen to date.
		</p>
	</li>
	<li>
		<p>
            With VXAO, occlusion and lighting information is gathered from a ‚Äòworld space‚Äô voxel representation of the scene, which takes into account a large area around the viewer. Included in this voxelization are objects and details currently invisible to the viewer, and those behind the viewer, too. The result is scene-wide Ambient Occlusion shadowing, instead of ‚Äòscreen space‚Äô shadowing based on what you can currently see. This allows AO shadows to be cast into a scene from objects near to the player but just outside of their view, and from occluded objects in the distance large enough to affect the appearance of the scene.
		</p>
	</li>
	<li>
		<p>
            Use on:
		</p>
		<ul>
			<li>
				<p>
                    Rise of the Tomb Raider.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=VWiSex0NhlI" 
				class="external-link" 
				target="_blank" >
                SSAO vs HBAO+ vs VXAO, Rise of the Tomb Raider
			</a>
            .
		</p>
	</li>
</ul>
<h3
	id="mssao-multi-resolution-screen-space-ambient-occlusion" >
    MSSAO (Multi-Resolution Screen-Space Ambient Occlusion)
</h3>
<ul>
	<li>
		<p>
			<a
				href="https://www.comp.nus.edu.sg/~lowkl/publications/mssao_cgi2011.pdf" 
				class="external-link" 
				target="_blank" >
                MSSAO 2011
			</a>
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250917182637.png" width="400" >
            .
		</p>
	</li>
	<li>
		<p>
            You can combine results from different occlusion radius.
		</p>
	</li>
	<li>
		<p>
            It is a technique from a 2010 paper.
		</p>
	</li>
</ul>
<h3
	id="ssao-screen-space-ambient-occlusion" >
    SSAO (Screen-Space Ambient Occlusion)
</h3>
<ul>
	<li>
		<p>
            ‚ÄúWhere should I 
			<em>
                remove
			</em>
            &nbsp;light because it‚Äôs blocked?‚Äù
		</p>
	</li>
	<li>
		<p>
            Ray Traced Ambient Occlusion itself is an approximation of Indirect Light, and SSAO is an approximation of Ray Traced Ambient Occlusion.
		</p>
		<ul>
			<li>
				<p>
                    So it's an approximation of an approximation.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            SSAO only acts on 
			<em>
                ambient light
			</em>
            . It does not affect direct light.
		</p>
	</li>
	<li>
		<p>
            Think of SSAO as a shadow-only pass, faking soft contact shadows.
		</p>
	</li>
	<li>
		<p>
            Ray Traced AO would be the &quot;correct&quot; ambient occlusion, but that is kinda difficult to say as Ambient Occlusion itself is a &quot;fake term&quot;; SSAO approximates it.
		</p>
	</li>
	<li>
		<p>
            It's much faster then Ray Traced AO.
		</p>
	</li>
	<li>
		<p>
            Approximates ambient occlusion as a cheap GI term; bent normals can guide diffuse light injection.
		</p>
	</li>
	<li>
		<p>
            Screen-space sampling; rasterization-based.
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://docs.godotengine.org/en/4.4/tutorials/3d/environment_and_post_processing.html#screen-space-ambient-occlusion-ssao" 
				class="external-link" 
				target="_blank" >
                SSAO - Godot
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    If you want to force SSAO to work with direct light too, use the 
					<strong>
                        Light Affect
					</strong>
                    &nbsp;parameter. Even though this is not physically correct, some artists like how it looks.
				</p>
			</li>
			<li>
				<p>
                    SSAO looks best when combined with a real source of indirect light, like 
					<em>
                        VoxelGI
					</em>
                    .
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            <img src="assets/image_20250917181805.png" width="450" >
            .
		</p>
	</li>
</ul>
<h3
	id="combined-adaptive-compute-ambient-occlusion-cacao" >
    Combined Adaptive Compute Ambient Occlusion (CACAO)
</h3>
<ul>
	<li>
		<p>
			<a
				href="https://gpuopen.com/fidelityfx-cacao/" 
				class="external-link" 
				target="_blank" >
                AMD FidelityFX Combined Adaptive Compute Ambient Occlusion
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            Released in May 2020.
		</p>
	</li>
	<li>
		<p>
            As of 2023, it became part of the AMD FidelityFX SDK.
		</p>
	</li>
	<li>
		<p>
            Updated in May 2025.
		</p>
	</li>
	<li>
		<p>
            &quot;Artist control, etc, but deviates from the rendering equation&quot;.
		</p>
	</li>
</ul>
<h3
	id="screen-space-global-illumination-based-invariance-ssvgi" >
    Screen-Space Global Illumination Based-Invariance (SSVGI)
</h3>
<ul>
	<li>
		<p>
            The author of Radiance Cascading worked on this technique for PoE1.
		</p>
	</li>
	<li>
		<p>
            Uses exclusively image space data.
		</p>
	</li>
	<li>
		<p>
            Calculates GI for every point from scratch.
		</p>
	</li>
	<li>
		<p>
            Still needs denoising.
		</p>
	</li>
	<li>
		<p>
            Uses Screen Space Shadow Hierarchy.
		</p>
		<ul>
			<li>
				<p>
                    &quot;Shadow cascade&quot;.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="light-propagation-volumes-lpv" >
    Light Propagation Volumes (LPV)
</h3>
<ul>
	<li>
		<p>
            First introduced by Crytek in 2009.
		</p>
	</li>
	<li>
		<p>
            LV calculation of global illumination consists of three steps:
		</p>
		<ul>
			<li>
				<p>
                    Injection virtual points lights obtained from Reflective Shadow Maps into LPV 3D grid.
				</p>
			</li>
			<li>
				<p>
                    Propagation of light intensity in grid stored in spherical harmonics coefficients.
				</p>
			</li>
			<li>
				<p>
                    Lookup for light intensity in LPV while scene rendering.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            LPV sits in the same family as Voxel-GI (both use a 3D grid) and is functionally closer to runtime probe/volume methods like DDGI than to precomputed PRT ‚Äî but LPV‚Äôs propagation approach and data layout make its behavior and trade-offs distinct.
		</p>
		<ol>
			<li>
				<p>
                    Inject light into a 3D regular grid (the ‚Äúvolume‚Äù) from direct light sources or from virtual point lights / reflective shadow maps. The injected values are usually stored as low-order spherical harmonics (SH) or simple directional bands per cell.
				</p>
			</li>
			<li>
				<p>
                    Iteratively propagate those values between neighboring grid cells (a diffusion / scattering sweep). The propagation step moves energy through the grid and approximates multiple diffuse bounces.
				</p>
			</li>
			<li>
				<p>
                    At shading time, the renderer samples the grid (trilinear / tetrahedral interpolation) and uses the sampled radiance/SH to illuminate surfaces (usually only the diffuse term).
				</p>
			</li>
		</ol>
	</li>
	<li>
		<p>
            Key implementation notes: LPV normally stores very low angular detail (few SH bands or directional channels) and relies on repeated propagation iterations to spread light. It does not explicitly store full scene geometry inside the grid (although depth/normal heuristics can be used to reduce obvious leakage).
		</p>
	</li>
	<li>
		<p>
			<strong>
                Usage
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    LPV fell out of favor in many production engines because its core design produces persistent, hard-to-fix artifacts (notably light bleeding and poor directional fidelity) and because alternative runtime GI approaches (probe-based DDGI variants, voxel-cone tracing, and hardware-accelerated ray-traced GI) offer better trade-offs for modern, dynamic scenes and artist workflows. The choice is engineering- and platform-dependent; LPV still makes sense in limited cases (very low-cost, low-frequency indirect lighting), but it is no longer the common ‚Äúgo-to‚Äù for high-quality dynamic GI in AAA engines.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://dev.epicgames.com/documentation/en-us/unreal-engine/light-propagation-volumes?application_version=4.27" 
				class="external-link" 
				target="_blank" >
                LPV - Unreal Engine 5
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    Deprecated in UE5, used in UE4.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<a
				href="https://cg.ivd.kit.edu/publications/p2009/LPVIC3_Kaplanyan_2009/LPVIC3_Kaplanyan_2009.pdf?utm_source=chatgpt.com" 
				class="external-link" 
				target="_blank" >
                Light Propagation Volumes - Cry Engine 3 - 2009
			</a>
            .
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://github.com/djbozkosz/Light-Propagation-Volumes" 
				class="external-link" 
				target="_blank" >
                LPV Implementation Demo
			</a>
            .
		</p>
	</li>
</ul>
<h3
	id="ambient-color" >
    Ambient Color
</h3>
<ul>
	<li>
		<p>
            <img src="assets/image_20250918204343.png" width="450" >
            .
		</p>
	</li>
	<li>
		<p>
            Usually 2 colors, applied depending on the normal of the surface.
		</p>
	</li>
</ul>
<h3
	id="instant-radiosity-quotevirtual-lightsquote" >
    Instant Radiosity (&quot;Virtual Lights&quot;)
</h3>
<ul>
	<li>
		<p>
            It's not instant, and has nothing to do with the 'Radiosity' method.
		</p>
	</li>
	<li>
		<ol>
			<li>
			</li>
		</ol>
	</li>
	<li>
		<p>
            Approximates GI by spawning virtual point lights (VPLs) from primary light bounces.
		</p>
	</li>
	<li>
		<p>
            Then renders the scene lit by these many point lights (with importance sampling and clamping to reduce artifacts).
		</p>
	</li>
	<li>
		<p>
            It‚Äôs an approximation to many-bounce light transport.
		</p>
	</li>
	<li>
		<p>
            The method requires finding secondary bounces to spawn virtual point lights (VPLs). This is traditionally done with ray tracing.
		</p>
		<ul>
			<li>
				<p>
                    However, simplified rasterization approximations (e.g., reflective shadow maps) exist that avoid explicit ray casting.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
            A &quot;Light Source&quot; is the Sun, and a &quot;Virtual Light&quot; is the Moon.
		</p>
	</li>
	<li>
		<p>
            Starting from the light source, generate virtual lights placed where the light illuminates a surface, by sampling random directions from the light source.
		</p>
	</li>
	<li>
		<p>
            The virtual lights account for indirect illumination; the indirect illumination becomes direct illumination from the virtual lights.
		</p>
	</li>
	<li>
		<p>
            It converts the problem of indirect illumination into the problem of rendering many light sources.
		</p>
	</li>
	<li>
		<p>
            It can account for one or many bounces of light; you just need to keep creating virtual lights.
		</p>
	</li>
	<li>
		<p>
            This is the opposite of Path Tracing; it's called 
			<em>
                Light Tracing
			</em>
            .
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250918112329.png" width="400" >
            .
		</p>
	</li>
	<li>
		<p>
            CryEngine 3:
		</p>
		<ul>
			<li>
				<p>
                    This class of approaches is based on the idea of representing indirect lighting as a cloud set of virtual point light sources (VPL). Consequently, this technique has a great potential to speed up with GPU. Its main advantages are&nbsp;&nbsp;good veracity and absence of any scene/lighting/camera constraints. Unfortunately, the main disadvantage of these methods is inadequate performance primarily because of the necessity to render at least 300-400 shadow- casting VPLs for an arbitrary scene to represent the precise solution without artifacts and flickering.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="horizon-based-ambient-occlusion-hbao" >
    Horizon-Based Ambient Occlusion (HBAO)
</h3>
<ul>
	<li>
		<p>
            &quot;Image-Space Horizon-based Ambient Occlusion&quot; - Nvidia Siggraph 2008.
		</p>
	</li>
	<li>
		<p>
            Very expensive trigonometry operations and too slow at the time.
		</p>
	</li>
</ul>
<h3
	id="alchemy-screen-space-ambient-obscurance-alchemyao" >
    Alchemy Screen-Space Ambient Obscurance (AlchemyAO)
</h3>
<ul>
	<li>
		<p>
            HBAO+ is an optimization of this technique.
		</p>
	</li>
</ul>
<h3
	id="metropolis-light-transport-mlt" >
    Metropolis Light Transport (MLT)
</h3>
<ul>
	<li>
		<p>
			<a
				href="https://graphics.stanford.edu/papers/metro/metro.pdf" 
				class="external-link" 
				target="_blank" >
                Metropolis Light Transport
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            Siggraph 1997.
		</p>
	</li>
	<li>
		<p>
            Monte Carlo method that explores light paths in a ‚Äúmutation‚Äù process, emphasizing important contributions (e.g., caustics).
		</p>
	</li>
	<li>
		<p>
            Ray tracing-based Monte Carlo.
		</p>
	</li>
</ul>
<h3
	id="path-tracing" >
    Path Tracing
</h3>
<ul>
	<li>
		<p>
			<a
				href="https://www.youtube.com/watch?v=iOlehM5kNSk" 
				class="external-link" 
				target="_blank" >
                Ray Tracing, Path Tracing, Global Illumination, BVH
			</a>
            .
		</p>
		<ul>
			<li>
				<p>
                    Great video. Very illustrative.
				</p>
			</li>
			<li>
				<p>
                    Path Tracing and Global Illumination:
				</p>
				<ul>
					<li>
						<p>
                            Nothing &quot;new&quot; in the explanations. It's based on a lot of material I studied.
						</p>
					</li>
					<li>
						<p>
                            Sometimes it complicates some explanations, making things seem a bit magical and &quot;untouchable&quot;
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    BVH: Binary Volume Hierarchy.
				</p>
				<ul>
					<li>
						<p>
                            Maybe it's the most relevant explanation.
						</p>
					</li>
					<li>
						<p>
                            <img src="assets/image_20251006231423.png" width="400" >
                            .
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    <img src="assets/image_20251006231924.png" width="400" >
                    .
				</p>
				<ul>
					<li>
						<p>
                            Interesting performance statistics.
						</p>
					</li>
				</ul>
			</li>
			<li>
				<p>
                    The video ends at 22:15.
				</p>
			</li>
		</ul>
	</li>
	<li>
		<p>
			<input
				type="checkbox" 
				disabled=""
>
            
			<a
				href="https://www.scratchapixel.com/lessons/3d-basic-rendering/global-illumination-path-tracing/introduction-global-illumination-path-tracing.html" 
				class="external-link" 
				target="_blank" >
                Global Illumination and Path Tracing
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            The standard method for computing global illumination today.
		</p>
	</li>
	<li>
		<p>
            Finds light paths starting from the 
			<em>
                camera
			</em>
            .
		</p>
	</li>
	<li>
		<p>
            Project a ray and pick a random direction.
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250918104831.png" width="250" >
            &nbsp;
            <img src="assets/image_20250918105000.png" width="300" >
            .
		</p>
	</li>
	<li>
		<p>
            It tries to find all possible light paths starting from the camera to the light source.
		</p>
	</li>
	<li>
		<p>
            The amount of light paths consider is indicated by the SPP (Samples Per pixel).
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250918105138.png" width="250" >
            &nbsp;
            <img src="assets/image_20250918105219.png" width="250" >
            &nbsp;
            <img src="assets/image_20250918105246.png" width="250" >
            &nbsp;
            <img src="assets/image_20250918105337.png" width="250" >
            &nbsp;
            <img src="assets/image_20250918105419.png" width="250" >
            .
		</p>
	</li>
	<li>
		<p>
			<strong>
                Denoiser
			</strong>
            :
		</p>
		<ul>
			<li>
				<p>
                    <img src="assets/image_20250918105955.png" width="400" >
                    .
				</p>
			</li>
			<li>
				<ol>
					<li>
					</li>
				</ol>
			</li>
			<li>
				<p>
                    Use the samples from the neighboring pixels, to estimate the indirect illumination for the current pixel.
				</p>
			</li>
			<li>
				<p>
                    AI and Deep Learning is used today.
				</p>
			</li>
			<li>
				<p>
					<em>
                        Spacial Denoising
					</em>
                    :
				</p>
			</li>
			<li>
				<p>
					<em>
                        Temporal Denoising
					</em>
                    :
				</p>
				<ul>
					<li>
						<p>
                            Cheaper and it doesn't blur.
						</p>
					</li>
					<li>
						<p>
                            For a game that has a camera moving all the time, you'll have to use:
						</p>
						<ul>
							<li>
								<p>
                                    Motion Vectors.
								</p>
								<ul>
									<li>
										<p>
                                            How things moved between frames.
										</p>
									</li>
									<li>
										<p>
                                            <img src="assets/image_20250918121801.png" width="300" >
                                            .
										</p>
									</li>
								</ul>
							</li>
							<li>
								<p>
                                    Reject samples that were reprojected incorrectly.
								</p>
							</li>
							<li>
								<p>
                                    <img src="assets/image_20250918121813.png" width="300" >
                                    .
								</p>
							</li>
						</ul>
					</li>
				</ul>
			</li>
			<li>
				<p>
					<strong>
                        Joint Bileteral Filter
					</strong>
                    :
				</p>
				<ul>
					<li>
						<p>
                            <img src="assets/image_20250918121942.png" width="350" >
                            .
						</p>
					</li>
					<li>
						<p>
                            Takes into account the depth and normals.
						</p>
					</li>
					<li>
						<p>
                            <img src="assets/image_20250918122051.png" width="350" >
                            .
						</p>
						<ul>
							<li>
								<p>
                                    My understanding is that there is less blur based on the depth buffer, in the image on the left.
								</p>
							</li>
						</ul>
					</li>
				</ul>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="photon-mapping" >
    Photon Mapping
</h3>
<ul>
	<li>
		<p>
            1995 to 2001.
		</p>
	</li>
	<li>
		<p>
			<a
				href="https://web.cs.wpi.edu/~emmanuel/courses/cs563/write_ups/zackw/photon_mapping/PhotonMapping.html" 
				class="external-link" 
				target="_blank" >
                Photon Mapping
			</a>
            .
		</p>
	</li>
	<li>
		<p>
            Two-pass algorithm: first, photons are traced from light sources and stored; second, the radiance is gathered at surfaces to compute illumination.
		</p>
	</li>
	<li>
		<p>
            Handles caustics and diffuse interreflections.
		</p>
	</li>
	<li>
		<p>
            Uses Ray tracing (photon tracing) + data structure (k-d tree for photon storage).
		</p>
	</li>
	<li>
		<p>
            CryEngine 3:
		</p>
		<ul>
			<li>
				<p>
                    These methods are less popular than the others in real-time graphics because of their performance issues. Usually this class of techniques is based on the classical photon-mapping approach. These methods usually use GPU texture fetching and rendering units to accelerate the photon map evaluation. The usual optimizations for these techniques are irradiance caching, importance sampling, and the incremental approach. One drawback of these methods is that the scene needs to be preprocessed to get the unique representation for the photon map. Another problem is photon map updates caused by scene and lighting changes, which leads to highly inconsistent performance and intermittent stalls.
				</p>
			</li>
		</ul>
	</li>
</ul>
<h3
	id="radiosity" >
    Radiosity
</h3>
<ul>
	<li>
		<ol>
			<li>
			</li>
		</ol>
	</li>
	<li>
		<p>
            Origin of the 
			<em>
                Cornell Box
			</em>
            .
		</p>
	</li>
	<li>
		<p>
            Radiosity is a global illumination method that solves light transport between diffuse-only surfaces by discretizing geometry into patches and solving a linear system of energy exchange.
		</p>
	</li>
	<li>
		<p>
            Produces smooth, view-independent GI (often offline or precomputed).
		</p>
	</li>
	<li>
		<p>
            Surface-based; patch-to-patch energy exchange (matrix solve). Ray tracing is optional for visibility (hemicube or ray casting).
		</p>
	</li>
	<li>
		<p>
            It doesn't use ray tracing inherently. Radiosity is based on solving a radiosity matrix (energy exchange between surface patches).
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250918104614.png" width="300" >
            .
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250918104634.png" width="300" >
            .
		</p>
	</li>
	<li>
		<p>
            <img src="assets/image_20250918104652.png" width="350" >
            .
		</p>
	</li>
</ul>

				</article>
			</main>
			<footer
				id="central-footer" >
                üßë‚Äçüíª built and copyrighted by
				<a
					href="https://github.com/caioraphael1" 
					target="_blank" >
                    Caio Raphael
				</a>
                üìÖ 2025-2026 üöÄ
			</footer>
		</main>
		<script
			src="/static/studies.70566.js" >
		</script>
	</body>
</html>
